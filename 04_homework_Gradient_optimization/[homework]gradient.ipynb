{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[homework]gradient.ipynb","provenance":[{"file_id":"1uL5fRnCO3W97tsRl-zrPfJb5InTiv6Y1","timestamp":1633449663687},{"file_id":"1AmkNemwLmXp3SyRb6uFyTh9ZputZVrGl","timestamp":1633376460679}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"KxIquH6GK7kt"},"source":["<p style=\"align: center;\"><img align=center src=\"https://s8.hostingkartinok.com/uploads/images/2018/08/308b49fcfbc619d629fe4604bceb67ac.jpg\" width=500 height=450/></p>\n","\n","<h3 style=\"text-align: center;\"><b>\"Глубокое обучение\". Базовый поток</b></h3>\n","\n","<h2 style=\"text-align: center;\"><b>Домашнее задание. Производная, градиент и градиентный спуск\n","</b></h2>\n","\n","В этом домашнем задании вам предстоит поработать с понятием производной и градиента, а также написать градиентный спуск и его вариации.\n","\n","__Напоминание:__\n","Производной функции $f$ в точке $x$ называется выражение\n","\n","$$\\lim_{h→0}\\frac{f(x+h)−f(x)}{h}$$\n","Или, что то же самое,\n","$$\\lim_{x→x_0}\\frac{f(x)−f(x_0)}{x-x_0}$$\n","\n","Если такой предел существует, то и производная существует (и равна этому пределу)."]},{"cell_type":"code","metadata":{"id":"MIzAFQsnvod2"},"source":["from copy import copy, deepcopy\n","\n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v6JiQgamvod9"},"source":["## Задание 1 ##"]},{"cell_type":"markdown","metadata":{"id":"y38bKm9Tvod-"},"source":["Какие из перечисленных функций имеют производную в нуле $(x_0 = 0)$?\n","\n","1) $f(x) = |x|^2$\n","\n","2) $f(x) = \\frac{sin(x)}{x}$\n","\n","3) $f(x) = |x|$\n","\n","4) $f(x) = \n","     \\begin{cases}\n","       x^2, &\\text{$x \\ne 0$}; \\\\\n","       0, &\\text{$x = 0$}\n","     \\end{cases}$"]},{"cell_type":"markdown","metadata":{"id":"mfdVpqeTvoeA"},"source":["**Ответ:**"]},{"cell_type":"markdown","metadata":{"id":"nc4hJhHuvoeC"},"source":["## Задание 2 ##"]},{"cell_type":"markdown","metadata":{"id":"D_MN9myzvoeD"},"source":["Посчитайте производную $f(x)=x^x$ в точке $x_0 = e$\n","\n","Ответ округлите до одного знака после запятой.\n","\n","*Указание*. Представьте функцию $f(x)$ как $e^{g(x)}$ для некоторой $g$."]},{"cell_type":"markdown","metadata":{"id":"rBcFA6ErvoeF"},"source":["**Ответ:**"]},{"cell_type":"markdown","metadata":{"id":"WsJ_W3VXvoeH"},"source":["## Задание 3 ##"]},{"cell_type":"markdown","metadata":{"id":"sj-eLSJyvoeI"},"source":["Вычислите производную $f(x)=tg(x)⋅\\ln(\\cos(x^2)+1)$, в точке $x_0 = 0$. Ответ округлите до двух знаков после запятой."]},{"cell_type":"markdown","metadata":{"id":"ctY73lF1voeK"},"source":["**Ответ:**"]},{"cell_type":"markdown","metadata":{"id":"vHjndqfCvoeL"},"source":["## Задание 4 ##"]},{"cell_type":"markdown","metadata":{"id":"ve1wPT74voeN"},"source":["​Ваше задание --- написать python-функцию, которая в качестве аргумента принимает:\n","\n","числовую функцию $f$, у которой необходимо вычислить производную\n","число $\\varepsilon$ --- его необходимо использовать в качестве \"малого шага\" для приближённого вычисления производной.\n","Функция должна в свою очередь возвращать числовую функцию $f'$, равную производной функции $f$.\n","\n","Однако не подумайте, что вас просят написать что-то, что будет вычислять эту самую производную аналитически. Производную следует вычислять по формуле $$f'(x)\\approx \\frac{f(x+\\varepsilon) - f(x)}{\\varepsilon}.$$\n","​"]},{"cell_type":"code","metadata":{"id":"tYuZF08nvoeO"},"source":["def numerical_derivative_1d(func, epsilon):\n","    \"\"\"\n","    Функция для приближённого вычисления производной функции одной переменной. \n","    :param func: float -> float — произвольная дифференцируемая функция\n","    :param epsilon: float — максимальная величина приращения по оси Ох\n","    :return: другая функция, которая приближённо вычисляет производную в точке\n","    \"\"\"\n","    def deriv_func(x):\n","        \"\"\"\n","        :param x: float — точка, в которой нужно вычислить производную\n","        :return: приближённое значение производной в этой точке\n","        \"\"\"\n","        return #YOUR CODE\n","        \n","    return deriv_func"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ogo1thL-voeT"},"source":["# Проверьте себя!\n","def polynom_to_prime(x):\n","    return 20 * x**5 + x**3 - 5 * x**2 + 2 * x + 2.0\n","\n","\n","def primed_poly(x):\n","    return 100 * x**4 + 3 * x**2 -10 * x + 2.0\n","\n","\n","approx_deriv = numerical_derivative_1d(polynom_to_prime, 1e-5)\n","\n","grid = np.linspace(-2, 2, 100)\n","right_flag = True\n","tol = 0.05\n","debug_print = []\n","\n","for x in grid:\n","    estimation_error = abs(primed_poly(x) - approx_deriv(x)) \n","    if estimation_error > tol:\n","        debug_print.append((estimation_error, primed_poly(x), approx_deriv(x)))\n","        right_flag = False\n","\n","if not right_flag:\n","    print(\"Что-то не то...\")\n","    print(debug_print)\n","    plt.plot(grid, primed_poly(grid), label=\"Истинная производная\")\n","    plt.plot(grid, approx_deriv(grid), label=\"Численное приближение\")\n","    plt.legend()\n","\n","print(str(right_flag))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oJVDE5FovoeZ"},"source":["## Задание 5 ##"]},{"cell_type":"markdown","metadata":{"id":"91UYo01wvoea"},"source":["В этом задании Вы должны найти минимум функций с помощью градиентного спуска.\n","\n","Вам на вход подаются функция `func`, ее производная `deriv` (*), а также начальная точка `start`, на выходе - точка локального минимума. Для вашего удобства мы написали функцию для отрисовки траектории градиентного спуска\n","\n","(*) - вам не нужно будет ее вычислять. То, что вы написали в предыдущем задании, вам пригодится чуть позже.\n","\n","В первой реализации градиентного спуска можете предполагать, что на вход подаются функции с единственным, глобальным минимумом. Перед тем, как писать код, ответьте себе на следующие вопросы:\n","\n","* Как понять, что пора остановиться? Это может зависеть от градиента или расстояния между двумя соседними шагами алгоритма, так и от числа уже выполненных итераций.\n","* Как правильно менять величину шага (`learning rate`) от итерации к итерации?\n","\n","В этом пункте гарантируется, что существует решение, использующее обычный градиентный спуск с фиксированным learning rate и наперёд заданным количеством итераций. "]},{"cell_type":"markdown","metadata":{"id":"sfBgQXzxfKRk"},"source":["На каждой итерации вызывайте `callback(x, f(x))`, где `x` это результат шага градиентного спуска.   \n","Это нужно для отрисовки шагов алгоритма."]},{"cell_type":"code","metadata":{"id":"6o-LAqEHARF1"},"source":["def grad_descent_v1(f, deriv, x0=None, lr=0.1, iters=100, callback=None):\n","    \"\"\" \n","    Реализация градиентного спуска для функций с одним локальным минимумом,\n","    совпадающим с глобальным. Все тесты будут иметь такую природу.\n","    :param func: float -> float — функция \n","    :param deriv: float -> float — её производная\n","    :param x0: float — начальная точка\n","    :param lr: float — learning rate\n","    :param iters: int — количество итераций\n","    :param callback: callable — функция логирования\n","    \"\"\"\n","\n","    if x0 is None:\n","        # Если точка не дана, сгенерируем случайную\n","        # из стандартного нормального распределения.\n","        # При таком подходе начальная точка может быть\n","        # любой, а не только из какого-то ограниченного диапазона\n","        # np.random.seed(179)\n","        x0 = np.random.uniform()\n","\n","    x = x0\n","\n","    callback(x, f(x))  # не забывайте логировать\n","\n","    #YOUR CODE. Сделайте итерации градиентного спуска для x\n","\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z-Is2v1J4dJy"},"source":["### Отрисовка и тесты\n","Рекомедуем пользоваться!"]},{"cell_type":"code","metadata":{"id":"kwZyc1vVd3h9"},"source":["def plot_convergence_1d(func, x_steps, y_steps, ax, grid=None, title=\"\"):\n","    \"\"\"\n","    Функция отрисовки шагов градиентного спуска. \n","    Не меняйте её код без необходимости! \n","    :param func: функция, которая минимизируется градиентным спуском\n","    :param x_steps: np.array(float) — шаги алгоритма по оси Ox\n","    :param y_steps: np.array(float) — шаги алгоритма по оси Оу\n","    :param ax: холст для отрисовки графика\n","    :param grid: np.array(float) — точки отрисовки функции func\n","    :param title: str — заголовок графика\n","    \"\"\"\n","    ax.set_title(title, fontsize=16, fontweight=\"bold\")\n","\n","    if grid is None:\n","        grid = np.linspace(np.min(x_steps), np.max(x_steps), 100)\n","\n","    fgrid = [func(item) for item in grid]\n","    ax.plot(grid, fgrid)\n","    yrange = np.max(fgrid) - np.min(fgrid)\n","\n","    arrow_kwargs = dict(linestyle=\"--\", color=\"grey\", alpha=0.4)\n","    for i, _ in enumerate(x_steps):\n","        if i + 1 < len(x_steps):\n","            ax.arrow(\n","                x_steps[i], y_steps[i], \n","                x_steps[i + 1] - x_steps[i],\n","                y_steps[i + 1] - y_steps[i], \n","                **arrow_kwargs\n","            )\n","\n","    n = len(x_steps)\n","    color_list = [(i / n, 0, 0, 1 - i / n) for i in range(n)]\n","    ax.scatter(x_steps, y_steps, c=color_list)\n","    ax.scatter(x_steps[-1], y_steps[-1], c=\"red\")\n","    ax.set_xlabel(r\"$x$\")\n","    ax.set_ylabel(r\"$y$\")\n","\n","\n","class LoggingCallback:\n","    \"\"\"\n","    Класс для логирования шагов градиентного спуска. \n","    Сохраняет точку (x, f(x)) на каждом шаге.\n","    Пример использования в коде: callback(x, f(x))\n","    \"\"\"\n","    def __init__(self):\n","        self.x_steps = []\n","        self.y_steps = []\n","\n","    def __call__(self, x, y):\n","        self.x_steps.append(x)\n","        self.y_steps.append(y)\n","\n","\n","def test_convergence_1d(grad_descent, test_cases, tol=1e-2, axes=None, grid=None):\n","    \"\"\"\n","    Функция для проверки корректности вашего решения в одномерном случае.\n","    Она же используется в тестах на Stepik, так что не меняйте её код!\n","    :param grad_descent: ваша реализация градиентного спуска\n","    :param test_cases: dict(dict), тесты в формате dict с такими ключами:\n","        - \"func\" — функция (обязательно)\n","        - \"deriv\" — её производная (обязательно)\n","        - \"start\" — начальная точка start (м.б. None) (опционально) \n","        - \"low\", \"high\" — диапазон для выбора начальной точки (опционально)\n","        - \"answer\" — ответ (обязательно)\n","    При желании вы можете придумать и свои тесты.\n","    :param tol: предельное допустимое отклонение найденного ответа от истинного\n","    :param axes: матрица холстов для отрисовки, по ячейке на тест\n","    :param grid: np.array(float), точки на оси Ох для отрисовки тестов\n","    :return: флаг, корректно ли пройдены тесты, и дебажный вывод в случае неудачи\n","    \"\"\"\n","    right_flag = True\n","    debug_log = []\n","    for i, key in enumerate(test_cases.keys()):\n","        # Формируем входные данные и ответ для алгоритма.\n","        answer = test_cases[key][\"answer\"]\n","        test_input = deepcopy(test_cases[key])\n","        del test_input[\"answer\"]\n","        # Запускаем сам алгоритм.\n","        callback = LoggingCallback()  # Не забываем про логирование\n","        res_point = grad_descent(*test_input.values(), callback=callback)\n","        # Отрисовываем результаты.\n","        if axes is not None:\n","            ax = axes[np.unravel_index(i, shape=axes.shape)]\n","            x_steps = np.array(callback.x_steps)\n","            y_steps = np.array(callback.y_steps)\n","            plot_convergence_1d(\n","                test_input[\"func\"], x_steps, y_steps, \n","                ax, grid, key\n","            )\n","            ax.axvline(answer, 0, linestyle=\"--\", c=\"red\",\n","                        label=f\"true answer = {answer}\")\n","            ax.axvline(res_point, 0, linestyle=\"--\", c=\"xkcd:tangerine\", \n","                        label=f\"estimate = {np.round(res_point, 3)}\")\n","            ax.legend(fontsize=16)\n","        # Проверяем, что найдення точка достаточно близко к истинной\n","        if abs(answer - res_point) > tol or np.isnan(res_point):\n","            debug_log.append(\n","                f\"Тест '{key}':\\n\"\n","                f\"\\t- ответ: {answer}\\n\"\n","                f\"\\t- вывод алгоритма: {res_point}\"\n","            )\n","            right_flag = False\n","    return right_flag, debug_log"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1eOO6t0cvoeh"},"source":["test_cases = {\n","    \"square\": {\n","        \"func\" : lambda x: x * x, \n","        \"deriv\" : lambda x: 2 * x, \n","        \"start\" : 2, \n","        \"answer\" : 0.0\n","    },\n","    \"module\": {\n","        \"func\" : lambda x: abs(x),  \n","        \"deriv\" : lambda x: 1 if x > 0 else -1,\n","        \"start\" : 2, \n","        \"answer\" : 0.0\n","    },\n","    \"third_power\": {\n","        \"func\" : lambda x: abs((x - 1)**3),\n","        \"deriv\" : lambda x: 3 * (x - 1)**2 * np.sign(x-1),\n","        \"start\" : -1, \n","        \"answer\" : 1.0\n","    },\n","    \"ln_x2_1\": {\n","        \"func\" : lambda x: np.log((x + 1)**2 + 1),  \n","        \"deriv\" : lambda x: 2 * (x + 1) / (x**2 +1), \n","        \"start\" : 1, \n","        \"answer\" : -1.0\n","    }\n","}\n","\n","\n","\n","tol = 1e-2  # желаемая точность \n","fig, axes = plt.subplots(2,2, figsize=(14, 12))\n","fig.suptitle(\"Градиентный спуск, версия 1\", fontweight=\"bold\", fontsize=20)\n","grid = np.linspace(-2, 2, 100)\n","\n","is_correct, debug_log = test_convergence_1d(\n","    grad_descent_v1, test_cases, tol, \n","    axes, grid\n",")\n","if not is_correct:\n","    print(\"Не сошлось. Дебажный вывод:\")\n","    for log_entry in debug_log:\n","        print(log_entry)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bjYYcxG0voel"},"source":["## Задание 6 ##"]},{"cell_type":"markdown","metadata":{"id":"3Bqka1-1voen"},"source":["Это задание чуть сложнее. Если раньше Вам нужно было просто найти минимум у довольно хорошей функции, то сейчас в тестах будут плохие. У них может быть несколько локальных минимумов, вам же нужно найти глобальный минимум у каждой функции.\n","\n","В общем случае такая задача невыполнима, но у вас будут одномерные функции и все самое интересное будет сосредоточено в районе нуля. А именно, известно что глобальный минимум лежит в пределах (`low`, `high`) (параметры алгоритма). Вам нужно модифицировать градиентный спуск, который вы написали в предыдущем задании, чтобы он работал и в таком случае. \n","\n","Сначала запустите градиентный спуск из прошлого пункта на тестах из ноутбука. Скорее всего, некоторые из них не пройдут. Подумайте, как исправить ситуацию.\n","\n","И снова не забывайте вызывать `callback(x, f(x))` на каждом шаге алгоритма!\n","\n","**Возможное решение** Если вы хотите поэкспериментировать и ощутить всю боль от оптимизации таких функций, сначала подумайте сами, не пытаясь следовать нашим указаниям. Тем не менее, для тех из вас, у кого таких наклонностей нет, мы выписали одно из возможных решений, которое приводит к успеху.\n","\n","\n","\n","* Сделайте шаг обучения не константным, а зависящим от номера итерации. Неплохая эвристика --- домножать `lr` на $ \\frac{1}{ \\sqrt{iteration}}$. \n","\n","* В этой задаче в функциях могут после первого же шага градиентного спуска появляться очень большие значения. Для того, чтобы не вылезать за пределы отрезка, на котором ищется минимум, после каждого шага спуска используйте ``np.clip`` к очередной точке ``x_n``. \n","\n","* Разбейте весь отрезок на несколько (3-6) подотрезков и найдите минимум на каждом из отрезков (на каждом отрезке, кстати, можно сделать больше одного запуска). Затем из всех найденных результатов выберите минимальный. \n","\n","* Авторское решение использует параметры ``iters = 5000`` и ``lr = 0.05``\n"," \n","\n","Больше о тонкостях градиентного спуска можно прочитать, например, в <a href=https://github.com/amkatrutsa/optimization-fivt/blob/master/07-GD/lecture7.pdf>лекциях МФТИ</a>.\n"]},{"cell_type":"code","metadata":{"id":"OsdOjX743Hoj"},"source":["def grad_descent_v2(f, df, low=None, high=None, callback=None):\n","    \"\"\" \n","    Реализация градиентного спуска для функций с несколькими локальным минимумами,\n","    но с известной окрестностью глобального минимума. \n","    Все тесты будут иметь такую природу.\n","    :param func: float -> float — функция \n","    :param deriv: float -> float — её производная\n","    :param low: float — левая граница окрестности\n","    :param high: float — правая граница окрестности\n","    :param callback: callalbe -- функция логирования\n","    \"\"\"\n","    def find_local_min(f, df, low_local, high_local, iters=5000, lr=0.05):\n","        #функция для нахождения минимума функции f на промежутке (low_local, high_local)\n","        x0 = np.random.uniform(low_local, high_local)\n","        x = x0\n","        \n","        for i in range(iters):\n","            #YOUR CODE. Don't forget to clip x to [low_local, high_local]\n","\n","        callback(x, f(x))\n","\n","        return x\n","\n","\n","    # вам нужно запустить find_local_min несколько раз с разными границами и среди полученных ответов выбрать тот, при котором f имеет наименьшее значение \n","    # подсказка: np.argmin\n","    # YOUR CODE\n","    \n","    # Разбейте отрезок [low,high] на 3-6 равных частей \n","\n","    # Для каждой части запустите find_local_min несколько \n","    # (преподавательский код запускает 10) раз\n","        \n","    best_estimate = #Найдите общий минимум по всем запускам. Возможно, вы захотите \n","    #использовать np.argmin\n","    \n","    return best_estimate"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VgXx51m4voeu"},"source":["test_cases = {\n","    \"poly1\" : {\n","        \"func\" : lambda x: x**4 + 3 * x**3 + x**2 - 1.5 * x + 1,\n","        \"deriv\" : lambda x: 4 * x**3 + 9 * x**2 + 2 * x - 1.5,\n","        \"low\" : -4, \"high\" : 2, \"answer\" : -1.88\n","    },\n","    \"poly2\" : {\n","        \"func\" : lambda x: x**4 + 3 * x**3 + x**2 - 2 * x + 1.0,\n","        \"deriv\" : lambda x: 4 * x**3 + 9 * x**2 + 2 * x - 2.0, \n","        \"low\" : -3, \"high\" : 3, \"answer\" : 0.352\n","    },\n","    \"another yet poly\" : {\n","        \"func\" : lambda x: x**6 + x**4 - 10 * x**2 - x ,\n","        \"deriv\" : lambda x: 6 * x**5 + 4 * x**3 - 20 * x - 1, \n","        \"low\" : -2, \"high\" : 2, \"answer\" : 1.24829\n","    },\n","    \"and another yet poly\" : {\n","        \"func\" : lambda x: x**20 + x**2 - 20 * x + 10,\n","        \"deriv\" : lambda x: 20 * x**19 + 2 * x - 20, \n","        \"low\" : -0, \"high\" : 2, \"answer\" : 0.994502\n","    },\n","    \"|x|/x^2 - x + sqrt(-x) + (even polynom)\" : {\n","        \"func\" : lambda x: 5 * np.abs(x)/x**2 - 0.5 * x + 0.1 * np.sqrt(-x) + 0.01 * x**2 ,\n","        \"deriv\" : lambda x: -0.5 - 0.05/np.sqrt(-x) + 0.02 * x + 5/(x * np.abs(x)) - (10 * np.abs(x))/x**3,\n","        \"low\" : -4, \"high\" : -2, \"answer\" : -2.91701\n","    },\n","}\n","\n","tol = 1e-2 # желаемая точность\n","\n","fig, axes = plt.subplots(2,4, figsize=(24, 8))\n","fig.suptitle(\"Градиентный спуск, версия 2\", fontweight=\"bold\", fontsize=20)\n","grid = np.linspace(-3, 3, 100)\n","\n","is_correct, debug_log = test_convergence_1d(\n","    grad_descent_v2, test_cases, tol, \n","    axes, grid\n",")\n","\n","if not is_correct:\n","    print(\"Не сошлось. Дебажный вывод:\")\n","    for log_entry in debug_log:\n","        print(log_entry)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Pl7exWe3YLd"},"source":["​\n","# Основные положения дифференциального исчисления функций многих переменных\n","\n","Если вдруг Вы не знаете или не помните дифференциальное исчисление функций многих переменных, то вот несколько ключевых определений, которые помогут Вам справиться с заданиями 7-9. Здесь и ниже рассматриваются скалярные функции многих переменных, т.е. $f:U\\rightarrow \\mathbb{R}$, где $U$ --- область в $\\mathbb{R}^n$. \n","\n","\n","Мы хотим ввести некоторый аналог производной. Что мы можем делать уже сейчас --- это вычислять производные функции многих переменных по отдельным аргументам.\n","\n","## Дифференцируемость функции многих переменных\n","**Определение**.* Частной производной функции нескольких переменных* $f(x_1,x_2,\\ldots, x_n)$ по аргументу $x_i$ в точке $\\overline{x^0} = (x_1^0, x_2^0, \\ldots, x_n^0)$ называется производная функции $f$ по $x_i$ в точке $x^0_i$ как функции одного аргумента при фиксированных значениях $x_1^0$, $x_2^0, \\ldots$, $x_{i-1}^0$, $x_{i+1}^0, \\ldots$, $x_n^0$. Иными словами, частная производная равна вот такому пределу:\n","\n","$$\\lim_{h \\to 0}\\frac{f(x_1,\\ldots,x_{i-1}, x_i + h, x_{i+1}, \\ldots, x_n) - f(x_1,\\ldots,x_n)}{h}.$$\n","\n","**Обозначение**:\n","\n","$$f'_{x_i}(\\overline{x^0}); \\quad \\frac{\\partial f}{\\partial x_i}(\\overline{x_0}).$$\n","\n","**Определение**. Функция $f: \\mathbb{R}^n \\to \\mathbb{R}$ называется *дифференцируемой в точке* $\\overline{x^0} = (x_1^0, x_2^0, \\ldots, x_n^0)$, если имеет место представление\n","$$f(\\overline{x^0} + \\overline{\\Delta x}) = f(\\overline{x^0}) + \\langle \\overline{a}, \\overline{\\Delta x} \\rangle + o(|\\overline{\\Delta x}|),$$ где $\\overline{a} \\in \\mathbb{R}^n$ --- некоторый $n$-мерный вектор, который называется градиентом функции $f$ в точке $\\overline{x^0}$. \n","\n","**Обозначения для градиента**:\n","$$\\mathrm{grad} f(\\overline{x^0}) = \\nabla f(\\overline{x^0}) = f'(\\overline{x^0}).$$\n","\n","## Связь градиента и частных производных\n","Как можно заметить, определение выше, во-первых, полностью аналогично свойству производной функции одной переменной. Во-вторых, это определение довольно бесполезно. Оказывается, во всех \"хороших\" случаях справедливо следующее утверждение.\n","\n","**Теорема**.\n","Пусть функция $f$ имеет в точке $\\overline{x^0}$ непрерывные частные производные по каждой компоненте $x_i$. Тогда $f$ дифференцируема в точке $\\overline{x^0}$, причём её градиент равен вектору из частных производных, то есть \n","$$\\left(\\frac{\\partial f}{\\partial x_1}(\\overline{x^0}), \\frac{\\partial f}{\\partial x_2}(\\overline{x^0}), \\ldots, \\frac{\\partial f}{\\partial x_n}(\\overline{x^0})\\right).$$\n","\n","Суть этой теоремы заключается в том, что во всех ``хороших'' случаях градиент существует и его очень просто вычислить --- нужно просто посчитать частные производные по всем переменным.\n","\n","**Замечание**.\n","Градиент указывает на направление наискорейшего роста значения функции. Иными словами, при движении точки, стартующей в $\\overline{x^0}$, по вектору $\\mathrm{grad} f(\\overline{x^0})$, значение функции увеличивается.\n","\n"," \n","\n","## Алгоритм градиентного спуска\n","Алгоритм градиентного спуска для поиска минимума функции $n$ переменных $f(x_1,x_2,\\ldots, x_n)$ состоит в итеративном поиске точки минимума функции по формуле для $k+1$-ой точки через $k$-ую точку и градиент в $k$-ой точке.\n","\n","$$\\overline{x}^{k+1} = \\overline{x}^k - \\lambda\\nabla f(\\overline{x}^k),$$\n","\n","где $\\lambda$ --- положительное число, называемое learning rate. Обратите внимание, что здесь верхние индексы обозначают не степень, а номер точки в последовательности! То есть под $\\overline{x}^k$ понимается вектор $(x^k_1,\\ldots, x^k_n)$ --- $k$-ая точка из последовательности приближений, которую строит алгоритм.\n","\n","​"]},{"cell_type":"markdown","metadata":{"id":"pxQWFDuovoe-"},"source":["## Задание 7 ##"]},{"cell_type":"markdown","metadata":{"id":"8-8n8ZzIvoe_"},"source":["В лекции было несколько функций, чьи градиенты Вам было предложено вычислить.\n","\n","Вычислите градиент следующей функции:\n","\n","$$\\psi(x,y,z) = sin(xz) - y^2z + e^x$$\n","\n","Заполните пропуски в коде"]},{"cell_type":"markdown","metadata":{"id":"8Q47B6rkvofB"},"source":["**Ответ:**"]},{"cell_type":"code","metadata":{"id":"d4uFHoIfsV_0"},"source":["from math import sin, cos, tan, exp, sqrt, pi\n","import numpy as np\n","\n","def grad_1(x, y, z):\n","    #возвращает кортеж из 3 чисел --- частных производных по x,y,z \n","    \n","    dx = #YOUR CODE\n","    dy = #YOUR CODE\n","    dz = #YOUR CODE\n","    return (dx, dy, dz)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gKK81eg_ikMr"},"source":["#Тестируем нашу функцию\n","import numpy as np\n","\n","assert np.allclose(grad_1(1,1,1), (3.258584134327185, -2, -0.45969769413186023), atol=5e-6)\n","assert np.allclose(grad_1(1, 8, 0), (2.718281828459045, 0, -63.0), atol=5e-6)\n","assert np.allclose(grad_1(-11,pi,1), (0.004442399688841031, -6.283185307179586, -9.918287078957917), atol=5e-6)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JZ3JJG_mvofD"},"source":["## Задание 8 ##"]},{"cell_type":"markdown","metadata":{"id":"Vn9o18L5vofE"},"source":["Еще один градиент, похожий на тот, что был на лекции:\n","\n","$\\psi(x,y,z) = ln(cos(e^{x+y})) - ln(xy)$ \n","\n","Заполните пропуски в функции ниже\n"]},{"cell_type":"code","metadata":{"id":"EYcDdBZ2-BaT"},"source":["def grad_2(x, y, z):\n","    #возвращает кортеж из 3 чисел --- частных производных по x,y,z \n","    dx = #YOUR CODE\n","    dy = #YOUR CODE\n","    dz = #YOUR CODE\n","    return (dx, dy, dz)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gAjyySeKi8YN"},"source":["#Тестируем нашу функцию\n","\n","assert np.allclose(grad_2(1,1,0), (-15.73101919885423, -15.73101919885423, 0), atol=5e-6)\n","assert np.allclose(grad_2(-10, 3, 0), (0.09999916847105042, -0.3333341648622829, 0), atol=5e-6)\n","assert np.allclose(grad_2(15 ,4, 0), (54654806.79650013, 54654806.6131668,0), atol=5e-6)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rJi66t39voey"},"source":["## Задание 9 ##\n","А теперь все вместе!\n","\n","У вас есть только функция, которую Вам отдают в качестве аргумента и вы должны найти её минимум.\n","\n","Вы будете искать глобальный, у вас это должно получиться лишь потому, что тут они хорошие.\n","\n","Да, и еще, теперь они не одномерные, а двумерные. Также вам будут даны начальные точки, сходимость из которых гарантируется.\n","\n","***Подсказка*** можете использовать следующие параметры:\n","\n","* Отклонение при вычислении производной $\\varepsilon = 10^{-10}$\n","* Критерий остановки: кол-во итераций $10^4$\n","* Длина шага градиентного спуска $lr = 0.5$\n","\n","\n","И вновь мы предоставляем функцию отрисовки шагов для пущего удобства."]},{"cell_type":"code","metadata":{"id":"egm2nGHCvoe1"},"source":["def numerical_derivative_2d(func, epsilon):\n","    \"\"\"\n","    Функция для приближённого вычисления градиента функции двух переменных. \n","    :param func: np.array[2] -> float — произвольная дифференцируемая функция\n","    :param epsilon: float — максимальная величина приращения по осям\n","    :return: другая функция, которая приближённо вычисляет градиент в точке\n","    \"\"\"\n","    def grad_func(x):\n","        \"\"\"\n","        :param x: np.array[2] — точка, в которой нужно вычислить градиент\n","        :return: np.array[2] — приближённое значение градиента в этой точке\n","        \"\"\"\n","        <YOUR CODE>\n","\n","        return <YOUR CODE>\n","    \n","    return grad_func\n","\n","\n","def grad_descent_2d(func, low, high, start=None, callback=None):\n","    \"\"\" \n","    Реализация градиентного спуска для функций двух переменных \n","\n","    Обратите внимание, что здесь градиент функции не дан.\n","    Его нужно вычислять приближённо.\n","\n","    :param func: np.ndarray -> float — функция \n","    :param low: левая граница интервала по каждой из осей\n","    :param high: правая граница интервала по каждой из осей\n","    \"\"\"\n","    eps = 1e-10\n","    df = numerical_derivative_2d(func, eps)\n","    \n","    \n","    <YOUR CODE>\n","\n","    return <YOUR CODE>"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ARnTlYWcLGo0"},"source":["def plot_convergence_2d(func, steps, ax, xlim, ylim, cmap=\"viridis\", title=\"\"):\n","    \"\"\"\n","    Функция отрисовки шагов градиентного спуска. \n","    Не меняйте её код без необходимости! \n","    :param func: функция, которая минимизируется градиентным спуском\n","    :param steps: np.array[N x 2] — шаги алгоритма\n","    :param ax: холст для отрисовки графика\n","    :param xlim: tuple(float), 2 — диапазон по первой оси\n","    :param ylim: tuple(float), 2 — диапазон по второй оси\n","    :param cmap: str — название палитры\n","    :param title: str — заголовок графика\n","    \"\"\"\n","\n","    ax.set_title(title, fontsize=20, fontweight=\"bold\")\n","    # Отрисовка значений функции на фоне\n","    xrange = np.linspace(*xlim, 100)\n","    yrange = np.linspace(*ylim, 100)\n","    grid = np.meshgrid(xrange, yrange)\n","    X, Y = grid\n","    fvalues = func(\n","        np.dstack(grid).reshape(-1, 2)\n","    ).reshape((xrange.size, yrange.size))\n","    ax.pcolormesh(xrange, yrange, fvalues, cmap=cmap, alpha=0.8)\n","    CS = ax.contour(xrange, yrange, fvalues)\n","    ax.clabel(CS, CS.levels, inline=True)\n","    # Отрисовка шагов алгоритма в виде стрелочек\n","    arrow_kwargs = dict(linestyle=\"--\", color=\"black\", alpha=0.8)\n","    for i, _ in enumerate(steps):\n","        if i + 1 < len(steps):\n","            ax.arrow(\n","                *steps[i],\n","                *(steps[i+1] - steps[i]),\n","                **arrow_kwargs\n","            )\n","    # Отрисовка шагов алгоритма в виде точек\n","    n = len(steps)\n","    color_list = [(i / n, 0, 0, 1 - i / n) for i in range(n)]\n","    ax.scatter(steps[:, 0], steps[:, 1], c=color_list, zorder=10)\n","    ax.scatter(steps[-1, 0], steps[-1, 1], \n","               color=\"red\", label=f\"estimate = {np.round(steps[-1], 2)}\")\n","    # Финальное оформление графиков\n","    ax.set_xlim(xlim)\n","    ax.set_ylim(ylim)\n","    ax.set_ylabel(\"$y$\")\n","    ax.set_xlabel(\"$x$\")\n","    ax.legend(fontsize=16)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UFgjqiYZvoe5"},"source":["def test_convergence_2d(grad_descent_2d, test_cases, tol, axes=None):\n","    \"\"\"\n","    Функция для проверки корректности вашего решения в двумерном случае.\n","    Она же используется в тестах на Stepik, так что не меняйте её код!\n","    :param grad_descent_2d: ваша реализация градиентного спуска\n","    :param test_cases: dict(dict), тесты в формате dict с такими ключами:\n","        - \"func\" — функция \n","        - \"deriv\" — её производная \n","        - \"low\", \"high\" — диапазон для выбора начальной точки \n","        - \"answer\" — ответ \n","    При желании вы можете придумать и свои тесты.\n","    :param tol: предельное допустимое отклонение найденного ответа от истинного\n","    :param axes: матрица холстов для отрисовки, по ячейке на тест\n","    :return: флаг, корректно ли пройдены тесты, и дебажный вывод в случае неудачи\n","    \"\"\"\n","    right_flag = True\n","    debug_log = []\n","    for i, key in enumerate(test_cases.keys()):\n","        # Формируем входные данные и ответ для алгоритма.\n","        answer = test_cases[key][\"answer\"]\n","        test_input = deepcopy(test_cases[key])\n","        del test_input[\"answer\"]\n","        # Запускаем сам алгоритм.\n","        callback = LoggingCallback()  # Не забываем про логирование\n","        res_point = grad_descent_2d(**test_input, callback=callback)\n","        # Отрисовываем результаты.\n","        if axes is not None:\n","            ax = axes[np.unravel_index(i, shape=axes.shape)]\n","            plot_convergence_2d(\n","                np.vectorize(test_input[\"func\"], signature=\"(n)->()\"), \n","                np.vstack(callback.x_steps), \n","                ax=ax, \n","                xlim=(test_input[\"low\"], test_input[\"high\"]), \n","                ylim=(test_input[\"low\"], test_input[\"high\"]),\n","                title=key\n","            )   \n","        # Проверяем, что найденная точка достаточно близко к истинной\n","        if np.linalg.norm(answer - res_point, ord=1) > tol:\n","            debug_log.append(\n","                f\"Тест '{key}':\\n\"\n","                f\"\\t- ответ: {answer}\\n\"\n","                f\"\\t- вывод алгоритма: {res_point}\"\n","            )\n","            right_flag = False\n","    return right_flag, debug_log\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vR_-Ucs4giUQ","colab":{"base_uri":"https://localhost:8080/","height":231},"executionInfo":{"status":"error","timestamp":1634574755745,"user_tz":-180,"elapsed":25,"user":{"displayName":"Yury Yarovikov","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gip8__BUAkkFW7zB1tjXwB7Y8uEezomM5ErVG2V=s64","userId":"05223355485824927663"}},"outputId":"5d3517c1-cce3-49b3-8499-e8fe5a299388"},"source":["test_cases = {\n","    \"concentric_circles\" : {\n","        \"func\" : lambda x: (\n","            -1 / ((x[0] - 1)**2 + (x[1] - 1.5)**2 + 1)\n","            * np.cos(2 * (x[0] - 1)**2 + 2 * (x[1] - 1.5)**2)\n","        ),\n","        \"low\" : -5,\n","        \"high\" : 5,\n","        \"start\": np.array([.2 , .7]),\n","        \"answer\" : np.array([1, 1.5])\n","    },\n","        \"other concentric circles\" : {\n","       \"func\" : lambda x: (\n","            -1 / ((x[0])**2 + (x[1] - 3)**2 + 1)\n","            * np.cos(2 * (x[0])**2 + 2 * (x[1] - 3)**2)\n","        ),\n","        \"low\" : -5,\n","        \"high\" : 5,\n","        \"start\": np.array([1.1, 3.3]),\n","        \"answer\" : np.array([0, 3])\n","    },\n","    \"straightened ellipses\" : {\n","        \"func\" : lambda x: (\n","            -1 / ((x[0] )**4 + (x[1] - 3)**6 + 1)\n","            * np.cos(2 * (x[0])**4 + 2 * (x[1] - 3)**6)\n","        ),\n","        \"low\" : -5,\n","        \"high\" : 5,\n","        \"start\": np.array([.8, 3.001]), # точка так близко к ответу тк в окрестности ответа градиент маленкьий и функция очень плохо сходится\n","        \"answer\" : np.array([0, 3])\n","    },\n","}\n","tol = 1e-2  # желаемая точность\n","\n","fig, axes = plt.subplots(1, 3, figsize=(30, 10), squeeze=False)\n","fig.suptitle(\"Градиентный спуск в 2D\", fontsize=25, fontweight=\"bold\")\n","is_correct, debug_log = test_convergence_2d(grad_descent_2d, test_cases, tol, axes)\n","\n","if not is_correct:\n","    print(\"Не сошлось. Дебажный вывод:\")\n","    for log_entry in debug_log:\n","        print(log_entry)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-5c0f65a384ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;34m\"low\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;34m\"high\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;34m\"start\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m.2\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m.7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;34m\"answer\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     },\n","\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"]}]},{"cell_type":"code","metadata":{"id":"6C0vjkL1P0dP"},"source":[""],"execution_count":null,"outputs":[]}]}