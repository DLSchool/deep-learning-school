{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[homework]seq2seq_and_attn.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"colab_type":"text","id":"jgH3CAcubM4z"},"cell_type":"markdown","source":["# ACHTUNG!!\n","translate(Hausaufgabe) -> **Homework**\n","\n","\n","на тему Seq2Seq модели в машинном переводе и\n","\n","Halo!\n","На семинаре мы создали простую seq2seq модель на основе rnn для перевода, а сейчас постараемся засунуть туда attention. Работать будем с тем же датасетом DE->EN (датасеты получше просто не влезают в память колаба, но если у вас есть CPU+тонна времени или GPU побольше, то можно попробовать построить перевод на WMT14 или IWSLT )\n"]},{"metadata":{"colab_type":"code","id":"1fPuwHEnVIzn","colab":{}},"cell_type":"code","source":["\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","\n","import spacy\n","\n","import random\n","import math\n","import time\n","\n","from torchtext.datasets import TranslationDataset, Multi30k #WMT14, IWSLT\n","from torchtext.data import Field, BucketIterator\n","\n","import torch.nn.functional as F"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"uQSnhb84VLU7","colab":{}},"cell_type":"code","source":["seed = 43\n","\n","random.seed(seed)\n","torch.manual_seed(seed)\n","np.random.seed(seed)\n","torch.backends.cudnn.deterministic = True\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"L10vdpVaVXBo","colab":{}},"cell_type":"code","source":["! python -m spacy download en\n","! python -m spacy download de\n","\n","\n","spacy_de = spacy.load('de')\n","spacy_en = spacy.load('en')"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"ferOqkOUVirW","colab":{}},"cell_type":"code","source":["def tokenize_de(text):\n","    \"\"\"\n","    Tokenizes German text from a string into a list of strings (tokens) and reverses it\n","    \"\"\"\n","    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n","\n","def tokenize_en(text):\n","    \"\"\"\n","    Tokenizes English text from a string into a list of strings (tokens)\n","    \"\"\"\n","    return [tok.text for tok in spacy_en.tokenizer(text)]\n","\n","# немецкий язык является полем SRC, а английский в поле TRG\n","SRC = Field(tokenize = tokenize_de, \n","            init_token = '<sos>', \n","            eos_token = '<eos>', \n","            lower = True)\n","\n","TRG = Field(tokenize = tokenize_en, \n","            init_token = '<sos>', \n","            eos_token = '<eos>', \n","            lower = True)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"u6pNY6cWW3j5","colab":{}},"cell_type":"code","source":["# В датасете содержится ~ 30к предложений средняя длина которых 11\n","train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),  fields = (SRC, TRG))"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"iOS3e7QZbLro"},"cell_type":"markdown","source":["Давайте посмотрим что у нас с датасетом и сделаем словари для SRC и TGT"]},{"metadata":{"colab_type":"code","id":"r0Xpf4IBW4Uf","colab":{}},"cell_type":"code","source":["labels = ['train', 'validation', 'test']\n","dataloaders = [train_data, valid_data, test_data]\n","for d, l in zip(dataloaders, labels):\n","    print(\"Number of sentences in {} : {}\".format(l, len(d.examples)))\n"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"Gg63m8haW4XC","colab":{}},"cell_type":"code","source":["SRC.build_vocab(train_data, min_freq = 2)\n","TRG.build_vocab(train_data, min_freq = 2)\n","print(\"Number of words in source vocabulary\", len(SRC.vocab))\n","print(\"Number of words in source vocabulary\", len(TRG.vocab))"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"LSd3la5FbJ5_"},"cell_type":"markdown","source":["## Encoder\n","\n","Энкодер будет ровно как в семинаре, с кдинственным изменением -- forward будет возвращать не только hidden, cell, но еще и outputs. Это нужно (надеюсь, вы уже поняли) для использования attention в декодере"]},{"metadata":{"colab_type":"code","id":"_Ar5SN6tW4ck","colab":{}},"cell_type":"code","source":["from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","\n","class Encoder(nn.Module):\n","    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n","        \"\"\"\n","        :param: input_dim is the size/dimensionality of the one-hot vectors that will be input to the encoder. This is equal to the input (source) vocabulary size.\n","        :param: emb_dim is the dimensionality of the embedding layer. This layer converts the one-hot vectors into dense vectors with emb_dim dimensions.\n","        :param: hid_dim is the dimensionality of the hidden and cell states.\n","        :param: n_layers is the number of layers in the RNN.\n","        :param: percentage of the dropout to use\n","        \n","        \"\"\"\n","        super().__init__()\n","        \n","        self.input_dim = input_dim\n","        self.emb_dim = emb_dim\n","        self.hid_dim = hid_dim\n","        self.n_layers = n_layers\n","        self.dropout = dropout\n","\n","        self.embedding = nn.Embedding(input_dim, emb_dim)\n"," \n","        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, src):\n","        \"\"\"\n","        :param: src sentences (src_len x batch_size)\n","        \"\"\"\n","        # embedded = <TODO> (src_len x batch_size x embd_dim)\n","        embedded = self.embedding(src)\n","        # dropout over embedding\n","        embedded = self.dropout(embedded)\n","        outputs, (hidden, cell) = self.rnn(embedded)\n","        # [Attention return is for lstm, but you can also use gru]\n","        return outputs, hidden, cell"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"-8QOCpKxfD3M"},"cell_type":"markdown","source":["## Decoder\n","Оп ля, а тут уже что-то новенькое\n","\n","Мы будем реализовывать attention, который будет смотреть из tgt в src (НЕ self-attention). \n","\n","Определим два класса -- Attention и DecoderAttn. Мы разбили их на два класса, чтобы можно было играться с типом внимания, не меняя код DecoderAttn. Как вы помните с лекции, в качестве аттеншена можно брать любую странную функцию (конкатенация, маленькая сеточка, ...), и все будет работать! Поэтому вам предлагается попробовать несколько разных.\n","\n","\n","---------------------\n","Есть два подхода к реализации аттеншена:\n","\n","Подход #1:\n","\n","1. Вычисляется embed\n","2. На основе hidden c прошлого шага, embedded и (возможно) enc_out вычисляется attention, а точнее, веса attention (поэтому не забудьте softmax!!). Размерность batch_size * max_len, max_len -- максимальная длина предложения в батче, т.е. shape[0] от выхода энкодера.\n","3. К enc_out применяется attention: чаще всего dot product от enc_out и attention_weights (не забудьте про измерение батч. Чтобы нормально вычислить dot_product по батчу, вам поможет torch.bmm)\n","4. Берутся attention и embedded и сворачиваются в один вектор размерности такой, чтобы кормить его self.lstm. Например, это можно сделать с помощью обычного линейного слоя\n","5. Вычисляется новое скрытое состояние new_hidden. Это наша self.lstm, примененная к выходу пункта 4.\n","6. Вычисляется prediction, как в семинаре\n","\n","Грубо говоря, вся разница с семинаром в том, что мы вместо того, чтобы embedded пихать в self.lstm, миксуем аттэншен на основе всего, что имеем (enc_out, hidden, embedded) и запихиваем в self.lstm микс аттэншена и embedded.\n","\n","![alt text](https://i.imgur.com/cmkRY0r.png)\n","\n","\n","Подход #2:\n","\n","1. Вычисляется embed\n","2. Вычисляется output, new_hidden (строчка output, (hidden, cell) = self.rnn(embedded, (hidden, cell)))\n","3. На основе output и enc_out вычисляется attention, а точнее, веса attention (поэтому не забудьте softmax!!)\n","3. К enc_out применяется attention: чаще всего dot product от enc_out и attention_weights (не забудьте про измерение батч. Чтобы нормально вычислить dot_product по батчу, вам поможет torch.bmm)\n","4. Вычисляется prediction на основе attention и output. Можно, например, взять nn.Linear() от конкатенации attention и output.\n","\n","Разница с первым подходом в том, что мы сначала вычисляем выход rnn слоя, а потом смотрим вниманием на src и на основе выхода rnn и attn считаем выход (prediction). \n","\n","![alt text](https://i.imgur.com/5aWjQWv.png)\n","\n","\n","Вам предлагается реализовать хотя бы 1 из вариантов и хотя бы 2 варианта функции attention (в классе Attention)\n"]},{"metadata":{"colab_type":"code","id":"gRgtzaf4bJp6","colab":{}},"cell_type":"code","source":["class Attention(nn.Module):\n","    def __init__(self, batch_size, method=\"one-layer-net\"): # add parameters needed for your type of attention\n","        super(Attention, self).__init__()\n","        self.method = method # attention method you'll use. e.g. \"cat\", \"one-layer-net\", \"dot\", ...\n","        \n","        <YOUR CODE HERE>\n","        \n","        def forward(self, last_hidden, encoder_outputs, seq_len=None):\n","            \n","            <YOUR CODE HERE>\n","            raise NotImplementedError\n","        \n","        \n","      \n","class DecoderAttn(nn.Module):\n","    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, attention, dropout=0.1):\n","        super(DecoderAttn, self).__init__()\n","        \n","        self.emb_dim = emb_dim\n","        self.hid_dim = hid_dim\n","        self.output_dim = output_dim\n","        self.n_layers = n_layers\n","        \n","        self.attn = attention # instance of Attention class\n","        \n","        self.dropout = dropout\n","\n","        # define layers\n","        self.embedding = nn.Embedding(self.output_dim, self.emb_dim)\n","        \n","        self.rnn = nn.LSTM(self.emb_dim, self.hid_dim, self.n_layers) #(lstm embd, hid, layers, dropout)\n","        self.out = nn.Linear(self.hid_dim, self.output_dim) # Projection :hid_dim x output_dim\n","        self.dropout = nn.Dropout(dropout)\n","\n","        # more layers you'll need for attention\n","        <YOUR CODE HERE>\n","        \n","    def forward(self, input, hidden, cell, encoder_output):\n","        # make decoder with attention\n","        # use code from seminar notebook as base and add attention to it\n","        <YOUR CODE HERE>\n","        \n","        \n","        return prediction, hidden, cell\n","\n","    def initHidden(self):\n","        return torch.zeros(1, 1, self.hidden_size, device=device)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"UlD7-nusfL86"},"cell_type":"markdown","source":["## Seq2Seq module"]},{"metadata":{"id":"HOineJlHpof2","colab_type":"text"},"cell_type":"markdown","source":["Здесь опять ничего не поменяется кроме того, что энкодер теперь возвращает свой output, а декодер его принимает"]},{"metadata":{"colab_type":"code","id":"v_YvVGzaW4fY","colab":{}},"cell_type":"code","source":["# we need that to put it first to the decoder in 'translate' method\n","BOS_IDX = SRC.vocab.stoi['<sos>']\n","\n","class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder, device):\n","        super().__init__()\n","        # Hidden dimensions of encoder and decoder must be equal\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","        self._init_weights() \n","        self.max_len=30\n","    \n","    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n","        \"\"\"\n","        :param: src (src_len x batch_size)\n","        :param: tgt\n","        :param: teacher_forcing_ration : if 0.5 then every second token is the ground truth input\n","        \"\"\"\n","        \n","        batch_size = trg.shape[1]\n","        max_len = trg.shape[0]\n","        trg_vocab_size = self.decoder.output_dim\n","        \n","        #tensor to store decoder outputs\n","        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n","        \n","        #last hidden state of the encoder is used as the initial hidden state of the decoder\n","        enc_out, hidden, cell = self.encoder(src, ) # TODO pass src throw encoder\n","        \n","        #first input to the decoder is the <sos> tokens\n","        input = trg[0, :] # TODO trg[idxs]\n","        \n","        for t in range(1, max_len):\n","            \n","            output, hidden, cell = self.decoder(input, hidden, cell, enc_out) #TODO pass state and input throw decoder \n","            outputs[t] = output\n","            teacher_force = random.random() < teacher_forcing_ratio\n","            top1 = output.max(1)[1]\n","            input = (trg[t] if teacher_force else top1)\n","        \n","        return outputs\n","    \n","    def translate(self, src):\n","        trg_vocab_size = self.decoder.output_dim\n","        \n","        #tensor to store decoder outputs\n","        outputs = []\n","        \n","        #last hidden state of the encoder is used as the initial hidden state of the decoder\n","        src = torch.tensor(src).to(self.device)\n","        hidden, cell = self.encoder(src.reshape((src.shape[0], 1))) # TODO pass src throw encoder\n","        \n","        #first input to the decoder is the <sos> tokens\n","        input = torch.tensor([BOS_IDX]).to(self.device)# TODO trg[idxs]\n","        \n","        for t in range(1, self.max_len):\n","            \n","            output, hidden, cell = self.decoder(input, hidden, cell) #TODO pass state and input throw decoder \n","            top1 = output.max(1)[1]\n","            outputs.append(top1)\n","            input = (top1)\n","        \n","        return outputs\n","    \n","    def _init_weights(self):\n","        p = 0.08\n","        for name, param in self.named_parameters():\n","            nn.init.uniform_(param.data, -p, p)\n","        \n"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"msbn2VypfUur","colab":{}},"cell_type":"code","source":["input_dim = len(SRC.vocab)\n","output_dim = len(TRG.vocab)\n","src_embd_dim =  tgt_embd_dim = 256\n","hidden_dim = 512\n","num_layers =  2\n","dropout_prob = 0.2\n","\n","batch_size = 64\n","PAD_IDX = TRG.vocab.stoi['<pad>']\n","\n","iterators = BucketIterator.splits((train_data, valid_data, test_data),\n","                                  batch_size = batch_size, device = device)\n","train_iterator, valid_iterator, test_iterator = iterators\n","\n","enc = Encoder(input_dim, src_embd_dim, hidden_dim, num_layers, dropout_prob)\n","dec = DecoderAttn(output_dim, tgt_embd_dim, hidden_dim, num_layers, dropout_prob)\n","model = Seq2Seq(enc, dec, device).to(device)\n"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"h5V9ZnK4fUxq","colab":{}},"cell_type":"code","source":["model"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"3vaUeDjTfU4k","colab":{}},"cell_type":"code","source":["def train(model, iterator, optimizer, criterion, clip):\n","    \n","    model.train()\n","    \n","    epoch_loss = 0\n","    \n","    for i, batch in enumerate(iterator):\n","        \n","        src = batch.src\n","        print(batch.src.shape)\n","        trg = batch.trg\n","        \n","        optimizer.zero_grad()\n","        \n","        output = model(src, trg)\n","        \n","        output = output[1:].view(-1, output.shape[-1])\n","        trg = trg[1:].view(-1)\n","        \n","        loss = criterion(output, trg)\n","        \n","        loss.backward()\n","        \n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        \n","        optimizer.step()\n","        \n","        epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(iterator)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"HfbTx2FMjaIM","colab":{}},"cell_type":"code","source":["def evaluate(model, iterator, criterion):\n","    \n","    model.eval()\n","    \n","    epoch_loss = 0\n","    \n","    with torch.no_grad():\n","    \n","        for i, batch in enumerate(iterator):\n","\n","            src = batch.src\n","            trg = batch.trg\n","\n","            output = model(src, trg, 0) #turn off teacher forcing !!\n","            output = output[1:].view(-1, output.shape[-1])\n","            trg = trg[1:].view(-1)\n","\n","\n","            loss = criterion(output, trg)\n","            \n","            epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(iterator)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"lQV_yqkLjcyQ","colab":{}},"cell_type":"code","source":["max_epochs = 100\n","CLIP = 1\n","\n","# TODO\n","optimizer = optim.Adam(model.parameters(), lr = 0.001)\n","criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(max_epochs):\n","    \n","    \n","    train_loss = round(train(model, train_iterator, optimizer, criterion, CLIP), 5)\n","    valid_loss = round(evaluate(model, valid_iterator, criterion),5)\n","    \n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'model.pt')\n","    \n","    print('Epoch: {} \\n Train Loss {}  Val loss {}:'.format(epoch, train_loss, valid_loss))\n","    print('Train Perplexity {}  Val Perplexity {}:'.format(np.exp(train_loss), np.exp(valid_loss)))\n"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"n5Zf6Kb1jhOI","colab":{}},"cell_type":"code","source":["test_loss = evaluate(model, test_iterator, criterion)\n","\n","print('| Test Loss: {} Test PPL:{}|'.format(test_loss, np.exp(test_loss)))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"an6HG7_uyjJN","colab_type":"code","colab":{}},"cell_type":"code","source":["EOS_IDX = SRC.vocab.stoi['<eos>']\n","\n","def translate(sentence):\n","    \"\"\"\n","    function that uses .translate() method of the model to translate german sentence into english\n","    params:\n","        sentence: tokenized gernam sentence\n","    \"\"\"\n","    sentence = sentence.lower()\n","    sent_vec = [SRC.vocab.stoi[token] for token in sentence.split()]\n","    translation_idx = model.translate(torch.tensor(sent_vec))\n","    for t in translation_idx:\n","        if t[0] != EOS_IDX:\n","            print(TRG.vocab.itos[t[0]], end=' ')\n","        else:\n","            break"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sP2TiDm18gyi","colab_type":"code","colab":{}},"cell_type":"code","source":["translate(\"ein klein apfel\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tBrHtQ_qrSKX","colab_type":"text"},"cell_type":"markdown","source":["ИИИИ давайте также научимся считать самую популярную метрику для перевода -- BLEU (https://en.wikipedia.org/wiki/BLEU)\n","\n","В общем-то, вам повезло -- ее писать руками скучно, да и nltk ее написало за вас:"]},{"metadata":{"id":"HFIVyXXeJrCr","colab_type":"code","colab":{}},"cell_type":"code","source":["from nltk.translate.bleu_score import corpus_bleu\n","def compute_bleu(inp_lines, out_lines):\n","    \"\"\" Estimates corpora-level BLEU score of model's translations given inp and reference out \"\"\"\n","    translations = [translate(line) for line in inp_lines]\n","    return corpus_bleu([[ref] for ref in out_lines], translations) * 100"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4bi1oJ_brjkO","colab_type":"code","colab":{}},"cell_type":"code","source":["compute_bleu(<batch_of_sentences>, <batch_of_sentences>)"],"execution_count":0,"outputs":[]}]}