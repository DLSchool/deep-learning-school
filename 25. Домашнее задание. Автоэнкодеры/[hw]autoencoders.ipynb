{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Копия блокнота \"[hw]autoencoders.ipynb\"","provenance":[{"file_id":"1EGsZa7t_iBh680M0cWBjgSbLayU95ye1","timestamp":1637967618735},{"file_id":"16GlpHEDebtEMcfVqjyMrzS_JqUeG2PK_","timestamp":1620897929964},{"file_id":"1C9udMkv_EMI4E8DfHzeb0Vqay4dG9uI9","timestamp":1620761433657}],"collapsed_sections":["dFi96giuYY7t","Ey8dD9s0YY7w","KN3D_k5W_WZz","-NDiCPYLm2bY"],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"LQ7i1HkmYY68"},"source":["<p style=\"align: center;\"><img align=center src=\"https://drive.google.com/uc?export=view&id=1I8kDikouqpH4hf7JBiSYAeNT2IO52T-T\" width=600 height=480/></p>\n","<h3 style=\"text-align: center;\"><b>Школа глубокого обучения ФПМИ МФТИ</b></h3>\n","\n","<h3 style=\"text-align: center;\"><b>Домашнее задание. Весна 2021</b></h3>\n","\n","# Autoencoders\n"]},{"cell_type":"markdown","metadata":{"id":"Wru2LNFuL2Iq"},"source":["# Часть 1. Vanilla Autoencoder (10 баллов)"]},{"cell_type":"markdown","metadata":{"id":"kr3STtdpYY7G"},"source":["## 1.1. Подготовка данных (0.5 балла)\n"]},{"cell_type":"code","metadata":{"id":"xTNi9JLRYY7I"},"source":["import numpy as np\n","from torch.autograd import Variable\n","from torchvision import datasets\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.utils.data as data_utils\n","import torch\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zvAjov5F2NvE"},"source":["def fetch_dataset(attrs_name = \"lfw_attributes.txt\",\n","                      images_name = \"lfw-deepfunneled\",\n","                      dx=80,dy=80,\n","                      dimx=64,dimy=64\n","    ):\n","\n","    #download if not exists\n","    if not os.path.exists(images_name):\n","        print(\"images not found, donwloading...\")\n","        os.system(\"wget http://vis-www.cs.umass.edu/lfw/lfw-deepfunneled.tgz -O tmp.tgz\")\n","        print(\"extracting...\")\n","        os.system(\"tar xvzf tmp.tgz && rm tmp.tgz\")\n","        print(\"done\")\n","        assert os.path.exists(images_name)\n","\n","    if not os.path.exists(attrs_name):\n","        print(\"attributes not found, downloading...\")\n","        os.system(\"wget http://www.cs.columbia.edu/CAVE/databases/pubfig/download/%s\" % attrs_name)\n","        print(\"done\")\n","\n","    #read attrs\n","    df_attrs = pd.read_csv(\"lfw_attributes.txt\",sep='\\t',skiprows=1,) \n","    df_attrs = pd.DataFrame(df_attrs.iloc[:,:-1].values, columns = df_attrs.columns[1:])\n","\n","\n","    #read photos\n","    photo_ids = []\n","    for dirpath, dirnames, filenames in os.walk(images_name):\n","        for fname in filenames:\n","            if fname.endswith(\".jpg\"):\n","                fpath = os.path.join(dirpath,fname)\n","                photo_id = fname[:-4].replace('_',' ').split()\n","                person_id = ' '.join(photo_id[:-1])\n","                photo_number = int(photo_id[-1])\n","                photo_ids.append({'person':person_id,'imagenum':photo_number,'photo_path':fpath})\n","\n","    photo_ids = pd.DataFrame(photo_ids)\n","    # print(photo_ids)\n","    #mass-merge\n","    #(photos now have same order as attributes)\n","    df = pd.merge(df_attrs,photo_ids,on=('person','imagenum'))\n","\n","    assert len(df)==len(df_attrs),\"lost some data when merging dataframes\"\n","\n","    # print(df.shape)\n","    #image preprocessing\n","    all_photos =df['photo_path'].apply(skimage.io.imread)\\\n","                                .apply(lambda img:img[dy:-dy,dx:-dx])\\\n","                                .apply(lambda img: resize(img,[dimx,dimy]))\n","\n","    all_photos = np.stack(all_photos.values)#.astype('uint8')\n","    all_attrs = df.drop([\"photo_path\",\"person\",\"imagenum\"],axis=1)\n","    \n","    return all_photos, all_attrs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W3KhlblLYY7P"},"source":["# The following line fetches you two datasets: images, usable for autoencoder training and attributes.\n","# Those attributes will be required for the final part of the assignment (applying smiles), so please keep them in mind\n","from get_dataset import fetch_dataset\n","data, attrs = fetch_dataset()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8MSzXXGoYY7X"},"source":["\n","Разбейте выборку картинок на train и val, выведите несколько картинок в output, чтобы посмотреть, как они выглядят, и приведите картинки к тензорам pytorch, чтобы можно было скормить их сети:"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"dFc8lTm_YY7Y"},"source":["<тут Ваш код>"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z9CC-DUhYY7i"},"source":["## 1.2. Архитектура модели (1.5 балла)\n","В этом разделе мы напишем и обучем обычный автоэнкодер.\n","\n","\n","\n","<img src=\"https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F4b8adf79-8e6a-4b7d-9061-8617a00edbb1%2F__2021-04-30__14.53.33.png?table=block&id=56f187b4-279f-4208-b1ed-4bda5f91bfc0&width=2880&userId=3b1b5e32-1cfb-4b0f-8705-5a524a8f56e3&cache=v2\" alt=\"Autoencoder\">\n","\n","\n","^ напомню, что автоэнкодер выглядит вот так"]},{"cell_type":"code","metadata":{"id":"csrNCYh-YY7j"},"source":["dim_code = <your code here> # выберите размер латентного вектора"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fjr-N8AWee-k"},"source":["Реализуем autoencoder. Архитектуру (conv, fully-connected, ReLu, etc) можете выбирать сами. Экспериментируйте!"]},{"cell_type":"code","metadata":{"id":"8SjHNX-rYY7k"},"source":["from copy import deepcopy\n","\n","class Autoencoder(nn.Module):\n","    def __init__(self):\n","        <определите архитектуры encoder и decoder>\n","        \n","    def forward(self, x):\n","        \n","        <реализуйте forward проход автоэнкодера\n","        в качестве ваозвращаемых переменных -- латентное представление картинки (latent_code) \n","        и полученная реконструкция изображения (reconstruction)>\n","        \n","        return reconstruction, latent_code"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"73lg3bI2YY7m"},"source":["criterion = <loss>\n","\n","autoencoder = Autoencoder()\n","\n","optimizer = <Ваш любимый оптимизатор>"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GpntmZCe5L6i"},"source":["## 1.3 Обучение (2 балла)"]},{"cell_type":"markdown","metadata":{"id":"Bdxg_3WJYY7o"},"source":["Осталось написать код обучения автоэнкодера. При этом было бы неплохо в процессе иногда смотреть, как автоэнкодер реконструирует изображения на данном этапе обучения. Наример, после каждой эпохи (прогона train выборки через автоэекодер) можно смотреть, какие реконструкции получились для каких-то изображений val выборки.\n","\n","А, ну еще было бы неплохо выводить графики train и val лоссов в процессе тренировки =)"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"3H3DOojrYY7o"},"source":["<тут Ваш код тренировки автоэнкодера>"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FAztAMA4YY7q"},"source":["Давайте посмотрим, как наш тренированный автоэекодер кодирует и восстанавливает картинки:"]},{"cell_type":"code","metadata":{"id":"I1J__yvxYY7r"},"source":["< тут Ваш код: выведите первые Х картинок и их реконструкций из val выборки на экран>"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1OPh9O6UYY7s"},"source":["Not bad, right? "]},{"cell_type":"markdown","metadata":{"id":"dFi96giuYY7t"},"source":["## 1.4. Sampling (2 балла)"]},{"cell_type":"markdown","metadata":{"id":"AOtUaPNYYY7t"},"source":["Давайте теперь будем не просто брать картинку, прогонять ее через автоэекодер и получать реконструкцию, а попробуем создать что-то НОВОЕ\n","\n","Давайте возьмем и подсунем декодеру какие-нибудь сгенерированные нами векторы (например, из нормального распределения) и посмотрим на результат реконструкции декодера:\n","\n","__Подсказка:__Е сли вместо лиц у вас выводится непонятно что, попробуйте посмотреть, как выглядят латентные векторы картинок из датасета. Так как в обучении нейронных сетей есть определенная доля рандома, векторы латентного слоя могут быть распределены НЕ как `np.random.randn(25, <latent_space_dim>)`. А чтобы у нас получались лица при запихивании вектора декодеру, вектор должен быть распределен так же, как латентные векторы реальных фоток. Так что в таком случае придется рандом немного подогнать."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"8IZykARRYY7u"},"source":["# сгенерируем 25 рандомных векторов размера latent_space\n","z = np.random.randn(25, <latent_space_dim>)\n","output = <скормите z декодеру>\n","<выведите тут полученные картинки>"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"Ey8dD9s0YY7w"},"source":["## Time to make fun! (4 балла)\n","\n","Давайте научимся пририсовывать людям улыбки =)"]},{"cell_type":"markdown","metadata":{"id":"i1v-8WwuYY7w"},"source":["<img src=\"https://i.imgur.com/tOE9rDK.png\" alt=\"linear\" width=\"700\" height=\"400\">"]},{"cell_type":"markdown","metadata":{"id":"eGE0M2GDYY7x"},"source":["План такой:\n","\n","1. Нужно выделить \"вектор улыбки\": для этого нужно из выборки изображений найти несколько (~15) людей с улыбками и столько же без.\n","\n","Найти людей с улыбками вам поможет файл с описанием датасета, скачанный вместе с датасетом. В нем указаны имена картинок и присутствубщие атрибуты (улыбки, очки...)\n","\n","2. Вычислить латентный вектор для всех улыбающихся людей (прогнать их через encoder) и то же для всех грустненьких\n","\n","3. Вычислить, собственно, вектор улыбки -- посчитать разность между средним латентным вектором улыбающихся людей и средним латентным вектором грустных людей\n","\n","4. А теперь приделаем улыбку грустному человеку: добавим полученный в пункте 3 вектор к латентному вектору грустного человека и прогоним полученный вектор через decoder. Получим того же человека, но уже не грустненького!"]},{"cell_type":"code","metadata":{"id":"f1oBX9EeYY7x"},"source":["<ваш код здесь>"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bXI6jprOYY7z"},"source":["Вуаля! Вы восхитительны!"]},{"cell_type":"markdown","metadata":{"id":"E2UAf0bpYY70"},"source":["Теперь вы можете пририсовывать людям не только улыбки, но и много чего другого -- закрывать/открывать глаза, пририсовывать очки... в общем, все, на что хватит фантазии и на что есть атрибуты в `all_attrs`:)"]},{"cell_type":"markdown","metadata":{"id":"QQnEGmknYY71"},"source":["# Часть 2: Variational Autoencoder (10 баллов) "]},{"cell_type":"markdown","metadata":{"id":"bWQNRjJq2uTz"},"source":["Займемся обучением вариационных автоэнкодеров — проапгрейженной версии AE. Обучать будем на датасете MNIST, содержащем написанные от руки цифры от 0 до 9"]},{"cell_type":"code","metadata":{"id":"qBXXr9njByYC"},"source":["batch_size = 32\n","# MNIST Dataset\n","train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n","test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n","\n","# Data Loader (Input Pipeline)\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2rHphW5l8Wgi"},"source":["## 2.1 Архитектура модели и обучение (2 балла)\n","\n","Реализуем VAE. Архитектуру (conv, fully-connected, ReLu, etc) можете выбирать сами. Рекомендуем пользоваться более сложными моделями, чем та, что была на семинаре:) Экспериментируйте!"]},{"cell_type":"code","metadata":{"id":"IoNVT5tYYY74"},"source":["class VAE(nn.Module):\n","    def __init__(self):\n","        <определите архитектуры encoder и decoder\n","        помните, у encoder должны быть два \"хвоста\", \n","        т.е. encoder должен кодировать картинку в 2 переменные -- mu и logsigma>\n","\n","    def encode(self, x):\n","        <реализуйте forward проход энкодера\n","        в качестве ваозвращаемых переменных -- mu и logsigma>\n","        \n","        return mu, logsigma\n","    \n","    def gaussian_sampler(self, mu, logsigma):\n","        if self.training:\n","            <засемплируйте латентный вектор из нормального распределения с параметрами mu и sigma>\n","        else:\n","            # на инференсе возвращаем не случайный вектор из нормального распределения, а центральный -- mu. \n","            # на инференсе выход автоэнкодера должен быть детерминирован.\n","            return mu\n","    \n","    def decode(self, z):\n","        <реализуйте forward проход декодера\n","        в качестве возвращаемой переменной -- reconstruction>\n","        \n","        return reconstruction\n","\n","    def forward(self, x):\n","        <используя encode и decode, реализуйте forward проход автоэнкодера\n","        в качестве ваозвращаемых переменных -- mu, logsigma и reconstruction>\n","        return mu, logsigma, reconstruction"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hAB77d-PYY76"},"source":["Определим лосс и его компоненты для VAE:"]},{"cell_type":"markdown","metadata":{"id":"UxJrkXGQo5bp"},"source":["Надеюсь, вы уже прочитали материал в towardsdatascience (или еще где-то) про VAE и знаете, что лосс у VAE состоит из двух частей: KL и log-likelihood.\n","\n","Общий лосс будет выглядеть так:\n","\n","$$\\mathcal{L} = -D_{KL}(q_{\\phi}(z|x)||p(z)) + \\log p_{\\theta}(x|z)$$\n","\n","Формула для KL-дивергенции:\n","\n","$$D_{KL} = -\\frac{1}{2}\\sum_{i=1}^{dimZ}(1+log(\\sigma_i^2)-\\mu_i^2-\\sigma_i^2)$$\n","\n","В качестве log-likelihood возьмем привычную нам кросс-энтропию."]},{"cell_type":"code","metadata":{"id":"ac5ey7uIYY77"},"source":["def KL_divergence(mu, logsigma):\n","    \"\"\"\n","    часть функции потерь, которая отвечает за \"близость\" латентных представлений разных людей\n","    \"\"\"\n","    loss = <напишите код для KL-дивергенции, пользуясь формулой выше>\n","    return \n","\n","def log_likelihood(x, reconstruction):\n","    \"\"\"\n","    часть функции потерь, которая отвечает за качество реконструкции (как mse в обычном autoencoder)\n","    \"\"\"\n","    loss = <binary cross-entropy>\n","    return loss(reconstruction, x)\n","\n","def loss_vae(x, mu, logsigma, reconstruction):\n","    return <соедините тут две компоненты лосса. Mind the sign!>"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZPJQu70eYY79"},"source":["И обучим модель:"]},{"cell_type":"code","metadata":{"id":"dtCjfqXdYY79"},"source":["criterion = loss_vae\n","\n","autoencoder = VAE()\n","\n","optimizer = <Ваш любимый оптимизатор>"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"rY1khca6YY7_"},"source":["<обучите модель на датасете MNIST>"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"scrolled":true,"id":"SkxW_8fkYY8B"},"source":["Давайте посмотрим, как наш тренированный VAE кодирует и восстанавливает картинки:"]},{"cell_type":"code","metadata":{"id":"4Jd3BWM_YY8C"},"source":["< тут Ваш код: выведите первые Х картинок и их реконструкций из val выборки на экран>"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PQXYIXjoYY8F"},"source":["Давайте попробуем проделать для VAE то же, что и с обычным автоэнкодером -- подсунуть decoder'у из VAE случайные векторы из нормального распределения и посмотреть, какие картинки получаются:"]},{"cell_type":"code","metadata":{"id":"bOhhH-osYY8G"},"source":["# вспомните про замечание из этого же пункта обычного AE про распределение латентных переменных\n","z = np.array([np.random.normal(0, 1, 100) for i in range(10)])\n","output = <скормите z декодеру>\n","<выведите тут полученные картинки>"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nzt-ENxCr6ul"},"source":["## 2.2. Latent Representation (2 балла)"]},{"cell_type":"markdown","metadata":{"id":"uIWy670xr-Uv"},"source":["Давайте посмотрим, как латентные векторы картинок лиц выглядят в пространстве.\n","Ваша задача -- изобразить латентные векторы картинок точками в двумерном просторанстве. \n","\n","Это позволит оценить, насколько плотно распределены латентные векторы изображений цифр в пространстве. \n","\n","Плюс давайте сделаем такую вещь: покрасим точки, которые соответствуют картинкам каждой цифры, в свой отдельный цвет\n","\n","Подсказка: красить -- это просто =) У plt.scatter есть параметр c (color), см. в документации.\n","\n","\n","Итак, план:\n","1. Получить латентные представления картинок тестового датасета\n","2. С помощтю `TSNE` (есть в `sklearn`) сжать эти представления до размерности 2 (чтобы можно было их визуализировать точками в пространстве)\n","3. Визуализировать полученные двумерные представления с помощью `matplotlib.scatter`, покрасить разными цветами точки, соответствующие картинкам разных цифр."]},{"cell_type":"code","metadata":{"id":"Bk94C6mCsx9c"},"source":["<ваш код получения латентных представлений, применения TSNE и визуализации>"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ifxhsvPss5h_"},"source":["Что вы думаете о виде латентного представления?"]},{"cell_type":"markdown","metadata":{"id":"ESPBHrL3YY8H"},"source":["__Congrats v2.0!__"]},{"cell_type":"markdown","metadata":{"id":"yIYuKFwijN2U"},"source":["## 2.3. Conditional VAE (6 баллов)\n"]},{"cell_type":"markdown","metadata":{"id":"c5l8Bu1RPjUx"},"source":["Мы уже научились обучать обычный AE на датасете картинок и получать новые картинки, используя генерацию шума и декодер. \n","Давайте теперь допустим, что мы обучили AE на датасете MNIST и теперь хотим генерировать новые картинки с числами с помощью декодера (как выше мы генерили рандомные лица). \n","И вот нам понадобилось сгенерировать цифру 8, и мы подставляем разные варианты шума, но восьмерка никак не генерится:(\n","\n","Хотелось бы добавить к нашему AE функцию \"выдай мне рандомное число из вот этого вот класса\", где классов десять (цифры от 0 до 9 образуют десять классов).  Conditional AE — так называется вид автоэнкодера, который предоставляет такую возможность. Ну, название \"conditional\" уже говорит само за себя.\n","\n","И в этой части задания мы научимся такие обучать."]},{"cell_type":"markdown","metadata":{"id":"0j8zNIwKPY-6"},"source":["### Архитектура\n","\n","На картинке ниже представлена архитектура простого Conditional VAE.\n","\n","По сути, единственное отличие от обычного -- это то, что мы вместе с картинкой в первом слое энкодера и декодера передаем еще информацию о классе картинки. \n","\n","То есть, в первый (входной) слой энкодера подается конкатенация картинки и информации о классе (например, вектора из девяти нулей и одной единицы). В первый слой декодера подается конкатенация латентного вектора и информации о классе."]},{"cell_type":"markdown","metadata":{"id":"Y6YloFEAPeM4"},"source":["\n","![alt text](https://sun9-63.userapi.com/impg/Mh1akf7mfpNoprrSWsPOouazSmTPMazYYF49Tw/djoHNw_9KVA.jpg?size=1175x642&quality=96&sign=e88baec5f9bb91c8443fba31dcf0a4df&type=album)\n","\n","![alt text](https://sun9-73.userapi.com/impg/UDuloLNKhzTBYAKewgxke5-YPsAKyGOqA-qCRg/MnyCavJidxM.jpg?size=1229x651&quality=96&sign=f2d21bfacc1c5755b76868dc4cfef39c&type=album)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hxg2tDSfRbLF"},"source":["На всякий случай: это VAE, то есть, latent у него все еще состоит из mu и sigma"]},{"cell_type":"markdown","metadata":{"id":"GpFbSXLaPrm1"},"source":["Таким образом, при генерации новой рандомной картинки мы должны будем передать декодеру сконкатенированные латентный вектор и класс картинки."]},{"cell_type":"markdown","metadata":{"id":"cX0zxklMPwI2"},"source":["P.S. Также можно передавать класс картинки не только в первый слой, но и в каждый слой сети. То есть на каждом слое конкатенировать выход из предыдущего слоя и информацию о классе."]},{"cell_type":"code","metadata":{"id":"ar701cHOkDKS"},"source":["class CVAE(nn.Module):\n","    def __init__(self):\n","        <определите архитектуры encoder и decoder\n","        помните, у encoder должны быть два \"хвоста\", \n","        т.е. encoder должен кодировать картинку в 2 переменные -- mu и logsigma>\n","\n","    def encode(self, x, class_num):\n","        <реализуйте forward проход энкодера\n","        в качестве ваозвращаемых переменных -- mu, logsigma и класс картинки>\n","        \n","        return mu, logsigma, class_num\n","    \n","    def gaussian_sampler(self, mu, logsigma):\n","        if self.training:\n","            <засемплируйте латентный вектор из нормального распределения с параметрами mu и sigma>\n","        else:\n","            # на инференсе возвращаем не случайный вектор из нормального распределения, а центральный -- mu. \n","            # на инференсе выход автоэнкодера должен быть детерминирован.\n","            return mu\n","    \n","    def decode(self, z, class_num):\n","        <реализуйте forward проход декодера\n","        в качестве возвращаемой переменной -- reconstruction>\n","        \n","        return reconstruction\n","\n","    def forward(self, x):\n","        <используя encode и decode, реализуйте forward проход автоэнкодера\n","        в качестве ваозвращаемых переменных -- mu, logsigma и reconstruction>\n","        return mu, logsigma, reconstruction"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VoMw-IFyP5A2"},"source":["### Sampling\n"]},{"cell_type":"markdown","metadata":{"id":"qe1zWyZHkLV2"},"source":["Тут мы будем сэмплировать из CVAE. Это прикольнее, чем сэмплировать из простого AE/VAE: тут можно взять один и тот же латентный вектор и попросить CVAE восстановить из него картинки разных классов!\n","Для MNIST вы можете попросить CVAE восстановить из одного латентного вектора, например, картинки цифры 5 и 7."]},{"cell_type":"code","metadata":{"id":"A0SQIhvNP9Dr"},"source":["<тут нужно научиться сэмплировать из декодера цифры определенного класса>"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nAWBu8rzQBgQ"},"source":["Splendid! Вы великолепны!\n"]},{"cell_type":"markdown","metadata":{"id":"Rt2S77cm3O1v"},"source":["### Latent Representations"]},{"cell_type":"markdown","metadata":{"id":"Nt7x8Ek_rHTE"},"source":["Давайте посмотрим, как выглядит латентное пространство картинок в CVAE и сравним с картинкой для VAE =)\n","\n","Опять же, нужно покрасить точки в разные цвета в зависимости от класса."]},{"cell_type":"code","metadata":{"id":"LSCYK7sH3KEc"},"source":["<ваш код получения латентных представлений, применения TSNE и визуализации>"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ET8IELWu3Z2c"},"source":["Что вы думаете насчет этой картинки? Отличается от картинки для VAE?"]},{"cell_type":"markdown","metadata":{"id":"SWkqHjvTCD_8"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"KN3D_k5W_WZz"},"source":["# BONUS 1: Denoising\n","\n","## Внимание! За бонусы доп. баллы не ставятся, но вы можете сделать их для себя."]},{"cell_type":"markdown","metadata":{"id":"12a1jkpkCsIU"},"source":["У автоэнкодеров, кроме сжатия и генерации изображений, есть другие практические применения. Про одно из них эта бонусная часть задания."]},{"cell_type":"markdown","metadata":{"id":"v8EN-8jlCtmd"},"source":["Автоэнкодеры могут быть использованы для избавления от шума на фотографиях (denoising). Для этого их нужно обучить специальным образом: input картинка будет зашумленной, а выдавать автоэнкодер должен будет картинку без шума. \n","То есть, loss-функция AE останется той же (MSE между реальной картинкой и выданной), а на вход автоэнкодеру будет подаваться зашумленная картинка."]},{"cell_type":"markdown","metadata":{"id":"j1OJg6jhlaZl"},"source":["<a href=\"https://ibb.co/YbRJ1nZ\"><img src=\"https://i.ibb.co/0QD164t/Screen-Shot-2020-06-04-at-4-49-50-PM.png\" alt=\"Screen-Shot-2020-06-04-at-4-49-50-PM\" border=\"0\"></a>"]},{"cell_type":"markdown","metadata":{"id":"ysI0BCuRDbvm"},"source":["Для этого нужно взять ваш любимый датасет (датасет лиц из первой части этого задания или любой другой) и сделать копию этого датасета с шумом. \n","\n","В питоне шум можно добавить так:"]},{"cell_type":"code","metadata":{"id":"X5e746iVDgSm"},"source":["noise_factor = 0.5\n","X_noisy = X + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X.shape) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9fSPkXMtDpd5"},"source":["<тут ваш код обучения автоэнкодера на зашумленных картинках. Не забудтье разбить на train/test!>"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B03NQ_sKDvg2"},"source":["<тут проверка, как AE убирает щум с тестовых картинок. Надеюсь, все получилось =)>"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-NDiCPYLm2bY"},"source":["# BONUS 2: Image Retrieval\n","\n","## Внимание! За бонусы доп. баллы не ставятся, но вы можете сделать их для себя."]},{"cell_type":"markdown","metadata":{"id":"xao_27WMm7AL"},"source":["Давайте представим, что весь наш тренировочный датасет -- это большая база данных людей. И вот мы получили картинку лица какого-то человека с уличной камеры наблюдения (у нас это картинка из тестового датасета) и хотим понять, что это за человек. Что нам делать? Правильно -- берем наш VAE, кодируем картинку в латентное представление и ищем среди латентныз представлений лиц нашей базы самые ближайшие!"]},{"cell_type":"markdown","metadata":{"id":"Y__bdS23ndeY"},"source":["План:\n","\n","1. Получаем латентные представления всех лиц тренировочного датасета\n","2. Обучаем на них LSHForest `(sklearn.neighbors.LSHForest)`, например, с `n_estimators=50`\n","3. Берем картинку из тестового датасета, с помощью VAE получаем ее латентный вектор\n","4. Ищем с помощью обученного LSHForest ближайшие из латентных представлений тренировочной базы\n","5. Находим лица тренировочного датасета, которым соответствуют ближайшие латентные представления, визуализируем!"]},{"cell_type":"markdown","metadata":{"id":"IksC2ucIoND-"},"source":["Немного кода вам в помощь: (feel free to delete everything and write your own)"]},{"cell_type":"code","metadata":{"id":"hK0YpLMRoEa0"},"source":["codes = <поучите латентные представления картинок из трейна>"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KisDrgZdoWdt"},"source":["# обучаем LSHForest\n","from sklearn.neighbors import LSHForest\n","lshf = LSHForest(n_estimators=50).fit(codes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y_S5zPb5obam"},"source":["def get_similar(image, n_neighbors=5):\n","  # функция, которая берет тестовый image и с помощью метода kneighbours у LSHForest ищет ближайшие векторы\n","  # прогоняет векторы через декодер и получает картинки ближайших людей\n","\n","  code = <получение латентного представления image>\n","    \n","  (distances,),(idx,) = lshf.kneighbors(code, n_neighbors=n_neighbors)\n","\n","  return distances, X_train[idx]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t2kjV5wupLP_"},"source":["def show_similar(image):\n","\n","  # функция, которая принимает тестовый image, ищет ближайшие к нему и визуализирует результат\n","    \n","    distances,neighbors = get_similar(image,n_neighbors=11)\n","    \n","    plt.figure(figsize=[8,6])\n","    plt.subplot(3,4,1)\n","    plt.imshow(image.cpu().numpy().transpose([1,2,0]))\n","    plt.title(\"Original image\")\n","    \n","    for i in range(11):\n","        plt.subplot(3,4,i+2)\n","        plt.imshow(neighbors[i].cpu().numpy().transpose([1,2,0]))\n","        plt.title(\"Dist=%.3f\"%distances[i])\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w3Ja1UNf_oJq"},"source":["<тут выведите самые похожие лица к какому-нибудь лицу из тестовой части датасета>"],"execution_count":null,"outputs":[]}]}