{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Part2, Seminar 2-blanks.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"colab_type":"text","id":"C1IMoqFPZ-_E"},"cell_type":"markdown","source":["<!-- #  Семинар: Рекурентные нейронные сети. -->\n","\n","В данной работе вам предлагается посмотреть на всю мощь рекурентных нейронных сетей решив небольшую задачу. \n","\n"," Предлагаю решить вам задачу расшифровки сообщения с помощью RNN. \n"," Представьте, что вам даны сообщения зашифрованные с помощью шифра Цезаря, являющимся одним из самый простых шифров в криптографии.\n"," "]},{"metadata":{"colab_type":"text","id":"1YNGZnxua61K"},"cell_type":"markdown","source":["Шифр цезаря работает следующим образом: каждя буква \n","исходного алфавита сдвигается на K символов вправо: \n","\n","Пусть нам дано сообщение: message=\"RNN IS NOT AI\", тогда наше шифрование выполняющиеся по правилу f, с K=2, даст нам результат:\n","f(message, K) = TPPAKUAPQVACK\n","\n","Для удобство можно взять символы только одного регистра в нашей имплементации, и сказать, что все буквы не английского алфавита будут отмечены как прочерк \"-\"."]},{"metadata":{"colab_type":"code","id":"zPH9sAiDe77A","colab":{}},"cell_type":"code","source":["import random\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"xoZVmCuEdO7Y","colab":{}},"cell_type":"code","source":["# Определим ключ и словарь\n","key = 2\n","vocab = [char for char in ' -ABCDEFGHIJKLMNOPQRSTUVWXYZ']"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"yAsJDtSsGgOV","outputId":"26271074-0e8a-43b4-8e1a-7320e080e3bd","executionInfo":{"status":"ok","timestamp":1553354624454,"user_tz":-180,"elapsed":1189,"user":{"displayName":"Агата Шевченко","photoUrl":"","userId":"17622558543368228675"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# Напишем функцию, которая делает \n","def encrypt(text, key):\n","    \"\"\"Returns the encrypted form of 'text'.\"\"\"\n","    indexes = [vocab.index(char) for char in text]\n","    encrypted_indexes = [(idx + key) % len(vocab) for idx in indexes]\n","    encrypted_chars = [vocab[idx] for idx in encrypted_indexes]\n","    encrypted = ''.join(encrypted_chars)\n","    return encrypted\n","\n","print(encrypt('RNN IS NOT AI', key))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["TPPAKUAPQVACK\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"Z6QNnDJPdxXC"},"cell_type":"markdown","source":["Теперь нам необходимо нагенерировать датасет для решения задачи обучения с учителем. Нашим датасетом может быть случайно зашифрованные фразы, и тогда его структура будет следующей:\n","message --- encrypted message\n","\n","Это пример параллельного корпуса из НЛП.\n","\n","Но нам необходимо представить каждую букву в виде ее номера в словаре, чтобы далее воспользоваться Embedding слоем. \n","\n","Для простоты давайте допустим, что все строки имеют одинаковую длину seq_len"]},{"metadata":{"colab_type":"code","id":"Te9ugR1AIjEw","colab":{}},"cell_type":"code","source":["num_examples = 256 # размер датасета\n","seq_len = 18 # максимальная длина строки\n","\n","\n","def encrypted_dataset(dataset_len, k):\n","    \"\"\"\n","    Return: List(Tuple(Tensor encrypted, Tensor source))\n","    \"\"\"\n","    dataset = []\n","    for x in range(dataset_len):\n","        random_message  = ''.join([random.choice(vocab) for x in range(seq_len)])\n","        encrypt_random_message = encrypt(''.join(random_message), k)\n","        src = [vocab.index(x) for x in random_message]\n","        tgt = [vocab.index(x) for x in encrypt_random_message]\n","        dataset.append([torch.tensor(tgt), torch.tensor(src)])\n","    return dataset"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0RUxQqTCGUtx","colab_type":"text"},"cell_type":"markdown","source":["**Pytorch RNN:**\n","$$h_t = \\text{tanh}(w_{ih} x_t + b_{ih} + w_{hh} h_{(t-1)} + b_{hh})$$\n","\n","**where : $h_t$ is the hidden state at time $t$, $x_t$ is\n","    the input at time $t$, and $h_{(t-1)}$ is the hidden state of the\n","    previous layer at time $t-1$ or the initial hidden state at time $0$.**\n","    \n","Args: \n","\n","        input_size: The number of expected features in the input $x$\n","        hidden_size: The number of features in the hidden state $h$\n","        num_layers: Number of recurrent layers. E.g., setting"]},{"metadata":{"colab_type":"code","id":"03y7VB9QLorQ","colab":{}},"cell_type":"code","source":["class Decipher(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, \n","                 rnn_type='simple'):\n","        \"\"\"\n","        :params: int vocab_size \n","        :params: int embedding_dim\n","        :params\n","        \"\"\"\n","        super(Decipher, self).__init__()\n","        self.embed = nn.Embedding(vocab_size, embedding_dim)\n","        if rnn_type == 'simple':\n","            self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers = 2)\n","         \n","        self.fc = nn.Linear(hidden_dim, vocab_size)\n","        self.initial_hidden = torch.zeros(2, 1, hidden_dim)\n","\n","        \n","    def forward(self, cipher):\n","        # CHECK INPUT SIZE\n","        # Unsqueeze 1 dimension for batches\n","        embd_x = self.embed(cipher).unsqueeze(1)\n","        out_rnn, hidden = self.rnn(embd_x, self.initial_hidden)\n","        # Apply the affine transform and transpose output in appropriate way\n","        # because you want to get the softmax on vocabulary dimension\n","        # in order to get probability of every letter\n","        return self.fc(out_rnn).transpose(1, 2)\n","      "],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"5rnb-2qIIjM7","colab":{}},"cell_type":"code","source":["# определим параметры нашей модели\n","embedding_dim = 5\n","hidden_dim = 10\n","vocab_size = len(vocab) \n","lr = 1e-3\n","\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","# Инициализируйте модель\n","model = Decipher(vocab_size, embedding_dim, hidden_dim)\n","\n","# Инициализируйте оптимизатор: рекомендуется Adam\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n","\n","num_epochs = 10"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"yZO1uR7fIjQ9","colab":{"base_uri":"https://localhost:8080/","height":527},"outputId":"870713d3-1709-4e10-a32b-04408af00e65","executionInfo":{"status":"ok","timestamp":1553359733380,"user_tz":-180,"elapsed":14680,"user":{"displayName":"Агата Шевченко","photoUrl":"","userId":"17622558543368228675"}}},"cell_type":"code","source":["k = 10\n","for x in range(num_epochs):\n","    print('Epoch: {}'.format(x))\n","    for encrypted, original in encrypted_dataset(num_examples, k):\n","\n","        scores = model(encrypted)\n","        original = original.unsqueeze(1)\n","        # Calculate loss\n","        loss = criterion(scores, original)\n","        # Zero grads\n","        optimizer.zero_grad()\n","        # Backpropagate\n","        loss.backward()\n","        # Update weights\n","        optimizer.step()\n","    print('Loss: {:6.4f}'.format(loss.item()))\n","\n","    with torch.no_grad():\n","        matches, total = 0, 0\n","        for encrypted, original in encrypted_dataset(num_examples, k):\n","            # Compute a softmax over the outputs\n","            predictions = F.softmax(model(encrypted), 1)\n","            # Choose the character with the maximum probability (greedy decoding)\n","            _, batch_out = predictions.max(dim=1)\n","            # Remove batch\n","            batch_out = batch_out.squeeze(1)\n","            # Calculate accuracy\n","            matches += torch.eq(batch_out, original).sum().item()\n","            total += torch.numel(batch_out)\n","        accuracy = matches / total\n","        print('Accuracy: {:4.2f}%'.format(accuracy * 100))"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Epoch: 0\n","Loss: 2.7919\n","Accuracy: 29.14%\n","Epoch: 1\n","Loss: 1.8439\n","Accuracy: 58.40%\n","Epoch: 2\n","Loss: 1.2827\n","Accuracy: 75.13%\n","Epoch: 3\n","Loss: 0.8501\n","Accuracy: 82.81%\n","Epoch: 4\n","Loss: 0.7013\n","Accuracy: 89.58%\n","Epoch: 5\n","Loss: 0.5837\n","Accuracy: 89.89%\n","Epoch: 6\n","Loss: 0.3777\n","Accuracy: 92.21%\n","Epoch: 7\n","Loss: 0.3410\n","Accuracy: 92.49%\n","Epoch: 8\n","Loss: 0.3734\n","Accuracy: 97.79%\n","Epoch: 9\n","Loss: 0.2824\n","Accuracy: 100.00%\n"],"name":"stdout"}]},{"metadata":{"colab_type":"code","id":"krOVEf61IjUJ","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"u9C5aMv1GUu9","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"-8E9YhMsGUvL","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}