{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-DOaQhWqbSVY"
   },
   "source": [
    "<img src=\"https://s8.hostingkartinok.com/uploads/images/2018/08/308b49fcfbc619d629fe4604bceb67ac.jpg\" width=500, height=450>\n",
    "<h3 style=\"text-align: center;\"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o0mOUchLPMAR"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xPszMQhYbSVb"
   },
   "source": [
    "<h2 style=\"text-align: center;\"><b>Word embeddings</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d8r43Kl9_r3t"
   },
   "source": [
    "Libraries we will need today (actually 2 first of them you'll need afterwards, too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade nltk gensim bokeh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kZTnWfoEbryt"
   },
   "source": [
    "**NLTK** (Natural Language Toolkit) -- a library with many features to work with natural language (e.g. lemmatization, tokenization, etc.)\n",
    "\n",
    "https://www.nltk.org\n",
    "\n",
    "We wull use it for tokenization for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U7vuP82fbdpx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Which', 'brand', 'should', 'go', 'with', 'the', 'GTX', '960', 'graphic', 'card', ',', 'MSI', ',', 'Zotac', 'or', 'ASUS', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "quora = list(open(\"./quora.txt\"))\n",
    "# look at 10th question from the dataset\n",
    "print(tokenizer.tokenize(quora[10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q72RDmVkBCC3"
   },
   "source": [
    "Let's tokenize all the texts from quora.txt file and get array of arrays of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Z6xzhsPc1jZ"
   },
   "outputs": [],
   "source": [
    "quora_tokenized = [tokenizer.tokenize(line.lower()) for line in quora]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6aHSIIThdSIF"
   },
   "source": [
    "There are many different models to train to get word embeddings (Word2Vec, GloVe, FastText, etc). For now let's try to train word2vec.\n",
    "\n",
    "Library **gensim** will help us with that -- it provides models for Word2Vec training and further vectors' investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iwPQDCXWdBrW"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# define a model (just like torch model) with parameters\n",
    "# and train it on our data\n",
    "model = Word2Vec(quora_tokenized, # data for model to train on\n",
    "                 size=32,         # embedding vector size\n",
    "                 min_count=5,     # consider words that occured at least 5 times\n",
    "                 window=3).wv     # define context as a 3-word window around the target word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YfzWThklBiJo"
   },
   "source": [
    "Now we have trained model and can play with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7xirOxjPdKAd"
   },
   "outputs": [],
   "source": [
    "# we can get a vector for any word in our vocabulary\n",
    "model.get_vector('tv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IFy6mJsXdhXE"
   },
   "outputs": [],
   "source": [
    "# and get words that have most similar vectors to vector of the given word\n",
    "model.most_similar('tv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k-CSminTez1z"
   },
   "source": [
    "Nice!\n",
    "\n",
    "But actually for big tasks as text classifications or something like we would like to have better vectors. If we started training them by ourselves, we would need a LARGE text corpus and LARGE amount of memory and time. But as we are poor students by now, and need to do our homework till the next week, we don't have it all by now.\n",
    "\n",
    "That is why there are pre-trained models or even pre-computed word vectors collections!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xoUBV_UMfRIu"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "model = api.load('glove-twitter-100') # list of available models: https://github.com/RaRe-Technologies/gensim-data#models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z30sJahPf2Y2"
   },
   "outputs": [],
   "source": [
    "# who is a person who has money and related to coding, but is really stupid?\n",
    "# (it's how a model trained on text corpus thinks, not me)\n",
    "model.most_similar(positive=[\"coder\", \"money\"], negative=[\"brain\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2aJOipoChANV"
   },
   "source": [
    "Also using gensim we can load pre-trained vectors from vectors file. We can for example load pre-trained google word2vec vectors:\n",
    "\n",
    "http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PwH86KytheiP"
   },
   "outputs": [],
   "source": [
    "# from gensim.models import KeyedVectors\n",
    "# model = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ntBcysEnh-DP"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cvVwJWM2CGHW"
   },
   "source": [
    "So let's visualize our embeddings on 2-d space. \n",
    "\n",
    "What we need is:\n",
    "1. get embeddings for a batch of words from the gensim model\n",
    "2. reduce dimensionality of those embeddings from 100-dim to 2-dim vectors\n",
    "3. normalize those vectors\n",
    "4. draw it on a plane!\n",
    "\n",
    "So let's do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aMn9AdhxhrOn"
   },
   "outputs": [],
   "source": [
    "# let's get some most popular words from model:\n",
    "n_words = 1000\n",
    "words = sorted(model.vocab.keys(), \n",
    "               key=lambda word: model.vocab[word].count, # sort by number of word occurencies\n",
    "               reverse=True)[:n_words]\n",
    "\n",
    "print(words[::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FStm52npiL3d"
   },
   "outputs": [],
   "source": [
    "# get embeddings for those words\n",
    "word_vectors = [model.get_vector(word) for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z0M2b2Weim4k"
   },
   "source": [
    "For vectors visualization we need to reduce dimensionality of vectors to 2 (or 3). We'll use **PCA** for that\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p3F9_sQmiaPx"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(word_vectors)\n",
    "word_vectors_pca = pca.transform(word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UXC_Tzz9kkGL"
   },
   "source": [
    "Also let's normalize vectors we got from PCA for better visibility\n",
    "\n",
    "We already did normalization by hand in one of the first homeworks in our course, but for now let's use sklearn api:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7AgGtiMQibR3"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler().fit(word_vectors_pca)\n",
    "word_vectors_pca = ss.transform(word_vectors_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dd1aScI_DNSP"
   },
   "source": [
    "Finally we are to draw an (interactive!) space of embeddings, using function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5YL3o6g2jsql"
   },
   "outputs": [],
   "source": [
    "import bokeh.models as bm, bokeh.plotting as pl\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
    "                 width=600, height=400, show=True, **kwargs):\n",
    "    if isinstance(color, str): color = [color] * len(x)\n",
    "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
    "\n",
    "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
    "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
    "\n",
    "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
    "    if show: pl.show(fig)\n",
    "    return fig\n",
    "\n",
    "\n",
    "draw_vectors(word_vectors_pca[:, 0], word_vectors_pca[:, 1], token=words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qv1hIhRoFcHC"
   },
   "source": [
    "Well, as we can see, this space does not look perfect, sometimes we see (it seems) different words grouped together.\n",
    "\n",
    "That happend mostly because our way to reduce dimensionality of vectors (PCA)  is not perfect. Let's try to use different algorithm for doing this: TSNE.\n",
    "\n",
    "TSNE is in some way creates embeddings itself, so we may hope that it will work better)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gHwUk0TBomVz"
   },
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html\n",
    "\n",
    "https://distill.pub/2016/misread-tsne/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qpSg31n_nlHO"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=50)\n",
    "tsne.fit(word_vectors)\n",
    "word_vectors_tsne = tsne.transform(word_vectors)\n",
    "\n",
    "ss = StandardScaler().fit(word_vectors_tsne)\n",
    "word_vectors_tsne = ss.transform(word_vectors_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TzBpTdLFnosK"
   },
   "outputs": [],
   "source": [
    "output_notebook()\n",
    "draw_vectors(word_vectors_tsne[:, 0], word_vectors_tsne[:, 1], token=words)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[seminar]embeddings.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
