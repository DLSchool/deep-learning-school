{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"align: center;\"><img align=center src=\"https://s8.hostingkartinok.com/uploads/images/2018/08/308b49fcfbc619d629fe4604bceb67ac.jpg\"  width=400 height=300></p>\n",
    "\n",
    "<h3 style=\"text-align: center;\"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: center;\"><b>Градиентный спуск и линейные модели: домашнее задание</b></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from matplotlib import pylab, gridspec, pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Градиентный спуск: повторение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим функцию от двух переменных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x1, x2):\n",
    "    return np.sin(x1)**2 + np.sin(x2) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Reminder:***  \n",
    "Что мы хотим? Мы хотим найти минимум этой функции (в машинному обучении мы обычно хотим найти минимум **функции потерь**, например, MSE), а точнее найти x1 и x2 такие, что при них значение f(x1,x2) минимально, то есть *точку экстремума*.  \n",
    "Как мы будем искать эту точку? Используем методы оптимизации (=минимизации в нашем случае). Одним из таких методов и является градиентный спуск. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем функцию, которая будет осуществлять градиентный спуск для функции f:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_descent(lr, num_iter=100):\n",
    "    \"\"\"\n",
    "    функция, которая реализует градиентный спуск в минимум для функции f от двух переменных. \n",
    "        param lr: learning rate алгоритма\n",
    "        param num_iter: количество итераций градиентного спуска\n",
    "    \"\"\"\n",
    "    global f\n",
    "    # в начале градиентного спуска инициализируем значения x1 и x2 rкакими-нибудь числами\n",
    "    cur_x1, cur_x2 = 1.5, -1\n",
    "    # будем сохранять значения аргументов и значений функции в процессе град. спуска в переменную states\n",
    "    steps = []\n",
    "    \n",
    "    # итерация цикла -- шаг градиентнго спуска\n",
    "    for iter_num in range(num_iter):\n",
    "        steps.append([cur_x1, cur_x2, f(cur_x1, cur_x2)])\n",
    "        \n",
    "        # чтобы обновить значения cur_x1 и cur_x2, как мы помним с последнего занятия, \n",
    "        # нужно найти производные (градиенты) функции f по этим переменным.\n",
    "        grad_x1 = <тут Ваш код>\n",
    "        grad_x2 = <тут Ваш код>\n",
    "                 \n",
    "        # после того, как посчитаны производные, можно обновить веса. \n",
    "        # не забудьте про lr!\n",
    "        cur_x1 -= <тут Ваш код>\n",
    "        cur_x2 -= <тут Ваш код>\n",
    "    return np.array(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим градиентный спуск:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "steps = grad_descent(lr=0.5, num_iter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем точки градиентного спуска на 3D-графике нашей функции. Звездочками будут обозначены точки (тройки x1, x2, f(x1, x2)), по которым Ваш алгоритм градиентного спуска двигался к минимуму.\n",
    "\n",
    "(Для того, чтобы написовать этот график, мы и сохраняли значения cur_x1, cur_x2, f(cur_x1, cur_x2) в steps в процессе спуска)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если у Вас правильно написана функция grad_descent, то звездочки на картинке должны сходиться к одной из точку минимума функции. Вы можете менять начальные приближения алгоритма, значения lr и num_iter и получать разные результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %matplotlib osx\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "\n",
    "path = []\n",
    "\n",
    "X, Y = np.meshgrid(np.linspace(-3, 3, 100), np.linspace(-3, 3, 100))\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "ax.plot(xs=steps[:, 0], ys=steps[:, 1], zs=steps[:, 2], marker='*', markersize=20,\n",
    "                markerfacecolor='y', lw=3, c='black')\n",
    "\n",
    "ax.plot_surface(X, Y, f(X, Y), cmap=cm.coolwarm)\n",
    "ax.set_zlim(0, 5)\n",
    "ax.view_init(elev=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ответ на КР  №1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустите Вашу функцию grad_descent c параметрами lr=0.3, num_iter=20 и начальными приближениями  cur_x1, cur_x2 = 1.5, -1. Сумма значений x1, x2 и f(x1, x2) (т.е. сумма значений в steps[-1]), умноженная на 10\\**6 и округленная до 2 знаков после запятой, будет ответом на первый вопрос кр."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "steps = grad_descent(<тут Ваш код>)\n",
    "np.sum(steps[-1]) * 10**6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Линейные модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем код для линейной регресси с семинара. Напомним, что найти веса W и b для линейной регресси можно двумя способами: обращением матриц (функция solve_weights) и градиентным спуском (функция grad_descent). Мы здесь будем рассматривать градиентный спуск."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = None\n",
    "b = None\n",
    "\n",
    "def mse(preds, y):\n",
    "    return ((preds - y)**2).mean()\n",
    "    \n",
    "def grad_descent(X, y, lr, num_iter=100):\n",
    "    global W, b\n",
    "    np.random.seed(40)\n",
    "    W = np.random.rand(X.shape[1])\n",
    "    b = np.array(np.random.rand(1))\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    N = X.shape[0]\n",
    "    for iter_num in range(num_iter):\n",
    "        preds = predict(X)\n",
    "        losses.append(mse(preds, y))\n",
    "        \n",
    "        w_grad = np.zeros_like(W)\n",
    "        b_grad = 0\n",
    "        for sample, prediction, label in zip(X, preds, y):\n",
    "            w_grad += 2 * (prediction - label) * sample\n",
    "            b_grad += 2 * (prediction - label)\n",
    "            \n",
    "        W -= lr * w_grad\n",
    "        b -= lr * b_grad\n",
    "    return losses\n",
    "\n",
    "def predict(X):\n",
    "    global W, b\n",
    "    return np.squeeze(X @ W + b.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим функцию:  \n",
    "\n",
    "$$f(x, y) = 0.43x+0.5y + 0.67$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Friendly reminder:***  \n",
    "Что мы хотим? Мы хотим уметь \"восстанавливать функцию\" -- то есть предсказывать значения функции в точках с координатами (x, y) (именно так и получается 3D-график -- (x, y, f(x,y)) в пространстве).  \n",
    "В чём сложность? Нам дан только конечный небольшой набор точек (30 в данном случае), по которому мы хотим восстановить зависимость, по сути, непрерывную. Линейная регрессия как раз подходит для восстановления линейной зависимости (а у нас функция сейчас как раз линейная -- только сложение аргументов и умножение на число)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cгерерируем шумные данные из этой функции (как на семинаре):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(40)\n",
    "func = lambda x, y: (0.43*x + 0.5*y + 0.67 + np.random.normal(0, 7, size=x.shape))\n",
    "\n",
    "X = np.random.sample(size=(30)) * 10\n",
    "Y = np.random.sample(size=(30)) * 150\n",
    "result_train = [func(x, y) for x, y in zip(X, Y)]\n",
    "data_train = np.concatenate([X.reshape(-1, 1), Y.reshape(-1, 1)], axis=1)\n",
    "\n",
    "pd.DataFrame({'x': X, 'y': Y, 'res': result_train}).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что же мы сгенерировали:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %matplotlib osx\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "ax = fig.gca(projection='3d')\n",
    "X, Y = np.meshgrid(np.linspace(0, 10, 100), np.linspace(0, 150, 100))\n",
    "ax.scatter(xs=data_train[:, 0], ys=data_train[:, 1], zs=result_train, c='r')\n",
    "ax.plot_surface(X,Y, 0.43*X + 0.5*Y + 0.67, color='green', alpha=0.3)\n",
    "\n",
    "ax.view_init(elev=60)\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте попробуем применить к этим данным нашу линейную регрессию и с помощью неё предсказать истинный график функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses = grad_descent(data_train, result_train, 1e-2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на график лосса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(losses), losses[-1];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И на полученную разделяющую плоскость:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %matplotlib osx\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "ax = fig.gca(projection='3d')\n",
    "X, Y = np.meshgrid(np.linspace(0, 10, 100), np.linspace(0, 150, 100))\n",
    "ax.scatter(xs=data_train[:, 0], ys=data_train[:, 1], zs=result_train, c='r')\n",
    "ax.plot_surface(X,Y, 0.43*X + 0.5*Y + 0.67, color='green', alpha=0.3)\n",
    "ax.plot_surface(X,Y, W[0]*X + W[1]*Y + b, color='blue', alpha=0.3)\n",
    "\n",
    "ax.view_init(elev=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(зелёная плоскость -- истинная функция, синяя плоскость -- предсказание)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Охх, лосс и коэффициенты (W и b) нашей модели быстро уходит в небеса, и график предсказан неправильно. Почему такое происходит?\n",
    "\n",
    "В данном случае дело в том, что признаки имеют разный *масштаб* (посмотрите на значения X и Y -- они лежат в разных диапазонах). Многие модели машинного обучения, в том числе линейные, будут плохо работать в таком случае (на самом деле это зависит от метода оптимизации, сейчас это градиентный спуск).  \n",
    "\n",
    "Есть несколько способов **масштабирования**:\n",
    "1. **Нормализация (она же стандартизация, StandardScaling)**:  \n",
    "\n",
    "$$x_{ij} = \\frac{x_{ij} - \\mu_j}{\\sigma_j}$$  \n",
    "(j -- номер признака, i -- номер объекта)  \n",
    "То есть вычитаем среднее по столбцу и делим на корень из дисперсии.\n",
    "\n",
    "2. **Приведение к отрезку [0,1] (MinMaxScaling)**:  \n",
    "\n",
    "$$x_{ij} = \\frac{x_{ij} - \\min_j}{\\max_j - \\min_j}$$  \n",
    "(j -- номер признака, i -- номер объекта)  \n",
    "То есть вычитаем минимум по столбцу и делим на разницу между минимумом и максимумом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем нормализацию:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Нормализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на среднее и разброс значений в признаках (координатах) до масштабирования:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть в первом столбце у нас значения = 5.6 $\\pm$ 2.7, а во втором столбце = 70.8 $\\pm$ 45.3. Масштаб разный, давайте исправим это."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Нормализуйте признаки** так, чтобы среднее значение в каждом слобце было 0, а минимальное и максимальное значения: -1 и 1 (т.е. дисперсия (разброс значений около среднего значения) = 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train_normalized = <тут Ваш код>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверьте средние и диспресию (.std()):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<тут Ваш код>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<тут Ваш код>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем теперь запустить регрессию с теми же параметрами lr и num_iter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses = grad_descent(data_train_normalized, result_train, 1e-2, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "print(losses[-1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим, что теперь коэффициенты по модулю не огромны, градиентный спуск не взрывается и лосс стабилен!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на полученную плоскость:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %matplotlib osx\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "ax.scatter(xs=data_train[:, 0], ys=data_train[:, 1], zs=result_train, c='r')\n",
    "\n",
    "X, Y = np.meshgrid(np.linspace(-1, 10, 100), np.linspace(-1, 150, 100))\n",
    "ax.plot_surface(X, Y, 0.43*X + 0.5*Y + 0.67, color='green', alpha=0.3)\n",
    "\n",
    "X, Y = np.meshgrid(np.linspace(-1, 10, 100), np.linspace(-1, 150, 100))\n",
    "# Hint: не забудьте нормализовать X и Y тоже\n",
    "ax.plot_surface(X, Y, W[0] * <тут Ваш код> + W[1] * <тут Ваш код> + b, color='blue', alpha=0.3)\n",
    "\n",
    "ax.view_init(elev=60)\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ответ на КР  №2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустите градиентный спуск на нормализованных данных с параметрами lr=1e-2, num_iter=200 и посчитайте сумму последних трех значений лосса. Полученное число, округленное до второго знака после запятой, будет ответом на второе задание кр."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses = grad_descent(<тут Ваш код>)\n",
    "np.sum(losses[-3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо \"сырой\" линейной регресси часто используют линейную регрессию с регуляризацией -- Lasso или Ridge регрессию. Они отличаются только типом \"штрафа\" за большие веса: учитывать модули (Lasso) или квадраты весов (Ridge)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Учитывая, что наш лосс в этой задаче -- Mean Squared Error (MSE), в случае Ridge-регресси будет:  \n",
    "\n",
    "$$Loss = MSE = \\sum (pred_i-y_i)^2 + \\alpha*\\sum W_i^2$$  \n",
    "\n",
    "А в случае Lasso-регресси будет:  \n",
    "\n",
    "$$Loss = MSE = \\sum (pred_i-y_i)^2 + \\alpha*\\sum |W_i|$$  \n",
    "\n",
    "\n",
    "Здесь $\\alpha$ -- заранее задаваемый гиперпараметр. Это вес, с которым второе слагаемое будет влиять на лосс."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавление регуляризации, как правило, помогает бороться с **переобучением**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробнее об этом можно почитать [тут](http://www.machinelearning.ru/wiki/index.php?title=%D0%9B%D0%B0%D1%81%D1%81%D0%BE) и [тут](http://www.machinelearning.ru/wiki/index.php?title=%D0%A0%D0%B8%D0%B4%D0%B6-%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D1%8F)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Полезные ссылки:\n",
    "1. Открытый курс по машинному обучению: https://habr.com/company/ods/blog/323890/\n",
    "2. Если вам интересно математическое обоснование всех методов, рекомендуем ознакомиться с этой книгой: https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[opt]derivative_gradient.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
