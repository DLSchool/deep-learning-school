{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s8.hostingkartinok.com/uploads/images/2018/08/308b49fcfbc619d629fe4604bceb67ac.jpg\" width=500, height=450>\n",
    "<h3 style=\"text-align: center;\"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\"><b>Домашнее задание: применение Dropout (на примере задачи регрессии)</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напомним как выглядит полносвязная нейросеть:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://juice-health.ru/images/technology/32.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"http://juice-health.ru/images/technology/32.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проблема"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переобучение (overfitting) — одна из проблем глубоких нейронных сетей, состоящая в следующем: модель хорошо объясняет только примеры из обучающей выборки, адаптируясь к обучающим примерам, вместо того чтобы учиться классифицировать примеры, не участвовавшие в обучении (теряя способность к обобщению)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/68/Overfitted_Data.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://upload.wikimedia.org/wikipedia/commons/6/68/Overfitted_Data.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Идея"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Главная идея Dropout — вместо обучения одной нейронной сети обучить ансамбль нескольких нейронных сетей, а затем усреднить полученные результаты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сети для обучения получаются с помощью исключения из сети (dropping out) нейронов с вероятностью $p$(в семинаре $keep\\_rate$), таким образом, вероятность того, что нейрон останется в сети, составляет $q = 1 - p$. “Исключение” нейрона означает, что при любых входных данных или параметрах он возвращает 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Слева — нейронная сеть до того, как к ней применили Dropout, справа — та же сеть после Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://hsto.org/web/dd8/171/16f/dd817116fc2348e78272577153e31d2d.jpeg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://hsto.org/web/dd8/171/16f/dd817116fc2348e78272577153e31d2d.jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Результат"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исключенные нейроны не вносят свой вклад в процесс обучения ни на одном из этапов алгоритма обратного распространения ошибки (backpropagation), поэтому исключение хотя бы одного из нейронов равносильно обучению новой нейронной сети.\n",
    "В двух словах, Dropout хорошо работает на практике, потому что предотвращает взаимоадаптацию нейронов на этапе обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим как хорошо на практике работает Dropout. Будем решать задачу **регресии**. Сгенерируем две выборки $test\\_y$, $y$ из одного распределения и будем обучаться на $y$, а смотреть ошибку на $test\\_y$. Будем использовать два способа обучения: c использованием Dropout и без него. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import sleep\n",
    "%config InlineBackend.figure_format = 'svg' \n",
    "torch.manual_seed(2)   \n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы показываем вам, как были сгенерированы $y$ и $test\\_y$, но, чтобы наблюдать правильность выполнения задания, мы зафиксировали какие-то рандомные выборки из этого распределения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Кол-во точек \n",
    "N_SAMPLES = 20\n",
    "\n",
    "# Строим выборку для обучения \n",
    "x = torch.linspace(-1, 1, N_SAMPLES).view(N_SAMPLES, -1)\n",
    "#y = x + 0.3 * torch.normal(torch.zeros(N_SAMPLES, 1), torch.ones(N_SAMPLES, 1))\n",
    "y = torch.tensor([[-1.3122],[-0.6198],[-1.1807],[-1.0171],[-0.5700],[-0.4886],[-0.0489],[ 0.0027],[-0.4012],[ 0.1495]\n",
    "                  ,[-0.2844],[ 0.1303],[ 0.3053],[ 0.7042],[ 0.5683],[ 1.1048],[ 0.4623],[ 0.4167],[ 0.8422],[ 1.2097]])\n",
    "\n",
    "# Строим тестовую выборку\n",
    "test_x = torch.linspace(-1, 1, N_SAMPLES).view(N_SAMPLES, -1)\n",
    "#test_y = test_x + 0.3 * torch.normal(torch.zeros(N_SAMPLES, 1), torch.ones(N_SAMPLES, 1))\n",
    "test_y = torch.tensor([[-1.3883],[-0.9020],[-0.8601],[-0.8968],[-0.2950],[-1.0038],[-0.5611],[ 0.4919],[-0.4285],[-0.0572],\n",
    "                       [ 0.4683],[ 0.9315],[ 0.1591],[ 0.3946],[ 0.1438],[ 0.7278],[ 1.1072],[ 0.6730],[ 1.0141],[ 0.6687]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отобразим наши данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter('''your code here''', c='magenta', s=20, alpha=0.6, label='train')\n",
    "plt.scatter('''your code here''', c='cyan', s=20, alpha=0.6, label='test')\n",
    "plt.legend(loc='upper left')\n",
    "plt.ylim((-2.5, 2.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <b>Важно! На вход нам подается один нейрон, так как точка $x$ имеет один признак(координату), всего объектов 20. Для каждого объкта независимо от других прогнозируется его $y$, но линейное преобразование на определенном слое для каждого объекта одинакова, а веса этого слоя определяются из минимизации суммарной ошибки на $y$</b>\n",
    " \n",
    " Создадим нейронную сеть с 3 слоями и в качестве нелинейности используем ReLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Размер и входа и выхода линейного скрытого слоя\n",
    "N_HIDDEN = 300\n",
    "net_overfitting = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, N_HIDDEN),\n",
    "    '''your code here'''\n",
    "    torch.nn.Linear(N_HIDDEN, 1),\n",
    ")\n",
    "print(net_overfitting)  # архитектура сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим Dropout после первого и второго слова с keep_rate=0.5, то есть удаляем нейрон с вероятностью 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_dropped = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, N_HIDDEN),\n",
    "    '''your code here'''\n",
    "    torch.nn.Linear(N_HIDDEN, 1),\n",
    ")\n",
    "print(net_dropped) # архитектура сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим градиентный спуск каждой нейронной сети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_ofit = torch.optim.Adam('''your code here''', lr=0.01)\n",
    "optimizer_drop = torch.optim.Adam('''your code here''', lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим квадратичную функцию потерь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.'''your code here'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Ошибки на y и test_y\n",
    "error = []\n",
    "error_test = []\n",
    "for t in range(500):\n",
    "    # Пропустим x по нейронной сети\n",
    "    pred_ofit = '''your code here'''\n",
    "    pred_drop = '''your code here'''\n",
    "    \n",
    "    loss_ofit = loss_func(pred_ofit, y)\n",
    "    loss_drop = loss_func(pred_drop, y)\n",
    "\n",
    "    # Cделаем шаг градиентного спуска\n",
    "    optimizer_ofit.zero_grad()\n",
    "    optimizer_drop.zero_grad()\n",
    "    '''your code here'''\n",
    "\n",
    "    if t % 20 == 0:\n",
    "        # перейдем к режиму eval, чтобы учесть поправку dropout\n",
    "        net_overfitting.eval()\n",
    "        net_dropped.eval()\n",
    "\n",
    "        # plotting\n",
    "        clear_output(wait=True)\n",
    "        sleep(0.05)\n",
    "        \n",
    "         # Пропустим test_x по нейронной сети\n",
    "        '''your code here'''\n",
    "        \n",
    "        #Отобразим наши данные и предсказания различных нейронных сетей на тестовых данных:\n",
    "        plt.scatter('''your code here''', c='magenta', s=20, alpha=0.6, label='train')\n",
    "        plt.scatter('''your code here''', c='cyan', s=20, alpha=0.6, label='test')\n",
    "        plt.plot(test_x.data.numpy(), test_pred_ofit.data.numpy(), 'r-', lw=3, label='overfitting')\n",
    "        plt.plot(test_x.data.numpy(), test_pred_drop.data.numpy(), 'b--', lw=3, label='dropout(50%)')\n",
    "        \n",
    "        # Ошибка на y и на на test_y\n",
    "        error.append((loss_func(test_pred_ofit, y).data.numpy(), loss_func(test_pred_drop, y).data.numpy()))\n",
    "        error_test.append('''your code here''')\n",
    "        \n",
    "        plt.text(-0.5, -1.8, 'overfitting loss=%.4f' % '''your code here''', fontdict={'size': 20, 'color':  'red'})\n",
    "        plt.text(-0.5, -2.3, 'dropout loss=%.4f' % '''your code here''', fontdict={'size': 20, 'color': 'blue'})\n",
    "        plt.legend(loc='upper left'); plt.ylim((-2.5, 2.5));#plt.pause(0.1)\n",
    "        plt.show()\n",
    "        \n",
    "        # вернемся к режиму train\n",
    "        net_overfitting.train()\n",
    "        net_dropped.train()\n",
    "\n",
    "# Выведим ошибки двух сетей попарно на y:\n",
    "print('overfitting loss train', 'dropout loss train')\n",
    "for i in range(len(error)):\n",
    "    print('%.4f\\t\\t\\t%.4f' % (error[i][0], error[i][1]))\n",
    "\n",
    "print()\n",
    "# Выведим ошибки двух сетей попарно на test_y:\n",
    "print('overfitting loss', '   dropout loss')\n",
    "for i in range(len(error)):\n",
    "    print('%.4f\\t\\t\\t%.4f' % '''your code here''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вопросы:\n",
    "1. Какая суммарная квадратичная ошибка между $y$ и $test\\_y$? (c помощью loss_func)\n",
    "<br>Ответ: ...\n",
    "2. Сколько параметров у линейного скрытого слоя (не забудьте про свободный член b)?\n",
    "<br> Ответ: ...\n",
    "3. Какая у вас получилась ошибка $overfitting\\:loss\\:train$ в итоге (округлите до 2 знаков после запятой)?\n",
    "<br> Ответ: ...\n",
    "4. Какая у вас получилась ошибка $dropout\\:loss\\:train$ в итоге (округлите до 2 знаков после запятой)?\n",
    "<br> Ответ: ...\n",
    "5. Какая у вас получилась ошибка $overfitting\\:loss$ в итоге (округлите до 2 знаков после запятой)?\n",
    "<br> Ответ: ...\n",
    "6. Какая у вас получилась ошибка $dropout\\:loss$ в итоге (округлите до 2 знаков после запятой)?\n",
    "<br> Ответ: ...\n",
    "\n",
    "<br> Сравните 3, 4 и подумайте, почему так получилось?\n",
    "<br> Сравните 4, 5 и подумайте, почему так получилось?\n",
    "<br> Так же сравните 4, 5 с предыдущими двумя ошибками."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
