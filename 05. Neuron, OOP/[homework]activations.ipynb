{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[homework]activations_new.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"colab_type":"text","id":"ISV_j8bo9if8"},"cell_type":"markdown","source":["<img src=\"https://s8.hostingkartinok.com/uploads/images/2018/08/308b49fcfbc619d629fe4604bceb67ac.jpg\" width=500, height=450>\n","<h3 style=\"text-align: center;\"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>"]},{"metadata":{"colab_type":"text","id":"RnuMlpJp9if9"},"cell_type":"markdown","source":["---"]},{"metadata":{"colab_type":"text","id":"lkoop-MT9if-"},"cell_type":"markdown","source":["<h2 style=\"text-align: center;\"><b>Домашнее задание: нейрон с разными функциями активации</b></h2>"]},{"metadata":{"colab_type":"text","id":"qGKppuWS9if-"},"cell_type":"markdown","source":["---"]},{"metadata":{"colab_type":"text","id":"6gfbXqXQ9if_"},"cell_type":"markdown","source":["### Сначала необходимо решить ноутбуки `[seminar]perceptron_new.ipynb` и `[seminar]neuron_new.ipynb`!"]},{"metadata":{"colab_type":"text","id":"IUjxg_IS31tn"},"cell_type":"markdown","source":["**Очень часто спрашивают -- а какую функции активации стоит выбрать?** В этом ноутбуке вам предлагается самим дойти до истины и сравнить нейроны с различными функциями активации (их качестве на двух выборках). Не забудьте убедиться, что все эксперименты с разными видами нейронов вы проводите в одинаковых условиях (иначе ведь эксперимент будет нечестным).\n","\n","В данном задании Вам нужно будет: \n","- самостоятельно реализовать класс **`Neuron()`** с разными функциями активации\n","- обучить и протестировать этот класс на сгенерированных и реальных данных (файлы с реальными данными помещены в папку /data в этой же директории)"]},{"metadata":{"colab_type":"text","id":"mHap62ES9igB"},"cell_type":"markdown","source":["В данном ноутбуке Вам предстоит реализовать нейрон с разными функциями активации: Sigmoid, ReLU, LeakyReLU и ELU."]},{"metadata":{"colab_type":"code","id":"p-OYlV519igB","colab":{}},"cell_type":"code","source":["from matplotlib import pyplot as plt\n","from matplotlib.colors import ListedColormap\n","import numpy as np\n","import pandas as pd"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Op5kD164iuut","colab_type":"text"},"cell_type":"markdown","source":["***В этом ноутбуке используется функция `numpy.random.rand`. Чтобы иметь неслучайные значения, которые помогут нам\n","проверить ответы на задание, мы введем свою функцию, которая принимает `seed`. Такая функция будет выдавать \n","одинаковые значения при одном и том же seed. Не стоит менять seed далее в коде, иначе ответы могут не сойтись.***"]},{"metadata":{"colab_type":"code","id":"t1VCt_rDHum3","colab":{}},"cell_type":"code","source":["def seed_random(seed, *args):\n","    np.random.seed(seed)\n","    return np.random.rand(*args)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"CmEexUY631uM"},"cell_type":"markdown","source":["---"]},{"metadata":{"colab_type":"text","id":"cgpFOaPm9igG"},"cell_type":"markdown","source":["В данном случае мы снова решаем задачу бинарной классификации (2 класса: 1 или 0):\n","\n","$$\n","Loss(\\hat{y}, y) = \\frac{1}{2n}\\sum_{i=1}^{n} (\\hat{y_i} - y_i)^2\n","$$  \n","\n","Здесь $w \\cdot X_i$ - скалярное произведение, а $\\sigma(w \\cdot X_i) =\\frac{1}{1+e^{-w \\cdot X_i}} $ - сигмоида ($i$ -- номер объекта в выборке).  \n","\n","**Примечание:** Здесь предполагается, что $b$ - свободный член - является частью вектора весов: $w_0$. Тогда, если к $X$ приписать единичный столбец, в скалярном произведении $b$ будет именно как свободный член."]},{"metadata":{"colab_type":"code","id":"yn3F0layHunD","colab":{}},"cell_type":"code","source":["def Loss(y_pred, y):\n","    <Ваш код здесь>"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"rYoQAdXoHunK"},"cell_type":"markdown","source":["Далее будут предложены несколько функций активации, и вам нужно реализовать класс `Neuron` по аналогии с тем, как это было на семинаре. Сам принцип тот же, но меняются формула обновления весов и формула предсказания.\n","\n","**Система такая**: Будут три функции активации, у первой все формулы даны, нужно лишь закодить. У второй будет написана производная, но не будет подставлена в $Loss$, это нужно сделать вам. У третьей будет лишь сама формула функции."]},{"metadata":{"colab_type":"text","id":"WHvdwJW9HunN"},"cell_type":"markdown","source":["<h2 style=\"text-align: center;\"><b>Нейрон с ReLU (Recitified Linear Unit)</b></h2>  "]},{"metadata":{"colab_type":"text","id":"AMA7Gi6KHunQ"},"cell_type":"markdown","source":["ReLU самая часто используемая (по крайней мере, пару лет назад) функция активации в нейронных сетях. Выглядит она очень просто:\n","\n","\\begin{equation*}\n","ReLU(x) =\n"," \\begin{cases}\n","   0, &\\text{$x \\le 0$}\\\\\n","   x, &\\text{$x \\gt 0$}\n"," \\end{cases}\n","\\end{equation*}\n","\n","Или по-другому:\n","\n","$$\n","ReLU(x) = \\max(0, x)\n","$$\n","\n","В (свободном) переводе Rectified Linear Unit = \"Усечённая линейная функция\". Собственно, мы по сути просто не даём проходить отрицательным числам.\n","\n","Производная здесь берётся как производная от кусочно-заданной функции, то есть на участках, где функция гладкая (x < 0 и x > 0), берем производную как обычно, и в нуле её доопредляем ее нулём:\n","\n","\\begin{equation*}\n","ReLU'(x) = \n"," \\begin{cases}\n","   0, &\\text{$x \\le 0$}\\\\\n","   1, &\\text{$x \\gt 0$}\n"," \\end{cases}\n","\\end{equation*}\n","\n","График этой функции и её производной выглядят так:\n","\n","<img src=\"https://upload-images.jianshu.io/upload_images/1828517-0828da0d1164c024.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240\" width=800, height=400>\n","\n","Подставим ReLu в Loss:\n","\n","$$Loss(\\hat{y}, y) = \\frac{1}{2n}\\sum_{i=1}^{n} (\\hat{y_i} - y_i)^2 = \\frac{1}{2n}\\sum_{i=1}^{n} (ReLU(w \\cdot X_i) - y_i)^2 = \\begin{equation*}\n","\\frac{1}{2n}\\sum_{i=1}^{n}\n"," \\begin{cases}\n","    y_i^2, &{w \\cdot X_i \\le 0}\\\\\n","   (w \\cdot X_i - y_i)^2, &{w \\cdot X_i \\gt 0}\n"," \\end{cases}\n","\\end{equation*}$$  \n","\n","(помните, что $w \\cdot X_i$ -- это число в данном случае (результат скалярного произведения двух векторов).\n","\n","Тогда формула для обновления весов при градиентном спуске будет такая (в матричном виде; рекомендуем вывести самим то, как это получается из формулы для одного объекта (см. `[seminar]neuron.ipynb`):\n","\n","$$ \\frac{\\partial Loss}{\\partial w} = \\begin{equation*}\n","\\sum_{i=1}^{n}\n"," \\begin{cases}\n","   0, &{w \\cdot X_i \\le 0}\\\\\n","   \\frac{1}{n} X_i^T (w \\cdot X_i - y), &{w \\cdot X_i \\gt 0}\n"," \\end{cases}\n","\\end{equation*}$$\n","\n","(напоминаем, что здесь $w \\cdot X$ -- матричное произведение вектора $w$ (ведь вектор -- тоже матрица, не так ли?) и матрицы $X$)\n","\n","Почему в первом случае будет 0? Потому что в формулу $y_i^2$ не входят веса , а мы берём производную именно по весам $w$.\n","\n","* Реализуйте ReLU и её производную:"]},{"metadata":{"colab_type":"code","id":"DCgAeho19igI","colab":{}},"cell_type":"code","source":["def relu(x):\n","    \"\"\"ReLU-функция\"\"\"\n","    <Ваш код здесь>"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"nXwsy-7J9igL","colab":{}},"cell_type":"code","source":["def relu_derivative(y):\n","    \"\"\"Производная ReLU. Мы вычисляем ее не по x, который подставили в ReLU, а по значению, который она вернула. \n","    На самом деле, мы могли бы так не делать и вычислять производную по x (код при этом даже не поменялся бы), \n","    но обычно на стадии обратного прохода у нас уже нет X @ w, который мы передавали в функцию, зато есть \n","    вычисленное значение активации - тот самый y\"\"\"\n","    <Ваш код здесь>"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"qKurn-7F9igN"},"cell_type":"markdown","source":["Теперь нужно написать нейрон с ReLU. Здесь всё очень похоже на перцептрон, но будут по-другому обновляться веса и другая функция активации:"]},{"metadata":{"colab_type":"code","id":"AM9vn3OX9igO","colab":{}},"cell_type":"code","source":["class NeuronReLU:\n","    def __init__(self, w=None, b=0):\n","        \"\"\"\n","        :param: w -- вектор весов\n","        :param: b -- смещение\n","        \"\"\"\n","        # Пока что мы не знаем размер матрицы X, а значит не знаем, сколько будет весов\n","        self.w = w\n","        self.b = b\n","        \n","    def activate(self, x):\n","        return <Ваш код здесь>\n","        \n","    def forward_pass(self, X):\n","        \"\"\"\n","        Рассчитывает ответ нейрона при предъявлении набора объектов\n","        :param: X -- матрица примеров размера (n, m), каждая строка - отдельный объект\n","        :return: вектор размера (n, 1) из нулей и единиц с ответами перцептрона \n","        \"\"\"\n","        n = X.shape[0]\n","        y_pred = np.zeros((n, 1))  # y_pred == y_predicted - предсказанные классы\n","        return <Ваш код здесь>\n","    \n","    def backward_pass(self, X, y, y_pred, learning_rate=0.005):\n","        \"\"\"\n","        Обновляет значения весов нейрона в соответствии с этим объектом\n","        :param: X -- матрица входов размера (n, m)\n","                y -- вектор правильных ответов размера (n, 1)\n","                learning_rate - \"скорость обучения\" (символ alpha в формулах выше)\n","        В этом методе ничего возвращать не нужно, только правильно поменять веса\n","        с помощью градиентного спуска.\n","        \"\"\"\n","        n = len(y)\n","        y = np.array(y).reshape(-1, 1)\n","        \n","        <Ваш код здесь>\n","    \n","    def fit(self, X, y, num_epochs=300):\n","        \"\"\"\n","        Спускаемся в минимум\n","        :param: X -- матрица объектов размера (n, m)\n","                y -- вектор правильных ответов размера (n, 1)\n","                num_epochs -- количество итераций обучения\n","        :return: losses -- вектор значений функции потерь\n","        \"\"\"\n","        if self.w is None:\n","            self.w = np.zeros((X.shape[1], 1))  # столбец (m, 1)\n","            self.b = 0  # смещение (число)\n","        Loss_values = []  # значения функции потерь на различных итерациях обновления весов\n","        \n","        for i in range(num_epochs):\n","            <Ваш код здесь>\n","            \n","        return Loss_values"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"thtFp-at9igS"},"cell_type":"markdown","source":["<h3 style=\"text-align: center;\"><b>Тестирование нейрона с ReLU</b></h3>  "]},{"metadata":{"colab_type":"text","id":"hOuYzf_u9igS"},"cell_type":"markdown","source":["Здесь Вам нужно самим протестировать новый нейрон **на тех же данных (\"Яблоки и Груши\")** по аналогии с тем, как это было проделано с перцептроном  (можете смело копировать код, только будьте осторожны - кое-что в нём всё же скорее всего придётся поправить).\n","В итоге нужно вывести: \n","* график, на котором будет показано, как изменяется функция потерь $Loss$ в зависимости от числа итераций обучения\n","* график с раскраской выборки сигмоидальным нейроном"]},{"metadata":{"colab_type":"text","id":"chEeb88gHuny"},"cell_type":"markdown","source":["***ПРИМЕЧАНИЕ***: пожалуйста, почаще проверяйте `.shape` у матриц и векторов: `self.w`, `X` и `y` внутри класса. Очень часто ошибка решается транспонированием или `.reshape()`'ом. Не забывайте проверять, что на что вы умножаете и какой вектор (какой размер) хотите получить на выходе -- это очень помогает не запутаться."]},{"metadata":{"colab_type":"text","id":"JGs_F5N331u9"},"cell_type":"markdown","source":["**(для теста) Проверка forward_pass()**"]},{"metadata":{"colab_type":"code","id":"aKmrhe_831vG","colab":{}},"cell_type":"code","source":["w = np.array([1., 2.]).reshape(2, 1)\n","b = 2.\n","X = np.array([[1., 3.],\n","              [2., 4.],\n","              [-1., -3.2]])\n","\n","neuron = NeuronReLU(w, b)\n","y_pred = neuron.forward_pass(X)\n","print (\"y_pred = \" + str(y_pred))"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"dvm9wt9kHuoA"},"cell_type":"markdown","source":["*Hint: \"**-0.**\" -- это просто ноль*"]},{"metadata":{"colab_type":"text","id":"VYG2uFcy31vP"},"cell_type":"markdown","source":["**(для теста) Проверка backward_pass()**"]},{"metadata":{"id":"HmhT0_uziuvS","colab_type":"text"},"cell_type":"markdown","source":["Просьба **не менять `learning rate=0.005` по-умолчанию**."]},{"metadata":{"colab_type":"code","id":"UvUkrgQy31vQ","colab":{}},"cell_type":"code","source":["y = np.array([1, 0, 1]).reshape(3, 1)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"pWooOLya31vX","colab":{}},"cell_type":"code","source":["neuron.backward_pass(X, y, y_pred)\n","\n","print (\"w = \" + str(neuron.w))\n","print (\"b = \" + str(neuron.b))"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"X085TCvBHuoQ"},"cell_type":"markdown","source":["\"Яблоки и Груши\":"]},{"metadata":{"colab_type":"code","id":"LEnSapY_HuoR","colab":{}},"cell_type":"code","source":["data = pd.read_csv(\"./data/apples_pears.csv\")\n","plt.figure(figsize=(10, 8))\n","plt.scatter(data.iloc[:, 0], data.iloc[:, 1], c=data['target'], cmap='rainbow')\n","plt.title('Яблоки и груши', fontsize=15)\n","plt.xlabel('симметричность', fontsize=14)\n","plt.ylabel('желтизна', fontsize=14)\n","plt.show();"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"03cIoaEsHuod","colab":{}},"cell_type":"code","source":["X = data.iloc[:,:2].values  # матрица объекты-признаки\n","y = data['target'].values.reshape((-1, 1))  # классы (столбец из нулей и единиц)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"dR6xZg6kHuol"},"cell_type":"markdown","source":["Выведите лосс при обучении нейрона с ReLU на этом датасете:"]},{"metadata":{"colab_type":"code","id":"FZDtNG6CHuop","colab":{}},"cell_type":"code","source":["%%time\n","\n","neuron = <Ваш код здесь>\n","Loss_values = <Ваш код здесь>\n","\n","plt.figure(figsize=(10, 8))\n","plt.plot(Loss_values)\n","plt.title('Функция потерь', fontsize=15)\n","plt.xlabel('номер итерации', fontsize=14)\n","plt.ylabel('$Loss(\\hat{y}, y)$', fontsize=14)\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"sPxo2YVeHuou"},"cell_type":"markdown","source":["Скорее всего сейчас у вас лосс -- это прямая линия, и вы видите, что веса не обновляются. Но почему?!"]},{"metadata":{"colab_type":"text","id":"mXUmEeUBHuov"},"cell_type":"markdown","source":["Всё просто -- возможно мы об этом вам ещё не говорили, но если присмотреться, то видно, что `self.w` и `self.b` иницилизируются нулями в начале `.fit()`-метода. Если расписать, как будет идти обновление, то видно, что из-за ReLU веса просто-напросто не будут обновляться, если начать с инициализации нулями. \n","\n","Это -- одна из причин, по которой в нейронных сетях веса инициализируют случаными числами (обычно из отрезка [0, 1)).\n","\n","Обучим нейрон, инициализировав случайно веса, **используя нашу функция `seed_random` с `seed=42 и 43` из начала ноутбука** (поставьте 10000 итераций), а также просьба **не менять `learning rate=0.005` по-умолчанию**."]},{"metadata":{"colab_type":"code","id":"E821cUM0Huo8","colab":{}},"cell_type":"code","source":["%%time\n","\n","neuron = NeuronReLU(w=seed_random(42, X.shape[1], 1), b=seed_random(43, 1))\n","Loss_values = <Ваш код здесь>  # num_epochs=10000\n","\n","plt.figure(figsize=(10, 8))\n","plt.plot(Loss_values)\n","plt.title('Функция потерь', fontsize=15)\n","plt.xlabel('номер итерации', fontsize=14)\n","plt.ylabel('$Loss(\\hat{y}, y)$', fontsize=14)\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"z3XeufnfHupE"},"cell_type":"markdown","source":["**(для теста) Проверка лосса:**"]},{"metadata":{"colab_type":"text","id":"HCn5DSP5HupK"},"cell_type":"markdown","source":["Выведите сумму первых пяти и последних пяти значений loss'а при обучении с num_epochs=10000, округлите до 4-го знака после запятой:"]},{"metadata":{"colab_type":"code","id":"ELZVl_9rHupL","scrolled":true,"colab":{}},"cell_type":"code","source":["<Ваш код здесь>"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"jkWMrc9uHupS"},"cell_type":"markdown","source":["Посмотрим, как предсказывает этот нейрон:"]},{"metadata":{"colab_type":"code","id":"hyjfcthcHupT","scrolled":false,"colab":{}},"cell_type":"code","source":["plt.figure(figsize=(10, 8))\n","plt.scatter(data.iloc[:, 0], data.iloc[:, 1], c=np.array(neuron.forward_pass(X) > 0.5).ravel(), cmap='spring')\n","plt.title('Яблоки и груши', fontsize=15)\n","plt.xlabel('симметричность', fontsize=14)\n","plt.ylabel('желтизна', fontsize=14)\n","plt.show();"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"fA9TCYBXHupW"},"cell_type":"markdown","source":["Должно разделиться более-менее неплохо. Но почему мы берём якобы \"прокаченную\" ReLU, которая часто используется, и она предсказывает хуже (намного дольше сходится), чем перцептрон с пороговой, который никто не использует? Вообще, где и какая функция активации \"выстрелит\" -- никто не знает заранее, это зависит в том числе от самих данных."]},{"metadata":{"colab_type":"text","id":"nDvxxQQDHupZ"},"cell_type":"markdown","source":["<img src=\"https://alumni.lscollege.ac.uk/files/2015/12/Interview-questions-square-image.jpg\" width=400 height=300>"]},{"metadata":{"colab_type":"text","id":"MSNZuWl9Hupd"},"cell_type":"markdown","source":["Но есть одна тенденция: пороговая функция активации и сигмоида (обычно всё же только сигмоида) чаще используются именно на **выходном слое** нейросети в задаче классификации -- ими предсказывают вероятности объектов принадлежать одному из классов, в то время как продвинутые функции активации (ReLU и те, что будут дальше) используются внутри нейросети, то есть на **скрытых слоях**. \n","\n","ReLU как будто моделирует то, что нейрон \"загорается\" по аналогии с тем, как это происходит с биологическим, поэтому помещаеть ReLU на выходной слой обычно плохая идея."]},{"metadata":{"colab_type":"text","id":"_X3lDHElHupo"},"cell_type":"markdown","source":["Однако ничто не мешает помещать ReLU на выходной слой, а сигмоиду -- внутрь. Deep Learning -- \"очень экспериментальная\" область: вы можете сделать открытие своими собственными руками, просто поменяв что-то незначительное, например, функцию активации."]},{"metadata":{"colab_type":"text","id":"sET710OVHupp"},"cell_type":"markdown","source":["**Плюсы ReLU:**"]},{"metadata":{"colab_type":"text","id":"HY3iXGBWHupr"},"cell_type":"markdown","source":["* дифференцируемая (с доопределнием в нуле)\n","* нет проблемы затухающих градиентов, как в сигмоиде"]},{"metadata":{"colab_type":"text","id":"Nr_3XwTWHups"},"cell_type":"markdown","source":["**Возможные минусы ReLU:**"]},{"metadata":{"colab_type":"text","id":"0TAwVdRXHupt"},"cell_type":"markdown","source":["* не центрирована около 0 (может мешать скорости сходимсти)\n","* зануляет все отрицательные входы, тем самым веса у занулённых нейронов могут часто *не обновляться*, эту проблему иногда называют *мёртвые нейроны*"]},{"metadata":{"colab_type":"text","id":"Vj1JGXTPHupu"},"cell_type":"markdown","source":["С последней проблемой можно побороться, а именно:"]},{"metadata":{"colab_type":"text","id":"Yn2taDMNHupv"},"cell_type":"markdown","source":["<h2 style=\"text-align: center;\"><b>Нейрон с LeakyReLU (Leaky Recitified Linear Unit)</b></h2>  "]},{"metadata":{"colab_type":"text","id":"iCRBWooSHupx"},"cell_type":"markdown","source":["LeakyReLU очень слабо отличается от ReLU, но часто помогает сети обучаться быстрее, поскольку нет проблемы \"мёртвых нейронов\":\n","\n","\\begin{equation*}\n","LeakyReLU(x) =\n"," \\begin{cases}\n","   \\alpha x, &\\text{$x \\le 0$}\\\\\n","   x, &\\text{$x \\gt 0$}\n"," \\end{cases}\n","\\end{equation*}\n","\n","где $\\alpha$ -- маленькое число от 0 до 1.\n","\n","Производная здесь берётся так же, но вместо нуля будет $\\alpha$:\n","\n","\\begin{equation*}\n","LeakyReLU'(x) = \n"," \\begin{cases}\n","   \\alpha, &\\text{$x \\le 0$}\\\\\n","   1, &\\text{$x \\gt 0$}\n"," \\end{cases}\n","\\end{equation*}\n","\n","График этой функции:\n","\n","<img src=\"https://cdn-images-1.medium.com/max/1600/0*UtLlZJ80TMIM7kXk.\" width=400 height=300>\n","\n","Подставим LeakyReLu в Loss:\n","\n","$$\n","Loss(\\hat{y}, y) = \\frac{1}{2n}\\sum_{i=1}^{n} (\\hat{y_i} - y_i)^2 = \\frac{1}{2n}\\sum_{i=1}^{n} (LeakyReLU(w \\cdot X_i) - y_i)^2 =\n","\\begin{equation*}\n","\\frac{1}{2n}\\sum_{i=1}^{n} \n"," \\begin{cases}\n","   (\\alpha \\cdot w \\cdot X_i - y_i)^2, &{w \\cdot X_i \\le 0}\\\\\n","   (w \\cdot X_i - y_i)^2, &{w \\cdot X_i \\gt 0}\n"," \\end{cases}\n","\\end{equation*}\n","$$  \n","\n","Формула для обновления весов при градиентном спуске:\n","\n","$$ \\frac{\\partial Loss}{\\partial w} = \\begin{equation*}\n","\\frac{1}{n}\\sum_{i=1}^{n} \n"," \\begin{cases}\n","   \\alpha X_i^T (w \\cdot X_i - y), &{w \\cdot X_i \\le 0}\\\\\n","    X_i^T (w \\cdot X_i - y), &{w \\cdot X_i \\gt 0}\n"," \\end{cases}\n","\\end{equation*}$$\n","\n","* Реализуйте LeakyReLU и её производную:"]},{"metadata":{"colab_type":"code","id":"yuBLORHs6reh","colab":{}},"cell_type":"code","source":["def leaky_relu(x, alpha=0.01):\n","    \"\"\"LeakyReLU-функция\"\"\"\n","    <Ваш код здесь>"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"PZObir-96rek","colab":{}},"cell_type":"code","source":["def leaky_relu_derivative(y, alpha=0.01):\n","    \"\"\"Производная LeakyReLU. Тут мы тоже вычисляем производную по y. Пояснения, почему мы так делаем,\n","    есть выше\"\"\"\n","    <Ваш код здесь>"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"TE25WcUQ6ren"},"cell_type":"markdown","source":["Теперь нужно написать нейрон с LeakyReLU функцией активации. Здесь всё очень похоже на перцептрон, но будут по-другому обновляться веса и другая функция активации:"]},{"metadata":{"colab_type":"code","id":"P1hF_ufY6ren","colab":{}},"cell_type":"code","source":["class NeuronLeakyReLU:\n","    def __init__(self, w=None, b=0):\n","        \"\"\"\n","        :param: w -- вектор весов\n","        :param: b -- смещение\n","        \"\"\"\n","        # Пока что мы не знаем размер матрицы X, а значит не знаем, сколько будет весов\n","        self.w = w\n","        self.b = b\n","        \n","    def activate(self, x):\n","        return <Ваш код здесь>\n","        \n","    def forward_pass(self, X):\n","        \"\"\"\n","        Рассчитывает ответ нейрона при предъявлении набора объектов\n","        :param: X -- матрица примеров размера (n, m), каждая строка - отдельный объект\n","        :return: вектор размера (n, 1) из нулей и единиц с ответами перцептрона \n","        \"\"\"\n","        n = X.shape[0]\n","        y_pred = np.zeros((n, 1))  # y_pred == y_predicted - предсказанные классы\n","        return <Ваш код здесь>\n","        \n","    def backward_pass(self, X, y, y_pred, learning_rate=0.005):\n","        \"\"\"\n","        Обновляет значения весов нейрона в соответствии с этим объектом\n","        :param: X -- матрица входов размера (n, m)\n","                y -- вектор правильных ответов размера (n, 1)\n","                learning_rate - \"скорость обучения\" (символ alpha в формулах выше)\n","        В этом методе ничего возвращать не нужно, только правильно поменять веса\n","        с помощью градиентного спуска.\n","        \"\"\"\n","        n = len(y)\n","        y = np.array(y).reshape(-1, 1)\n","        \n","        <Ваш код здесь>\n","    \n","    def fit(self, X, y, num_epochs=300):\n","        \"\"\"\n","        Спускаемся в минимум\n","        :param: X -- матрица объектов размера (n, m)\n","                y -- вектор правильных ответов размера (n, 1)\n","                num_epochs -- количество итераций обучения\n","        :return: losses -- вектор значений функции потерь\n","        \"\"\"\n","        if self.w is None:\n","            self.w = np.zeros((X.shape[1], 1))  # столбец (m, 1)\n","            self.b = 0  # смещение (число)\n","        Loss_values = []  # значения функции потерь на различных итерациях обновления весов\n","        \n","        for i in range(num_epochs):\n","            <Ваш код здесь>\n","        \n","        return Loss_values"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"RhbHyAib6req"},"cell_type":"markdown","source":["<h3 style=\"text-align: center;\"><b>Тестирование нейрона с LeakyReLU</b></h3>  "]},{"metadata":{"colab_type":"text","id":"45LYM788HuqF"},"cell_type":"markdown","source":["***ПРИМЕЧАНИЕ***: пожалуйста, почаще проверяйте `.shape` у матриц и векторов: `self.w`, `X` и `y` внутри класса. Очень часто ошибка решается транспонированием или `.reshape()`'ом. Не забывайте проверять, что на что вы умножаете и какой вектор (какой размер) хотите получить на выходе -- это очень помогает не запутаться.  \n","\n","**Везде далее для тестирования не меняйте $\\alpha$=0.01 в `leaky_relu()` и в `leaky_relu_derivative()`**"]},{"metadata":{"id":"WzU3A_wsiuwf","colab_type":"text"},"cell_type":"markdown","source":["Просьба **не менять `learning rate=0.005` по-умолчанию**."]},{"metadata":{"colab_type":"text","id":"CuxkU6lIj1dn"},"cell_type":"markdown","source":["**(для теста) Проверка forward_pass()**"]},{"metadata":{"colab_type":"code","id":"5_YgDU9wj1dp","colab":{}},"cell_type":"code","source":["w = np.array([1., 2.]).reshape(2, 1)\n","b = 2.\n","X = np.array([[1., 3.],\n","              [2., 4.],\n","              [-1., -3.2]])\n","\n","neuron = NeuronLeakyReLU(w, b)\n","y_pred = neuron.forward_pass(X)\n","print (\"y_pred = \" + str(y_pred))"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"9G20aXOcj1ds"},"cell_type":"markdown","source":["*Hint: \"**-0.**\" -- это просто ноль*"]},{"metadata":{"colab_type":"text","id":"VoyrITZ_j1dt"},"cell_type":"markdown","source":["**(для теста) Проверка backward_pass()**"]},{"metadata":{"colab_type":"text","id":"GxZrHFtQj1du"},"cell_type":"markdown","source":["Просьба **не менять `learning rate=0.005` по-умолчанию**."]},{"metadata":{"colab_type":"code","id":"C3U507I_j1dv","colab":{}},"cell_type":"code","source":["y = np.array([1, 0, 1]).reshape(3, 1)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"R8--K7CGj1d6","colab":{}},"cell_type":"code","source":["neuron.backward_pass(X, y, y_pred)\n","\n","print (\"w = \" + str(neuron.w))\n","print (\"b = \" + str(neuron.b))"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"PsCNxcsDHuqb"},"cell_type":"markdown","source":["\"Яблоки и Груши\":"]},{"metadata":{"colab_type":"code","id":"MEXxd-YRHuqg","colab":{}},"cell_type":"code","source":["data = pd.read_csv(\"./data/apples_pears.csv\")\n","plt.figure(figsize=(10, 8))\n","plt.scatter(data.iloc[:, 0], data.iloc[:, 1], c=data['target'], cmap='rainbow')\n","plt.title('Яблоки и груши', fontsize=15)\n","plt.xlabel('симметричность', fontsize=14)\n","plt.ylabel('желтизна', fontsize=14)\n","plt.show();"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"uzQyBSMOHuqk","colab":{}},"cell_type":"code","source":["X = data.iloc[:,:2].values  # матрица объекты-признаки\n","y = data['target'].values.reshape((-1, 1))  # классы (столбец из нулей и единиц)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"d-92UxDqHuqo"},"cell_type":"markdown","source":["Обучим нейрон, инициализировав случайно веса (поставьте 10000 итераций)."]},{"metadata":{"id":"Sr_wrwQsiuwz","colab_type":"text"},"cell_type":"markdown","source":["Просьба **не менять `learning rate=0.005` по-умолчанию**."]},{"metadata":{"colab_type":"code","id":"SGztqTFhHuqo","colab":{}},"cell_type":"code","source":["%%time\n","\n","neuron = NeuronLeakyReLU(w=seed_random(13, X.shape[1], 1), b=seed_random(14, 1))\n","Loss_values = <Ваш код здесь>  # num_epochs=10000\n","\n","plt.figure(figsize=(10, 8))\n","plt.plot(Loss_values)\n","plt.title('Функция потерь', fontsize=15)\n","plt.xlabel('номер итерации', fontsize=14)\n","plt.ylabel('$Loss(\\hat{y}, y)$', fontsize=14)\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"lWBO-RI_Huqz"},"cell_type":"markdown","source":["**(для теста) Проверка лосса:**"]},{"metadata":{"colab_type":"text","id":"GqXREMzNHuq0"},"cell_type":"markdown","source":["Выведите сумму первых пяти и последних пяти значений loss'а при обучении с num_epochs=10000, округлите до 4-го знака после запятой:"]},{"metadata":{"colab_type":"text","id":"zvj3t74RHuq3"},"cell_type":"markdown","source":["Посмотрим, как предсказывает этот нейрон:"]},{"metadata":{"colab_type":"code","id":"jfFexRvcHuq5","scrolled":false,"colab":{}},"cell_type":"code","source":["plt.figure(figsize=(10, 8))\n","plt.scatter(data.iloc[:, 0], data.iloc[:, 1], c=np.array(neuron.forward_pass(X) > 0.5).ravel(), cmap='spring')\n","plt.title('Яблоки и груши', fontsize=15)\n","plt.xlabel('симметричность', fontsize=14)\n","plt.ylabel('желтизна', fontsize=14)\n","plt.show();"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"IgTac_vFHuq8"},"cell_type":"markdown","source":["**Плюсы LeakyReLU:**"]},{"metadata":{"colab_type":"text","id":"iAw-tzuLHuq9"},"cell_type":"markdown","source":["* дифференцируемая (с доопределнием в нуле)\n","* нет проблемы затухающих градиентов, как в сигмоиде\n","* нет проблемы \"мёртвых нейронов\", как в ReLU"]},{"metadata":{"colab_type":"text","id":"CMUGymwyHuq_"},"cell_type":"markdown","source":["**Возможные минусы LeakyReLU:**"]},{"metadata":{"colab_type":"text","id":"uQyqD8-LHurA"},"cell_type":"markdown","source":["* не центрирована около 0 (может мешать скорости сходимсти)\n","* немного не устойчива к \"шуму\" (см. лекции Стэнфорда)"]},{"metadata":{"colab_type":"text","id":"e7k07EGyHurB"},"cell_type":"markdown","source":["<h2 style=\"text-align: center;\"><b>Нейрон с ELU (Exponential Linear Unit)</a></b></h2>  \n","<h2 style=\"text-align: center;\"><b>(необязательная часть, проверяться не будет)</b></h2>"]},{"metadata":{"colab_type":"text","id":"dWH3Zk2zHurB"},"cell_type":"markdown","source":["\n","ELU -- не так давно предложенная (в 2015 году) функция активации, которая, как говорят авторы статьи, лучше LeakyReLU. Вот формула ELU:\n","\n","\\begin{equation*}\n","ELU(\\alpha, x) =\n"," \\begin{cases}\n","   \\alpha (e^x - 1), &\\text{$x \\le 0$}\\\\\n","   x, &\\text{$x \\gt 0$}\n"," \\end{cases}\n","\\end{equation*}\n","\n","где $\\alpha$ -- маленькое число от 0 до 1.\n","\n","Производная здесь берётся так же, но вместо нуля будет $\\alpha$:\n","\n","\\begin{equation*}\n","ELU'(x) = \n"," \\begin{cases}\n","   ELU(\\alpha, x) + \\alpha, &\\text{$x \\le 0$}\\\\\n","   1, &\\text{$x \\gt 0$}\n"," \\end{cases}\n","\\end{equation*}\n","\n","Здесь в производной использован постой трюк -- сделано $- \\alpha + \\alpha$, чтобы вычислять было проще.\n","\n","График этой функции:\n","\n","<img src=\"http://p0.ifengimg.com/pmop/2017/0907/A004001DD141881BFD8AD62E5D31028C3BE3FAD1_size14_w446_h354.png\" width=500 height=400>\n","\n","Подставим LeakyReLu в Loss:\n","\n","$$Loss(\\hat{y}, y) = \\frac{1}{2n}\\sum_{i=1}^{n} (\\hat{y_i} - y_i)^2 = \\frac{1}{2n}\\sum_{i=1}^{n} (ELU(\\alpha, w \\cdot X_i) - y_i)^2 = \\begin{equation*}\n","\\frac{1}{2n}\\sum_{i=1}^{n} \n"," \\begin{cases}\n","   (\\alpha (e^{w \\cdot X_i} - 1) - y_i)^2, &{w \\cdot X_i \\le 0}\\\\\n","   (w \\cdot X_i - y_i)^2, &{w \\cdot X_i \\gt 0}\n"," \\end{cases}\n","\\end{equation*}$$  \n","\n","Формула для обновления весов при градиентном спуске.. Здесь вам нужно выписать её самим, и это чуть сложнее, чем раньше. Брать производную \"в лоб\" некрасиво и неудобно. Нужно воспользоваться **правилом цепочки**, оно же **правило взятия производной сложной функции**:\n","\n","$$ \\frac{\\partial Loss}{\\partial w} = \\begin{equation*}\n","\\frac{1}{n}\\sum_{i=1}^{n} \n"," \\begin{cases}\n","    X_i^T (ELU(\\alpha, x) + \\alpha) (\\alpha (e^{w \\cdot X_i} - 1) - y_i), &{w \\cdot X_i \\le 0}\\\\\n","    X_i^T (w \\cdot X_i - y_i), &{w \\cdot X_i \\gt 0}\n"," \\end{cases}\n","\\end{equation*}$$\n","\n","* Реализуйте ELU и её производную:"]},{"metadata":{"colab_type":"code","id":"u7rc5PaI_tb4","colab":{}},"cell_type":"code","source":["def elu(x, alpha=0.01):\n","    \"\"\"ELU-функция\"\"\"\n","    <Ваш код здесь>"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"63ORnClM_0gf","colab":{}},"cell_type":"code","source":["def elu_derivative(y, alpha=0.01):\n","    \"\"\"Производная ELU, снова вычисляем производную по значению\"\"\"\n","    <Ваш код здесь>"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"Wo5dq-2X_0ze"},"cell_type":"markdown","source":["Теперь нужно написать нейрон с ELU функцией активации:"]},{"metadata":{"code_folding":[],"colab_type":"code","id":"9lTuu7gG_06W","colab":{}},"cell_type":"code","source":["class NeuronELU:\n","    def __init__(self, w=None, b=0):\n","        \"\"\"\n","        :param: w -- вектор весов\n","        :param: b -- смещение\n","        \"\"\"\n","        # Пока что мы не знаем размер матрицы X, а значит не знаем, сколько будет весов\n","        self.w = w\n","        self.b = b\n","        \n","    def activate(self, x):\n","        return <Ваш код здесь>\n","        \n","    def forward_pass(self, X):\n","        \"\"\"\n","        Рассчитывает ответ нейрона при предъявлении набора объектов\n","        :param: X -- матрица примеров размера (n, m), каждая строка - отдельный объект\n","        :return: вектор размера (n, 1) из нулей и единиц с ответами перцептрона \n","        \"\"\"\n","        n = X.shape[0]\n","        y_pred = np.zeros((n, 1))  # y_pred == y_predicted - предсказанные классы\n","        return <Ваш код здесь>\n","    \n","    def backward_pass(self, X, y, y_pred, learning_rate=0.005):\n","        \"\"\"\n","        Обновляет значения весов нейрона в соответствии с этим объектом\n","        :param: X -- матрица входов размера (n, m)\n","                y -- вектор правильных ответов размера (n, 1)\n","                learning_rate - \"скорость обучения\" (символ alpha в формулах выше)\n","        В этом методе ничего возвращать не нужно, только правильно поменять веса\n","        с помощью градиентного спуска.\n","        \"\"\"\n","        n = len(y)\n","        y = np.array(y).reshape(-1, 1)\n","        \n","        <Ваш код здесь>\n","    \n","    def fit(self, X, y, num_epochs=300):\n","        \"\"\"\n","        Спускаемся в минимум\n","        :param: X -- матрица объектов размера (n, m)\n","                y -- вектор правильных ответов размера (n, 1)\n","                num_epochs -- количество итераций обучения\n","        :return: losses -- вектор значений функции потерь\n","        \"\"\"\n","        if self.w is None:\n","            self.w = np.zeros((X.shape[1], 1))  # столбец (m, 1)\n","            self.b = 0  # смещение (число)\n","        Loss_values = []  # значения функции потерь на различных итерациях обновления весов\n","        \n","        for i in range(num_epochs):\n","            <Ваш код здесь>\n","        \n","        return Loss_values"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"jonYgSivHurP"},"cell_type":"markdown","source":["***ПРИМЕЧАНИЕ***: пожалуйста, почаще проверяйте `.shape` у матриц и векторов: `self.w`, `X` и `y` внутри класса. Очень часто ошибка решается транспонированием или `.reshape()`'ом. Не забывайте проверять, что на что вы умножаете и какой вектор (какой размер) хотите получить на выходе -- это очень помогает не запутаться."]},{"metadata":{"colab_type":"text","id":"pskXX7KVHurf"},"cell_type":"markdown","source":["\"Яблоки и Груши\":"]},{"metadata":{"colab_type":"code","id":"oSzTCZu_Hurh","colab":{}},"cell_type":"code","source":["data = pd.read_csv(\"./data/apples_pears.csv\")\n","plt.figure(figsize=(10, 8))\n","plt.scatter(data.iloc[:, 0], data.iloc[:, 1], c=data['target'], cmap='rainbow')\n","plt.title('Яблоки и груши', fontsize=15)\n","plt.xlabel('симметричность', fontsize=14)\n","plt.ylabel('желтизна', fontsize=14)\n","plt.show();"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"B1upsgBzHurl","colab":{}},"cell_type":"code","source":["X = data.iloc[:,:2].values  # матрица объекты-признаки\n","y = data['target'].values.reshape((-1, 1))  # классы (столбец из нулей и единиц)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"EsRToko0Hurp"},"cell_type":"markdown","source":["Обучим нейрон, инициализировав случайно веса (поставьте 10000 итераций):"]},{"metadata":{"colab_type":"code","id":"P6lOu_FbHurr","colab":{}},"cell_type":"code","source":["%%time\n","\n","neuron = NeuronELU(w=seed_random(10, X.shape[1], 1), b=seed_random(11, 1))\n","Loss_values = <Ваш код здесь>  # num_epochs=10000\n","\n","plt.figure(figsize=(10, 8))\n","plt.plot(Loss_values)\n","plt.title('Функция потерь', fontsize=15)\n","plt.xlabel('номер итерации', fontsize=14)\n","plt.ylabel('$Loss(\\hat{y}, y)$', fontsize=14)\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"L59jJs5ZHurt"},"cell_type":"markdown","source":["**(для теста) Проверка лосса:**"]},{"metadata":{"colab_type":"text","id":"yyWV15PSHuru"},"cell_type":"markdown","source":["Выведите сумму первых пяти и последних пяти значений loss'а при обучении с num_epochs=10000, округлите до 4-го знака после запятой:"]},{"metadata":{"colab_type":"text","id":"vF92uB8_Hurx"},"cell_type":"markdown","source":["Посмотрим, как предсказывает этот нейрон:"]},{"metadata":{"colab_type":"code","id":"CW7nCIEyHur2","scrolled":false,"colab":{}},"cell_type":"code","source":["plt.figure(figsize=(10, 8))\n","plt.scatter(data.iloc[:, 0], data.iloc[:, 1], c=np.array(neuron.forward_pass(X) > 0.5).ravel(), cmap='spring')\n","plt.title('Яблоки и груши', fontsize=15)\n","plt.xlabel('симметричность', fontsize=14)\n","plt.ylabel('желтизна', fontsize=14)\n","plt.show();"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"MYiGj1wnHur4"},"cell_type":"markdown","source":["**Плюсы ELU:**"]},{"metadata":{"colab_type":"text","id":"xOGAh06uHur5"},"cell_type":"markdown","source":["* дифференцируемая (с доопределнием в нуле)\n","* нет проблемы затухающих градиентов, как в сигмоиде\n","* нет проблемы \"мёртвых нейронов\", как в ReLU\n","* более устойчива к \"шуму\" (см. лекции Стэнфорда)"]},{"metadata":{"colab_type":"text","id":"r4doD89fHur6"},"cell_type":"markdown","source":["**Возможные минусы ELU:**"]},{"metadata":{"colab_type":"text","id":"AO4bvkKGHur6"},"cell_type":"markdown","source":["* не очень хорошо центрирована около 0 (может мешать скорости сходимсти)\n","* вычислительно дольше, чем ReLU и LeakyReLU"]},{"metadata":{"colab_type":"text","id":"4_YkH_HDHur7"},"cell_type":"markdown","source":["---"]},{"metadata":{"colab_type":"text","id":"wQG-VZuUHur8"},"cell_type":"markdown","source":["И напоследок -- все покемоны (почти):"]},{"metadata":{"colab_type":"text","id":"F2JcPTqKHur-"},"cell_type":"markdown","source":["<img src=\"http://cdn-images-1.medium.com/max/1600/1*DRKBmIlr7JowhSbqL6wngg.png\">"]},{"metadata":{"colab_type":"text","id":"A82W6KU7HusA"},"cell_type":"markdown","source":["Не хватает `SeLU()` и `Swish()`. Про них можно прочитать здесь: [SeLU](https://arxiv.org/pdf/1706.02515.pdf), [Swish](https://arxiv.org/pdf/1710.05941.pdf).\n","\n","`Tanh()` (тангенс гиперболический) используется в основном в рекуррентных нейросетях, а `Maxout()` мы решили не рассматривать (так как, опять же, нами не было замечено, что он часто используется, однако про него ходят хорошие слухи).  "]},{"metadata":{"colab_type":"text","id":"-PfNPtrZ31vq"},"cell_type":"markdown","source":["---"]},{"metadata":{"colab_type":"text","id":"EiQGCe9lHusC"},"cell_type":"markdown","source":["Думаете, это все функции активации? Нет, ведь за функцию активации можно взять вообще почти любую дифференцируемую функцию (которая, как вы полагаете, будет помогать обучению). Ещё больше функций активации вы можете [найти на википедии](https://en.wikipedia.org/wiki/Activation_function)."]},{"metadata":{"colab_type":"text","id":"yhGNsWPQ9igf"},"cell_type":"markdown","source":["<h3 style=\"text-align: center;\"><b>Полезные ссылки</b></h3>\n","\n","0). Обязательно прочитайте (если вам позволяет английский) эту статью от Стэнфорда: http://cs231n.github.io/neural-networks-1/\n","\n","1). Хорошая статья про функции активации: https://www.jeremyjordan.me/neural-networks-activation-functions/\n","\n","2). [Видео от Siraj Raval](https://www.youtube.com/watch?v=-7scQpLossT7uo)\n","\n","3). Современная статья про функции активации. Теперь на хайпе активация $swish(x) = x\\sigma (\\beta x)$: https://arxiv.org/pdf/1710.05941.pdf (кстати, при её поиске в некоторой степени использовался neural architecture search)\n","\n","4). SeLU имеет очень интересные, доказанные с помощью теории вероятностей свойства: https://arxiv.org/pdf/1706.02515.pdf (да, в этой статье 102 страницы)\n","\n","5). [Список функций активации из википедии](https://en.wikipedia.org/wiki/Activation_function)"]}]}