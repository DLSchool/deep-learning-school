{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[seminar]neuron.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"colab_type":"text","id":"ISV_j8bo9if8"},"cell_type":"markdown","source":["<img src=\"https://s8.hostingkartinok.com/uploads/images/2018/08/308b49fcfbc619d629fe4604bceb67ac.jpg\" width=500, height=450>\n","<h3 style=\"text-align: center;\"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>"]},{"metadata":{"colab_type":"text","id":"RnuMlpJp9if9"},"cell_type":"markdown","source":["---"]},{"metadata":{"colab_type":"text","id":"lkoop-MT9if-"},"cell_type":"markdown","source":["<h2 style=\"text-align: center;\"><b>Нейрон с сигмодой</b></h2>"]},{"metadata":{"colab_type":"text","id":"qGKppuWS9if-"},"cell_type":"markdown","source":["---"]},{"metadata":{"colab_type":"text","id":"6gfbXqXQ9if_"},"cell_type":"markdown","source":["### Сначала необходимо решить ноутбук `[seminar]perceptron.ipynb`!"]},{"metadata":{"id":"IUjxg_IS31tn","colab_type":"text"},"cell_type":"markdown","source":["В данном задании Вам нужно будет: \n","- самостоятельно реализовать класс **`Neuron()`** с сигмоидальной функцией активации\n","- обучить и протестировать этот класс на сгенерированных и реальных данных (файлы с реальными данными помещены в папку /data в этой же директории)\n","- сравнить качество работы Вашего класса с классом из библиотеки `scikit-learn` (`sklearn.linear_model.Perceptron()`)"]},{"metadata":{"colab_type":"text","id":"mHap62ES9igB"},"cell_type":"markdown","source":["В данном ноутбуке Вам предстоит реализовать нейрон с разными функциями активации: Sigmoid, ReLU, LeakyReLU и ELU."]},{"metadata":{"colab":{},"colab_type":"code","id":"p-OYlV519igB"},"cell_type":"code","source":["from matplotlib import pyplot as plt\n","from matplotlib.colors import ListedColormap  # тут лежат разные штуки для цветовой магии\n","import numpy as np\n","import pandas as pd"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"OU1KkHre9igF"},"cell_type":"markdown","source":["Напомним, что **сигмоидальная функция (сигмоида)** выглядит так:  \n","    \n","$$\\sigma(x)=\\frac{1}{1+e^{-z}}$$"]},{"metadata":{"id":"sG06ukL831t-","colab_type":"text"},"cell_type":"markdown","source":["Её график:"]},{"metadata":{"id":"I8-Pa5CL31uD","colab_type":"text"},"cell_type":"markdown","source":["<img src=\"https://cdn-images-1.medium.com/max/1200/1*IDAnCFoeXqWL7F4u9MLossMtA.png\" width=500px height=350px>"]},{"metadata":{"id":"w3B2wpFb31uH","colab_type":"text"},"cell_type":"markdown","source":["---"]},{"metadata":{"id":"ygSraLzH31uJ","colab_type":"text"},"cell_type":"markdown","source":["**Упражнение 1**\n","\n","Посчитать производную этой функции."]},{"metadata":{"id":"CmEexUY631uM","colab_type":"text"},"cell_type":"markdown","source":["---"]},{"metadata":{"colab_type":"text","id":"cgpFOaPm9igG"},"cell_type":"markdown","source":["В данном случае мы снова решаем задачу бинарной классификации (2 класса: 1 или 0), но здесь уже будет другая функция активации:\n","\n","$$\n","Loss(\\hat{y}, y) = \\frac{1}{2n}\\sum_{i=1}^{n} (\\hat{y_i} - y_i)^2 = \\frac{1}{2n}\\sum_{i=1}^{n} (\\sigma(w \\cdot X_i) - y_i)^2\n","$$  \n","\n","Здесь $w \\cdot X_i$ - скалярное произведение, а $\\sigma(w \\cdot X_i) =\\frac{1}{1+e^{-w \\cdot X_i}} $ - сигмоида ($i$ -- номер объекта в выборке).  \n","\n","**Примечание:** Здесь предполагается, что $b$ - свободный член - является частью вектора весов: $w_0$. Тогда, если к $X$ приписать единичный столбец, в скалярном произведении $b$ будет именно как свободный член."]},{"metadata":{"colab_type":"text","id":"KUp-NwTw9igG"},"cell_type":"markdown","source":["Формула для обновления весов при градиентном спуске будет такая:"]},{"metadata":{"colab_type":"text","id":"IOqEcXvl9igH"},"cell_type":"markdown","source":["$$ \\frac{\\partial Loss}{\\partial w} = \\frac{1}{n} (\\sigma(w \\cdot X) - y)\\sigma(w \\cdot X)(1 - \\sigma(w \\cdot X))X$$"]},{"metadata":{"colab_type":"text","id":"r1kKhH1v9igI"},"cell_type":"markdown","source":["Реализуйте сигмоиду и её производную:"]},{"metadata":{"colab":{},"colab_type":"code","id":"DCgAeho19igI"},"cell_type":"code","source":["def sigmoid(x):\n","    \"\"\"Сигмоидальная функция\"\"\"\n","    pass"],"execution_count":0,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"nXwsy-7J9igL"},"cell_type":"code","source":["def sigmoid_derivative(x):\n","    \"\"\"Производная сигмоиды\"\"\"\n","    pass"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"qKurn-7F9igN"},"cell_type":"markdown","source":["Теперь нужно написать нейрон с сигмоидной функцией активации. Здесь всё очень похоже на перцептрон, но будут по-другому обновляться веса и другая функция активации:"]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":132},"colab_type":"code","id":"AM9vn3OX9igO","outputId":"0391ccb7-3655-4637-e1eb-314d67b164e4","executionInfo":{"status":"error","timestamp":1539422231000,"user_tz":-300,"elapsed":610,"user":{"displayName":"Григорий Лелейтнер","photoUrl":"","userId":"07179937308049589303"}}},"cell_type":"code","source":["class Neuron:\n","    def __init__(self, w=None, b=0):\n","        \"\"\"\n","        :param: w -- вектор весов\n","        :param: b -- смещение\n","        \"\"\"\n","        # Пока что мы не знаем размер матрицы X, а значит не знаем, сколько будет весов\n","        self.w = w\n","        self.b = b\n","        \n","    def activate(self, x):\n","        return # Ваш код здесь\n","        \n","    def forward_pass(self, X):\n","        \"\"\"\n","        Рассчитывает ответ перцептрона при предъявлении набора объектов\n","        :param: X -- матрица примеров размера (n, m), каждая строка - отдельный объект\n","        :return: вектор размера (n, 1) из нулей и единиц с ответами перцептрона \n","        \"\"\"\n","        n = X.shape[0]\n","        y_pred = np.zeros((n, 1))  # y_pred(icted) - предсказанные классы\n","        # Ваш код здесь\n","        return y_pred\n","    \n","    def backward_pass(self, X, y, y_pred, learning_rate=0.005):\n","        \"\"\"\n","        Обновляет значения весов перцептрона в соответствии с этим объектом\n","        :param: X -- матрица входов размера (n, m)\n","                y -- вектор правильных ответов размера (n, 1)\n","                learning_rate - \"скорость обучения\" (символ alpha в формулах выше)\n","        В этом методе ничего возвращать не нужно, только правильно поменять веса\n","        с помощью градиентного спуска.\n","        \"\"\"\n","        # Ваш код здесь\n","    \n","    def fit(self, X, y, num_epochs=300):\n","        \"\"\"\n","        Спускаемся в минимум\n","        :param: X -- матрица объектов размера (n, m)\n","                y -- вектор правильных ответов размера (n, 1)\n","                num_epochs -- количество итераций обучения\n","        :return: losses -- вектор значений функции потерь\n","        \"\"\"\n","        self.w = np.zeros((X.shape[1], 1))  # столбец (m, 1)\n","        self.b = 0  # смещение\n","        losses = []  # значения функции потерь на различных итерациях обновления весов\n","        \n","        for i in range(num_epochs):\n","            # Ваш код здесь\n","        \n","        return losses"],"execution_count":10,"outputs":[{"output_type":"error","ename":"IndentationError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-e9756e33c91e>\"\u001b[0;36m, line \u001b[0;32m51\u001b[0m\n\u001b[0;31m    return losses\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"]}]},{"metadata":{"colab_type":"text","id":"thtFp-at9igS"},"cell_type":"markdown","source":["### Тестирование нейрона"]},{"metadata":{"colab_type":"text","id":"hOuYzf_u9igS"},"cell_type":"markdown","source":["Здесь Вам нужно самим протестировать новый нейрон **на тех же данных (\"Яблоки и Груши\")** по аналогии с тем, как это было проделано с перцептроном  (можете смело копировать код, только будьте осторожны - кое-что в нём всё же скорее всего придётся поправить).\n","В итоге нужно вывести: \n","* график, на котором будет показано, как изменяется функция потерь $Loss$ в зависимости от числа итераций обучения\n","* график с раскраской выборки сигмоидальным нейроном"]},{"metadata":{"id":"JGs_F5N331u9","colab_type":"text"},"cell_type":"markdown","source":["**Проверка forward_pass()**"]},{"metadata":{"id":"aKmrhe_831vG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":238},"outputId":"0bc0b4ae-084b-463e-9602-76f2cd17a260","executionInfo":{"status":"error","timestamp":1539421983029,"user_tz":-300,"elapsed":660,"user":{"displayName":"Григорий Лелейтнер","photoUrl":"","userId":"07179937308049589303"}}},"cell_type":"code","source":["w = np.array([1., 2.]).reshape(2, 1)\n","b = 2.\n","X = np.array([[1., 3.],\n","              [2., 4.],\n","              [-1., -3.2]])\n","\n","neuron = Neuron(w, b)\n","y_pred = neuron.forward_pass(X)\n","print (\"y_pred = \" + str(y_pred))"],"execution_count":6,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-6dc59e3032e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m               [-1., -3.2]])\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mneuron\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneuron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"y_pred = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Neuron' is not defined"]}]},{"metadata":{"id":"ZWjbXaT431vN","colab_type":"text"},"cell_type":"markdown","source":["|Должно быть||\n","|------|-------|\n","|**y_pred**|[0.99987661, 0.99999386,0.00449627]|"]},{"metadata":{"id":"VYG2uFcy31vP","colab_type":"text"},"cell_type":"markdown","source":["**Проверка backward_pass()**"]},{"metadata":{"id":"UvUkrgQy31vQ","colab_type":"code","colab":{}},"cell_type":"code","source":["y = np.array([1, 0, 1]).reshape(3, 1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pWooOLya31vX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"outputId":"cd2b2358-b615-4413-e984-0966ede7f355","executionInfo":{"status":"error","timestamp":1539421988792,"user_tz":-300,"elapsed":577,"user":{"displayName":"Григорий Лелейтнер","photoUrl":"","userId":"07179937308049589303"}}},"cell_type":"code","source":["neuron.backward_pass(X, y, y_pred)\n","\n","print (\"w = \" + str(neuron.w))\n","print (\"b = \" + str(neuron.b))"],"execution_count":8,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-e4007abfdc84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mneuron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"w = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneuron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"b = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneuron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'neuron' is not defined"]}]},{"metadata":{"id":"DcVff1Is31vc","colab_type":"text"},"cell_type":"markdown","source":["|Должно быть||\n","|------|-------|\n","|**w**|[0.99985106, 1.99952388]|\n","|**b**|2.000148326741343|"]},{"metadata":{"colab":{},"colab_type":"code","id":"FWgwECpD9igT"},"cell_type":"code","source":["# Ваш код здесь (можете использовать много ячеек)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-PfNPtrZ31vq","colab_type":"text"},"cell_type":"markdown","source":["---"]},{"metadata":{"id":"iT8so51w31wD","colab_type":"text"},"cell_type":"markdown","source":["В **домашнем задании** вам нужно будет реализовать класс `Neuron()` с другими функциями активации: ReLu, LeakyReLU, ELU, SeLU, Swish (какими-то из них), в тесте будут проверяться веса и значения функции потерь при некотором количестве итераций."]},{"metadata":{"colab_type":"text","id":"yhGNsWPQ9igf"},"cell_type":"markdown","source":["<h3 style=\"text-align: center;\"><b>Полезные ссылки</b></h3>\n","\n","0). Обязательно прочитайте (если вам позволяет английский) эту статью от Стэнфорда: http://cs231n.github.io/neural-networks-1/\n","\n","1). Хорошая статья про функции активации: https://www.jeremyjordan.me/neural-networks-activation-functions/\n","\n","2). [Видео от Siraj Raval](https://www.youtube.com/watch?v=-7scQpLossT7uo)\n","\n","3). Современная статья про функции активации. Теперь на хайпе активация $swish(x) = x\\sigma (\\beta x)$: https://arxiv.org/pdf/1710.05941.pdf (кстати, при её поиске в некоторой степени использовался neural architecture search)\n","\n","4). SeLU имеет очень интересные, доказанные с помощью теории вероятностей свойства: https://arxiv.org/pdf/1706.02515.pdf (да, в этой статье 102 страницы)"]},{"metadata":{"id":"SEWQ9GP1BA67","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}