{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"simple_seq2seq_seminar.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"colab_type":"text","id":"jgH3CAcubM4z"},"cell_type":"markdown","source":["# Семинар на тему Seq2Seq модели в машинном переводе\n","\n","На лекции мы подробно познакомились с подходами к решению задачи машинного перевода.\n","Наиболее распространенными моделями последовательностей (seq2seq) являются модели кодер-декодер, которые (обычно) используют рекуррентную нейронную сеть (RNN) для кодирования исходного (входного) предложения в один вектор.  Вы можете думать о векторе контекста как об абстрактном представлении всего входного предложения. Этот вектор затем декодируется декодером, который учится выводить  предложение, генерируя его по одному слову за раз.\n","\n","$h_t = \\text{Encoder}(x_t, h_{t-1})$\n","\n","У нас есть последовательность $X = \\{x_1, x_2, ..., x_T\\}$, где $x_1 = \\text{<sos>;}, x_2 = \\text{the}$, и так далее. Начальное состояние, $h_0$,  может быть инициализировано вектором из нулей или обучаемым.\n","\n","\n","Как только последнее слово, $x_T$, был подан на Encoder, мы  используем  информацию в  последнем скрытом состоянии, $h_T$, в зависимости от контекста вектор, т. е. $h_T $ это векторное представление всего исходного предложения.\n","\n","После получения вектора всего предложения мы можем декодировать предложение уже на новом языке. На каждом шаге декодирования мы подаем правильное слово $y_t$,  дополняем это информацией о скрытом состоянии $s_{t-1}$, где  $s_t = \\text{DecoderRNN}(y_t, s_{t-1})$\n","\n","\n","![alt text](https://i.stack.imgur.com/f6DQb.png)\n","\n","\n","Мы всегда используем $<sos>$ для первого входа в декодер, $y_1$, но для последующих входов, $y_{\\text{from }t; 1}$, мы иногда будем использовать фактическое, основное истинное следующее слово в последовательности, $y_t$, а иногда использовать слово, предсказанное нашим декодером, $\\hat{y}_{t-1}$. Использование настоящих токенов в декодере называется Teacher Forcing [можно тут посмотреть](https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/)\n","\n","Мы будем  использовать TorchText и spaCy( как токенизатор) , чтобы помочь вам выполнить всю необходимую предварительную обработку быстрее чем мы делали раньше. В данной работе вам предлагается написать модель Seq2Seq и обучить ее на Multi30k. В данном задание мы будем подавать на вход перевернутые предложения, так как авторы seq2seq считали, что это улучшает качество перевода."]},{"metadata":{"colab_type":"code","id":"1fPuwHEnVIzn","colab":{}},"cell_type":"code","source":["\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","\n","import spacy\n","\n","import random\n","import math\n","import time\n","\n","from torchtext.datasets import TranslationDataset, Multi30k\n","from torchtext.data import Field, BucketIterator"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"uQSnhb84VLU7","colab":{}},"cell_type":"code","source":["seed = 43\n","\n","random.seed(seed)\n","torch.manual_seed(seed)\n","np.random.seed(seed)\n","torch.backends.cudnn.deterministic = True\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"L10vdpVaVXBo","outputId":"c00ca813-4579-4173-cd67-da8ef637f2ae","executionInfo":{"status":"ok","timestamp":1555777705100,"user_tz":-180,"elapsed":11573,"user":{"displayName":"Deep Learning School","photoUrl":"https://lh6.googleusercontent.com/-pCAqm81DR_s/AAAAAAAAAAI/AAAAAAAAAMk/4cGT0s91JQs/s64/photo.jpg","userId":"16549096980415837553"}},"colab":{"base_uri":"https://localhost:8080/","height":534}},"cell_type":"code","source":["! python -m spacy download en\n","! python -m spacy download de\n","\n","\n","spacy_de = spacy.load('de')\n","spacy_en = spacy.load('en')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n","\n","\u001b[93m    Linking successful\u001b[0m\n","    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n","    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n","\n","    You can now load the model via spacy.load('en')\n","\n","Collecting de_core_news_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.0.0/de_core_news_sm-2.0.0.tar.gz#egg=de_core_news_sm==2.0.0\n","\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.0.0/de_core_news_sm-2.0.0.tar.gz (38.2MB)\n","\u001b[K    100% |████████████████████████████████| 38.2MB 134.3MB/s \n","\u001b[?25hInstalling collected packages: de-core-news-sm\n","  Running setup.py install for de-core-news-sm ... \u001b[?25ldone\n","\u001b[?25hSuccessfully installed de-core-news-sm-2.0.0\n","\n","\u001b[93m    Linking successful\u001b[0m\n","    /usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n","    /usr/local/lib/python3.6/dist-packages/spacy/data/de\n","\n","    You can now load the model via spacy.load('de')\n","\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"MV8d8onrbN4W"},"cell_type":"markdown","source":[""]},{"metadata":{"colab_type":"code","id":"ferOqkOUVirW","colab":{}},"cell_type":"code","source":["def tokenize_de(text):\n","    \"\"\"\n","    Tokenizes German text from a string into a list of strings (tokens) and reverses it\n","    \"\"\"\n","    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n","\n","def tokenize_en(text):\n","    \"\"\"\n","    Tokenizes English text from a string into a list of strings (tokens)\n","    \"\"\"\n","    return [tok.text for tok in spacy_en.tokenizer(text)]\n","\n","# немецкий язык является полем SRC, а английский в поле TRG\n","SRC = Field(tokenize = tokenize_de, \n","            init_token = '<sos>', \n","            eos_token = '<eos>', \n","            lower = True)\n","\n","TRG = Field(tokenize = tokenize_en, \n","            init_token = '<sos>', \n","            eos_token = '<eos>', \n","            lower = True)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"u6pNY6cWW3j5","outputId":"569ded5a-29c0-440f-e763-d166a903f66b","executionInfo":{"status":"ok","timestamp":1555778026395,"user_tz":-180,"elapsed":11840,"user":{"displayName":"Deep Learning School","photoUrl":"https://lh6.googleusercontent.com/-pCAqm81DR_s/AAAAAAAAAAI/AAAAAAAAAMk/4cGT0s91JQs/s64/photo.jpg","userId":"16549096980415837553"}},"colab":{"base_uri":"https://localhost:8080/","height":115}},"cell_type":"code","source":["# В датасете содержится ~ 30к предложений средняя длина которых 11\n","train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),  fields = (SRC, TRG))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["downloading training.tar.gz\n"],"name":"stdout"},{"output_type":"stream","text":["training.tar.gz: 100%|██████████| 1.21M/1.21M [00:01<00:00, 745kB/s] \n"],"name":"stderr"},{"output_type":"stream","text":["downloading validation.tar.gz\n"],"name":"stdout"},{"output_type":"stream","text":["validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 229kB/s]\n"],"name":"stderr"},{"output_type":"stream","text":["downloading mmt_task1_test2016.tar.gz\n"],"name":"stdout"},{"output_type":"stream","text":["mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 218kB/s]\n"],"name":"stderr"}]},{"metadata":{"colab_type":"text","id":"iOS3e7QZbLro"},"cell_type":"markdown","source":["Давайте посмотрим что у нас с датасетом и сделаем словари для SRC и TGT"]},{"metadata":{"colab_type":"code","id":"r0Xpf4IBW4Uf","outputId":"06525284-2b51-4724-aa8e-d03bc2afd730","executionInfo":{"status":"ok","timestamp":1555778067129,"user_tz":-180,"elapsed":657,"user":{"displayName":"Deep Learning School","photoUrl":"https://lh6.googleusercontent.com/-pCAqm81DR_s/AAAAAAAAAAI/AAAAAAAAAMk/4cGT0s91JQs/s64/photo.jpg","userId":"16549096980415837553"}},"colab":{"base_uri":"https://localhost:8080/","height":65}},"cell_type":"code","source":["labels = ['train', 'validation', 'test']\n","dataloaders = [train_data, valid_data, test_data]\n","for d, l in zip(dataloaders, labels):\n","    print(\"Number of sentences in {} : {}\".format(l, len(d.examples)))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Number of sentences in train : 29000\n","Number of sentences in validation : 1014\n","Number of sentences in test : 1000\n"],"name":"stdout"}]},{"metadata":{"colab_type":"code","id":"Gg63m8haW4XC","outputId":"9a138b06-328a-41f0-f2ef-b559b439fede","executionInfo":{"status":"ok","timestamp":1555778111414,"user_tz":-180,"elapsed":682,"user":{"displayName":"Deep Learning School","photoUrl":"https://lh6.googleusercontent.com/-pCAqm81DR_s/AAAAAAAAAAI/AAAAAAAAAMk/4cGT0s91JQs/s64/photo.jpg","userId":"16549096980415837553"}},"colab":{"base_uri":"https://localhost:8080/","height":49}},"cell_type":"code","source":["SRC.build_vocab(train_data, min_freq = 2)\n","TRG.build_vocab(train_data, min_freq = 2)\n","print(\"Number of words in source vocabulary\", len(SRC.vocab))\n","print(\"Number of words in source vocabulary\", len(TRG.vocab))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Number of words in source vocabulary 7853\n","Number of words in source vocabulary 5893\n"],"name":"stdout"}]},{"metadata":{"id":"-FhP6bJOzASS","colab_type":"code","outputId":"70ea4ded-6178-4a81-dd05-bb673907bba3","executionInfo":{"status":"ok","timestamp":1555781459937,"user_tz":-180,"elapsed":444,"user":{"displayName":"Deep Learning School","photoUrl":"https://lh6.googleusercontent.com/-pCAqm81DR_s/AAAAAAAAAAI/AAAAAAAAAMk/4cGT0s91JQs/s64/photo.jpg","userId":"16549096980415837553"}},"colab":{"base_uri":"https://localhost:8080/","height":115}},"cell_type":"code","source":["SRC.process(\"Ein klein gespenster\")"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n","            2,    2,    2,    2,    2,    2,    2,    2],\n","        [   0, 3833,    0,  664,    0,    0, 5884, 3833,    0,  664,    0, 5884,\n","            0,    0, 5884,    0,    0,    0, 5884, 7011],\n","        [   3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n","            3,    3,    3,    3,    3,    3,    3,    3]])"]},"metadata":{"tags":[]},"execution_count":64}]},{"metadata":{"id":"k8DcUkJGzaJT","colab_type":"code","outputId":"57c40898-9b5a-4076-eff3-fec5dc42d978","executionInfo":{"status":"error","timestamp":1555781609517,"user_tz":-180,"elapsed":498,"user":{"displayName":"Deep Learning School","photoUrl":"https://lh6.googleusercontent.com/-pCAqm81DR_s/AAAAAAAAAAI/AAAAAAAAAMk/4cGT0s91JQs/s64/photo.jpg","userId":"16549096980415837553"}},"colab":{"base_uri":"https://localhost:8080/","height":389}},"cell_type":"code","source":["model(SRC.process(\"Ein klein gespenster\"), np.array([]), teacher_forcing_ratio=0)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-68-e6d7b964e371>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSRC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ein klein gespenster\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-52-1a2be80045ba>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \"\"\"\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtrg_vocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: tuple index out of range"]}]},{"metadata":{"colab_type":"text","id":"LSd3la5FbJ5_"},"cell_type":"markdown","source":["## Encoder\n","\n","Напишем для начала простой Encoder, который реализует следующий функционал:\n","\n","$ (h_t, c_t) = \\text{LSTM}(x_t, (h_{t-1}, c_{t-1}))$\n","\n","В  методе forward мы передаем исходное предложение $X$, которое преобразуется в embeddings, к которым применяется dropout . Эти вектора затем передаются в RNN. Когда мы передадим всю последовательность RNN, он автоматически выполнит для нас рекуррентный расчет скрытых состояний по всей последовательности! Вы можете заметить, что мы не передаем начальное скрытое или состояние ячейки в RNN. Это происходит потому, что, как отмечено в документации, если никакое скрытое состояние/ячейка не передается RNN, он автоматически создаст начальное скрытое состояние/ячейка как тензор всех нулей."]},{"metadata":{"colab_type":"code","id":"_Ar5SN6tW4ck","colab":{}},"cell_type":"code","source":["class Encoder(nn.Module):\n","    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n","        \"\"\"\n","        :param: input_dim is the size/dimensionality of the one-hot vectors that will be input to the encoder. This is equal to the input (source) vocabulary size.\n","        :param: emb_dim is the dimensionality of the embedding layer. This layer converts the one-hot vectors into dense vectors with emb_dim dimensions.\n","        :param: hid_dim is the dimensionality of the hidden and cell states.\n","        :param: n_layers is the number of layers in the RNN.\n","        :param: percentage of the dropout to use\n","        \n","        \"\"\"\n","        super().__init__()\n","        \n","        self.input_dim = input_dim\n","        self.emb_dim = emb_dim\n","        self.hid_dim = hid_dim\n","        self.n_layers = n_layers\n","        self.dropout = dropout\n","\n","        self.embedding = nn.Embedding(input_dim, emb_dim)\n"," \n","        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, src):\n","        \"\"\"\n","        :param: src sentences (src_len x batch_size)\n","        \"\"\"\n","        # embedded = <TODO> (src_len x batch_size x embd_dim)\n","        embedded = self.embedding(src)\n","        # dropout over embedding\n","        embedded = self.dropout(embedded)\n","        outputs, (hidden, cell) = self.rnn(embedded)\n","        # [Attention return is for lstm, but you can also use gru]\n","        return hidden, cell"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"-8QOCpKxfD3M"},"cell_type":"markdown","source":["## Decoder\n","Похожий на Encoder, но со слоем проекцией, который переводит из hidden_dim в output"]},{"metadata":{"colab_type":"code","id":"gRgtzaf4bJp6","colab":{}},"cell_type":"code","source":["class Decoder(nn.Module):\n","    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n","        super().__init__()\n","\n","        self.emb_dim = emb_dim\n","        self.hid_dim = hid_dim\n","        self.output_dim = output_dim\n","        self.n_layers = n_layers\n","        self.dropout = dropout\n","        \n","        self.embedding = nn.Embedding(self.output_dim, self.emb_dim)\n","        \n","        self.rnn = nn.LSTM(self.emb_dim, self.hid_dim, self.n_layers) #(lstm embd, hid, layers, dropout)\n","        \n","        self.out = nn.Linear(self.hid_dim, self.output_dim)# Projection :hid_dim x output_dim\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, input_, hidden, cell):\n","        \n","        \n","        # (1x batch_size)\n","        input_ = input_.unsqueeze(0)\n","        \n","        # (1 x batch_size x emb_dim)\n","        embedded = self.embedding(input_)# embd over input and dropout \n","        embedded = self.dropout(embedded)\n","                \n","        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n","        \n","        #sent len and n directions will always be 1 in the decoder\n","        \n","        # (batch_size x output_dim)\n","        \n","        prediction = self.out(output.squeeze(0)) #project out of the rnn on the output dim \n","        \n","        return prediction, hidden, cell\n"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"UlD7-nusfL86"},"cell_type":"markdown","source":["## Seq2Seq module"]},{"metadata":{"colab_type":"code","id":"v_YvVGzaW4fY","colab":{}},"cell_type":"code","source":["class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder, device):\n","        super().__init__()\n","        # Hidden dimensions of encoder and decoder must be equal\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","        self._init_weights()  \n","    \n","    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n","        \"\"\"\n","        :param: src (src_len x batch_size)\n","        :param: tgt\n","        :param: teacher_forcing_ration : if 0.5 then every second token is the ground truth input\n","        \"\"\"\n","        \n","        batch_size = trg.shape[1]\n","        max_len = trg.shape[0]\n","        trg_vocab_size = self.decoder.output_dim\n","        \n","        #tensor to store decoder outputs\n","        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n","        \n","        #last hidden state of the encoder is used as the initial hidden state of the decoder\n","        hidden, cell = self.encoder(src) # TODO pass src throw encoder\n","        \n","        #first input to the decoder is the <sos> tokens\n","        input = trg[0, :] # TODO trg[idxs]\n","        \n","        for t in range(1, max_len):\n","            \n","            output, hidden, cell = self.decoder(input, hidden, cell) #TODO pass state and input throw decoder \n","            outputs[t] = output\n","            teacher_force = random.random() < teacher_forcing_ratio\n","            top1 = output.max(1)[1]\n","            input = (trg[t] if teacher_force else top1)\n","        \n","        return outputs\n","    \n","    def _init_weights(self):\n","        p = 0.08\n","        for name, param in self.named_parameters():\n","            nn.init.uniform_(param.data, -p, p)\n","        \n"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"msbn2VypfUur","colab":{}},"cell_type":"code","source":["input_dim = len(SRC.vocab)\n","output_dim = len(TRG.vocab)\n","src_embd_dim =  tgt_embd_dim = 256\n","hidden_dim = 512\n","num_layers =  2\n","dropout_prob = 0.2\n","\n","batch_size = 128\n","PAD_IDX = TRG.vocab.stoi['<pad>']\n","\n","iterators = BucketIterator.splits((train_data, valid_data, test_data),\n","                                  batch_size = batch_size, device = device)\n","train_iterator, valid_iterator, test_iterator = iterators\n","\n","enc = Encoder(input_dim, src_embd_dim, hidden_dim, num_layers, dropout_prob)\n","dec = Decoder(output_dim, tgt_embd_dim, hidden_dim, num_layers, dropout_prob)\n","model = Seq2Seq(enc, dec, device).to(device)\n"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"h5V9ZnK4fUxq","outputId":"4114497e-a04a-4276-9555-f79ef2d0bcd2","executionInfo":{"status":"ok","timestamp":1555780300545,"user_tz":-180,"elapsed":327,"user":{"displayName":"Deep Learning School","photoUrl":"https://lh6.googleusercontent.com/-pCAqm81DR_s/AAAAAAAAAAI/AAAAAAAAAMk/4cGT0s91JQs/s64/photo.jpg","userId":"16549096980415837553"}},"colab":{"base_uri":"https://localhost:8080/","height":230}},"cell_type":"code","source":["model"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Seq2Seq(\n","  (encoder): Encoder(\n","    (embedding): Embedding(7853, 256)\n","    (rnn): LSTM(256, 512, num_layers=2)\n","    (dropout): Dropout(p=0.2)\n","  )\n","  (decoder): Decoder(\n","    (embedding): Embedding(5893, 256)\n","    (rnn): LSTM(256, 512, num_layers=2)\n","    (out): Linear(in_features=512, out_features=5893, bias=True)\n","    (dropout): Dropout(p=0.2)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":54}]},{"metadata":{"colab_type":"code","id":"3vaUeDjTfU4k","colab":{}},"cell_type":"code","source":["def train(model, iterator, optimizer, criterion, clip):\n","    \n","    model.train()\n","    \n","    epoch_loss = 0\n","    \n","    for i, batch in enumerate(iterator):\n","        \n","        src = batch.src\n","        trg = batch.trg\n","        \n","        optimizer.zero_grad()\n","        \n","        output = model(src, trg)\n","        \n","        output = output[1:].view(-1, output.shape[-1])\n","        trg = trg[1:].view(-1)\n","        \n","        loss = criterion(output, trg)\n","        \n","        loss.backward()\n","        \n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        \n","        optimizer.step()\n","        \n","        epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(iterator)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"HfbTx2FMjaIM","colab":{}},"cell_type":"code","source":["def evaluate(model, iterator, criterion):\n","    \n","    model.eval()\n","    \n","    epoch_loss = 0\n","    \n","    with torch.no_grad():\n","    \n","        for i, batch in enumerate(iterator):\n","\n","            src = batch.src\n","            trg = batch.trg\n","\n","            output = model(src, trg, 0) #turn off teacher forcing !!\n","\n","            output = output[1:].view(-1, output.shape[-1])\n","            trg = trg[1:].view(-1)\n","\n","\n","            loss = criterion(output, trg)\n","            \n","            epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(iterator)"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"dSQehz_9jaiR","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"lQV_yqkLjcyQ","outputId":"3cd40bc5-2654-452a-e01d-6e6abc9acc0c","executionInfo":{"status":"ok","timestamp":1555781004652,"user_tz":-180,"elapsed":399836,"user":{"displayName":"Deep Learning School","photoUrl":"https://lh6.googleusercontent.com/-pCAqm81DR_s/AAAAAAAAAAI/AAAAAAAAAMk/4cGT0s91JQs/s64/photo.jpg","userId":"16549096980415837553"}},"colab":{"base_uri":"https://localhost:8080/","height":510}},"cell_type":"code","source":["max_epochs = 10\n","CLIP = 1\n","\n","# TODO\n","optimizer = optim.Adam(model.parameters(), lr = 0.001)\n","criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(max_epochs):\n","    \n","    \n","    train_loss = round(train(model, train_iterator, optimizer, criterion, CLIP), 5)\n","    valid_loss = round(evaluate(model, valid_iterator, criterion),5)\n","    \n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'model.pt')\n","    \n","    print('Epoch: {} \\n Train Loss {}  Val loss {}:'.format(epoch, train_loss, valid_loss))\n","    print('Train Perplexity {}  Val Perplexity {}:'.format(np.exp(train_loss), np.exp(valid_loss)))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch: 0 \n"," Train Loss 4.26041  Val loss 5.10416:\n","Train Perplexity 70.83902149988542  Val Perplexity 164.70565965310226:\n","Epoch: 1 \n"," Train Loss 4.22852  Val loss 5.07115:\n","Train Perplexity 68.61560589222772  Val Perplexity 159.35748311412488:\n","Epoch: 2 \n"," Train Loss 4.23805  Val loss 5.0519:\n","Train Perplexity 69.27263840361465  Val Perplexity 156.31918896791194:\n","Epoch: 3 \n"," Train Loss 4.18014  Val loss 5.06719:\n","Train Perplexity 65.37500507407512  Val Perplexity 158.72767532345105:\n","Epoch: 4 \n"," Train Loss 4.12467  Val loss 4.98157:\n","Train Perplexity 61.847396241646955  Val Perplexity 145.702955816573:\n","Epoch: 5 \n"," Train Loss 4.08609  Val loss 4.91129:\n","Train Perplexity 59.506764787709216  Val Perplexity 135.81450216037183:\n","Epoch: 6 \n"," Train Loss 4.04193  Val loss 4.87235:\n","Train Perplexity 56.93612355142656  Val Perplexity 130.6275311818444:\n","Epoch: 7 \n"," Train Loss 4.03445  Val loss 4.87374:\n","Train Perplexity 56.511830182740894  Val Perplexity 130.80922970140324:\n","Epoch: 8 \n"," Train Loss 3.96459  Val loss 4.7727:\n","Train Perplexity 52.69865850973066  Val Perplexity 118.2380541166968:\n","Epoch: 9 \n"," Train Loss 3.91681  Val loss 4.71081:\n","Train Perplexity 50.23992352674265  Val Perplexity 111.14214859572216:\n"],"name":"stdout"}]},{"metadata":{"colab_type":"code","id":"n5Zf6Kb1jhOI","outputId":"46edd4a4-9cc9-49cc-c4e3-fbd9d01cb68b","executionInfo":{"status":"ok","timestamp":1555781269111,"user_tz":-180,"elapsed":970,"user":{"displayName":"Deep Learning School","photoUrl":"https://lh6.googleusercontent.com/-pCAqm81DR_s/AAAAAAAAAAI/AAAAAAAAAMk/4cGT0s91JQs/s64/photo.jpg","userId":"16549096980415837553"}},"colab":{"base_uri":"https://localhost:8080/","height":32}},"cell_type":"code","source":["test_loss = evaluate(model, test_iterator, criterion)\n","\n","print('| Test Loss: {} Test PPL:{}|'.format(test_loss, np.exp(test_loss)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["| Test Loss: 4.754251182079315 Test PPL:116.07670029917236|\n"],"name":"stdout"}]},{"metadata":{"id":"an6HG7_uyjJN","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}